# PIPELINE DEFINITION
# Name: fixed-deposit-recommendation-pipeline
# Inputs:
#    minio_bucket: str [Default: 'ai360fd-recommendation']
components:
  comp-feature-engineering:
    executorLabel: exec-feature-engineering
    inputDefinitions:
      parameters:
        file_path:
          parameterType: STRING
        minio_bucket:
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-fetch-data:
    executorLabel: exec-fetch-data
    inputDefinitions:
      parameters:
        minio_bucket:
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-predict-new-data:
    executorLabel: exec-predict-new-data
    inputDefinitions:
      parameters:
        minio_bucket:
          parameterType: STRING
        pipeline_model:
          parameterType: STRING
        processed_data:
          parameterType: STRING
        trained_model:
          parameterType: STRING
  comp-train-model:
    executorLabel: exec-train-model
    inputDefinitions:
      parameters:
        feature_data_path:
          parameterType: STRING
        minio_bucket:
          parameterType: STRING
    outputDefinitions:
      parameters:
        model_path:
          parameterType: STRING
        pipeline_path:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-feature-engineering:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - feature_engineering
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef feature_engineering(file_path: str, minio_bucket: str) -> str:\n\
          \    from pyspark.sql import SparkSession\n    from pyspark.sql.functions\
          \ import col, lit, when, lower, monotonically_increasing_id\n    from pyspark.sql.types\
          \ import DoubleType\n    from pyspark.ml.feature import MinMaxScaler, VectorAssembler,\
          \ StringIndexer\n    import boto3\n    import os\n    import shutil\n  \
          \  from datetime import datetime\n\n    # Initialize Spark Session\n   \
          \ spark = SparkSession.builder.getOrCreate()\n\n    # Initialize MinIO client\n\
          \    minio_client = boto3.client(\n        's3',\n        endpoint_url=\"\
          http://192.168.80.155:32000\",\n        aws_access_key_id=\"admin\",\n \
          \       aws_secret_access_key=\"dlyticaD123\"\n    )\n\n    # Download raw\
          \ data file from MinIO\n    local_raw_data_path = \"/tmp/raw_data.csv\"\n\
          \    minio_client.download_file(minio_bucket, file_path, local_raw_data_path)\n\
          \n    # Load Data\n    df = spark.read.csv(local_raw_data_path, header=True,\
          \ inferSchema=True)\n    df = df.toDF(*[col.split('.')[-1] for col in df.columns])\
          \  # Rename columns\n\n    # Fill Null Values\n    categorical_cols = ['gender',\
          \ 'employment_status', 'marital_status', 'occupation', 'age_group', 'withdrawal_trends']\n\
          \    numerical_cols = [\n        'total_accounts', 'total_saving_accounts',\
          \ 'total_fixed_accounts', 'total_loan_accounts',\n        'total_overdraft_accounts',\
          \ 'total_debit_transaction_count', 'total_credit_transaction_count',\n \
          \       'total_debit_transaction_amount', 'total_credit_transaction_amount',\
          \ 'average_debit_transaction_count',\n        'average_credit_transaction_count',\
          \ 'average_debit_amount', 'average_credit_amount',\n        'average_yearly_saving',\
          \ 'total_yearly_saving', 'total_dormant_days', 'total_dormant_account',\n\
          \        'total_active_account', 'first_account_opened_days', 'last_account_opened_days'\n\
          \    ]\n\n    df = df.fillna(0, subset=numerical_cols)\n    df = df.fillna(\"\
          Unknown\", subset=categorical_cols)\n\n    # Ensure categorical columns\
          \ are properly formatted\n    for col_name in categorical_cols:\n      \
          \  df = df.withColumn(col_name, lower(col(col_name)))\n\n    # Apply StringIndexer\
          \ with handleInvalid=\"keep\"\n    indexers = [\n        StringIndexer(inputCol=col,\
          \ outputCol=f\"{col}_indexed\", handleInvalid=\"keep\").fit(df)\n      \
          \  for col in categorical_cols\n    ]\n\n    for indexer in indexers:\n\
          \        df = indexer.transform(df)\n        df = df.drop(indexer.getInputCol()).withColumnRenamed(f\"\
          {indexer.getInputCol()}_indexed\", indexer.getInputCol())\n\n    # **Ensure\
          \ `recent_transaction_activity` and `recent_dormant_account` Exist Before\
          \ Use**\n    df = df.withColumn(\n        \"recent_transaction_activity\"\
          ,\n        when(col(\"last_transaction_date\").isNotNull(), 1).otherwise(0)\n\
          \    )\n\n    df = df.withColumn(\n        \"recent_dormant_account\",\n\
          \        when(col(\"last_dormant_date\").isNotNull(), 1).otherwise(0)\n\
          \    )\n\n    # **Updated Target Column Generation Logic**\n    df = df.withColumn(\n\
          \        \"recommendation_score\",\n        (when(col(\"total_saving_accounts\"\
          ) > 3, 3).otherwise(0) + \n         when(col(\"total_fixed_accounts\") ==\
          \ 0, 2).otherwise(0) + \n         when(col(\"total_loan_accounts\") == 0,\
          \ 1).otherwise(0) + \n         when(col(\"recent_transaction_activity\"\
          ) == 1, 2).otherwise(0) + \n         when(col(\"recent_dormant_account\"\
          ) == 1, -2).otherwise(0) + \n         when(col(\"total_credit_transaction_count\"\
          ) > col(\"total_debit_transaction_count\") * 1.5, 3).otherwise(0) + \n \
          \        when(col(\"first_account_opened_days\") > 5 * 365, 2).otherwise(0)\
          \ + \n         when(col(\"last_account_opened_days\") < 2 * 365, 2).otherwise(0)\
          \ + \n         when(col(\"total_overdraft_accounts\") > 0, -2).otherwise(0)\
          \ + \n         when(col(\"total_dormant_account\") > 0, -1).otherwise(0)\
          \ + \n         when(col(\"recently_active\") == 1, 2).otherwise(0))\n  \
          \  )\n\n    # **Apply Threshold for Recommendation (If score >= 6, recommend\
          \ FD)**\n    df = df.withColumn(\"recommendation_fd_target\", when(col(\"\
          recommendation_score\") >= 6, 1).otherwise(0))\n    df = df.drop(\"recommendation_score\"\
          )\n\n    # **Save to MinIO**\n    temp_folder = \"/tmp/temp_minio_csv\"\n\
          \    df.coalesce(1).write.mode(\"overwrite\").csv(temp_folder, header=True)\n\
          \n    csv_file = [f for f in os.listdir(temp_folder) if f.startswith(\"\
          part-\")][0]\n    final_csv_path = \"/tmp/final_output.csv\"\n    shutil.move(os.path.join(temp_folder,\
          \ csv_file), final_csv_path)\n\n    # Upload to MinIO\n    current_date\
          \ = datetime.now().strftime(\"%Y-%m-%d\")\n    saving_file_name = f\"data/AI360FDREC_final_processed_{current_date}.csv\"\
          \n\n    minio_client.upload_file(final_csv_path, minio_bucket, saving_file_name)\n\
          \n    # Cleanup\n    shutil.rmtree(temp_folder)\n    os.remove(final_csv_path)\n\
          \n    print(f\"CSV successfully uploaded to MinIO at s3://{minio_bucket}/{saving_file_name}\"\
          )\n\n    return saving_file_name\n\n"
        image: quay.io/datanature_dev/jupyternotebook:java_home13
    exec-fetch-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - fetch_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'pyarrow'\
          \ 's3fs' 'boto3' 'psycopg2-binary' 'trino' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef fetch_data(minio_bucket: str) -> str:\n    import boto3\n   \
          \ import os\n    from datetime import datetime\n\n    # Initialize MinIO\
          \ client using boto3\n    minio_client = boto3.client(\n        's3',\n\
          \        endpoint_url=\"http://192.168.80.155:32000\",\n        aws_access_key_id=\"\
          admin\",\n        aws_secret_access_key=\"dlyticaD123\"\n    )\n    # from\
          \ fastapi import FastAPI\n    from trino.dbapi import connect\n    # from\
          \ trino.auth import BasicAuthentication\n    # import json\n    import csv\n\
          \n    # Trino Connection Details\n    TRINO_HOST = \"192.168.80.155\"\n\
          \    TRINO_PORT = \"30686\"\n    TRINO_USER = \"admin\"\n    # TRINO_USER\
          \ = \"your_username\"\n    # TRINO_PASSWORD = \"admin123\"\n    TRINO_CATALOG\
          \ = \"iceberg\"\n    TRINO_SCHEMA = \"gold\"\n\n    # Your SQL Query (put\
          \ the full query here)\n    SQL_QUERY = \"\"\" \n         WITH debit_credit_summary\
          \ AS (\n    SELECT\n        a.cif_id,\n        a.nepali_fiscal_year,\n \
          \       g.schm_type,\n        SUM(total_debit_tran_vol) AS dr_amount_count,\n\
          \        SUM(total_credit_tran_vol) AS cr_amount_count,\n        COUNT(DISTINCT\
          \ CASE WHEN total_debit_tran_vol > 0 THEN a.nepali_month END) AS dr_month_count,\n\
          \        COUNT(DISTINCT CASE WHEN total_credit_tran_vol > 0 THEN a.nepali_month\
          \ END) AS cr_month_count,\n        SUM(total_debit_tran_count) AS total_debit_transaction,\n\
          \        SUM(total_credit_tran_count) AS total_credit_transaction,\n   \
          \     CASE\n            WHEN COUNT(DISTINCT CASE WHEN total_debit_tran_count\
          \ > 0 THEN a.nepali_month END) > 0 THEN\n                SUM(total_debit_tran_count)\
          \ / COUNT(DISTINCT CASE WHEN total_debit_tran_count > 0 THEN a.nepali_month\
          \ END)\n        END AS average_debit_transaction_count,\n        CASE\n\
          \            WHEN COUNT(DISTINCT CASE WHEN total_credit_tran_count > 0 THEN\
          \ a.nepali_month END) > 0 THEN\n                SUM(total_credit_tran_count)\
          \ / COUNT(DISTINCT CASE WHEN total_credit_tran_count > 0 THEN a.nepali_month\
          \ END)\n        END AS average_credit_transaction_count,\n        CASE\n\
          \            WHEN COUNT(DISTINCT CASE WHEN total_debit_tran_vol > 0 THEN\
          \ a.nepali_month END) > 0 THEN\n                SUM(total_debit_tran_vol)\
          \ / COUNT(DISTINCT CASE WHEN total_debit_tran_vol > 0 THEN a.nepali_month\
          \ END)\n            ELSE 0\n        END AS dr_average,\n        CASE\n \
          \           WHEN COUNT(DISTINCT CASE WHEN total_credit_tran_vol > 0 THEN\
          \ a.nepali_month END) > 0 THEN\n                SUM(total_credit_tran_vol)\
          \ / COUNT(DISTINCT CASE WHEN total_credit_tran_vol > 0 THEN a.nepali_month\
          \ END)\n            ELSE 0\n        END AS cr_average,\n        (CASE\n\
          \            WHEN COUNT(DISTINCT CASE WHEN total_credit_tran_vol > 0 THEN\
          \ a.nepali_month END) > 0 THEN\n                SUM(total_credit_tran_vol)\
          \ / COUNT(DISTINCT CASE WHEN total_credit_tran_vol > 0 THEN a.nepali_month\
          \ END)\n            ELSE 0\n        END) - (CASE\n            WHEN COUNT(DISTINCT\
          \ CASE WHEN total_debit_tran_vol > 0 THEN a.nepali_month END) > 0 THEN\n\
          \                SUM(total_debit_tran_vol) / COUNT(DISTINCT CASE WHEN total_debit_tran_vol\
          \ > 0 THEN a.nepali_month END)\n            ELSE 0\n        END) AS average_yearly_saving,\n\
          \        SUM(total_credit_tran_vol) - SUM(total_debit_tran_vol) AS total_yearly_saving\n\
          \n    FROM\n        gold.mv_fact_deposit_account_insights AS a\n    JOIN\n\
          \        gold.dim_gam AS g ON g.cif_id = a.cif_id\n    WHERE\n        g.schm_type\
          \ = 'SBA' AND g.schm_type != 'TDA'\n    GROUP BY\n        a.cif_id, a.nepali_fiscal_year,\
          \ g.schm_type\n    HAVING\n        a.nepali_fiscal_year = '2081/2082'\n\
          \    ),\n\n    customer_info AS (\n    SELECT\n        cif_id,\n       \
          \ CAST(SUBSTR(cust_dob, 1, 4) AS INTEGER) AS birth_year,\n        EXTRACT(YEAR\
          \ FROM CURRENT_DATE) - CAST(SUBSTR(cust_dob, 1, 4) AS INTEGER) AS age,\n\
          \        CASE \n            WHEN CAST(SUBSTR(cust_dob, 1, 4) AS INTEGER)\
          \ BETWEEN 1946 AND 1964 THEN 'Baby Boomers'\n            WHEN CAST(SUBSTR(cust_dob,\
          \ 1, 4) AS INTEGER) BETWEEN 1965 AND 1980 THEN 'Gen X'\n            WHEN\
          \ CAST(SUBSTR(cust_dob, 1, 4) AS INTEGER) BETWEEN 1981 AND 1996 THEN 'Gen\
          \ Y'\n            WHEN CAST(SUBSTR(cust_dob, 1, 4) AS INTEGER) BETWEEN 1997\
          \ AND 2012 THEN 'Gen Z'\n            ELSE 'Minor' \n        END AS age_group,\n\
          \        employment_status,\n        marital_status,\n        occupation,\n\
          \        salary_per_month,\n        gender\n    FROM\n        gold.dim_customers\n\
          \    WHERE\n        cust_type = 'INDIVIDUAL'\n    ),\n\n    salary_range\
          \ AS (\n    SELECT\n        cif_id,\n        CASE\n            WHEN salary_per_month\
          \ IS NULL THEN NULL\n            WHEN salary_per_month < 10000 THEN 'low\
          \ salary'\n            WHEN salary_per_month >= 10000 AND salary_per_month\
          \ < 30000 THEN 'moderate salary'\n            ELSE 'high salary'\n     \
          \   END AS salary_range\n    FROM\n        gold.dim_customers\n    ),\n\n\
          \    withdrawal_amount_trends AS (\n    SELECT\n        cif_id,\n      \
          \  SUM(total_credit_tran_vol) AS total_credit_tran_vol,\n        CASE\n\
          \            WHEN SUM(total_credit_tran_vol) < 0 THEN 'Negative Withdrawal'\n\
          \            WHEN SUM(total_credit_tran_vol) BETWEEN 0 AND 100000 THEN 'Low\
          \ Withdrawal'\n            WHEN SUM(total_credit_tran_vol) BETWEEN 100000\
          \ AND 1000000 THEN 'Moderate Withdrawal'\n            ELSE 'High Withdrawal'\n\
          \        END AS withdrawal_trends\n    FROM\n        gold.mv_fact_deposit_account_insights\n\
          \    GROUP BY\n        cif_id\n    ),\n\n    account_details AS (\n    SELECT\n\
          \        cif_id,\n        COUNT(foracid) AS total_accounts,\n        COUNT(CASE\
          \ WHEN schm_type = 'SBA' THEN foracid END) AS total_saving_accounts,\n \
          \       COUNT(CASE WHEN schm_type = 'TDA' THEN foracid END) AS total_fixed_accounts,\n\
          \        COUNT(CASE WHEN schm_type = 'LAA' THEN foracid END) AS total_loan_accounts,\n\
          \        COUNT(CASE WHEN schm_type = 'ODA' THEN foracid END) AS total_overdraft_accounts\n\
          \    FROM\n        gold.dim_gam\n    GROUP BY\n        cif_id\n    ),\n\n\
          \    dormant_table AS (\n    SELECT \n        cif_id,\n        MAX(CAST(SUBSTR(dormant_date,\
          \ 1, 10) AS DATE)) AS last_dormant_date,\n        SUM(DATE_DIFF('day', CAST(SUBSTR(dormant_date,\
          \ 1, 10) AS DATE), CURRENT_DATE)) AS total_dormant_days,\n        COUNT(CASE\
          \ WHEN dormant_date IS NOT NULL THEN foracid END) AS total_dormant_account,\n\
          \        COUNT(CASE WHEN dormant_date IS NULL THEN foracid END) AS total_active_account\n\
          \    FROM \n        gold.dim_deposit_accounts \n    GROUP BY \n        cif_id\n\
          \    ),\n\n    first_ac_open_last_ac_open AS (\n    SELECT\n        cif_id,\n\
          \        MAX(date_diff('day', acct_opn_date_date_part, CURRENT_DATE)) AS\
          \ first_account_opened_days,\n        MIN(date_diff('day', acct_opn_date_date_part,\
          \ CURRENT_DATE)) AS last_account_opened_days,\n        MIN(acct_opn_date_date_part)\
          \ AS first_account_date,\n        MAX(acct_opn_date_date_part) AS last_account_date\n\
          \    FROM\n        gold.dim_gam\n    GROUP BY\n        cif_id\n    ),\n\n\
          \    recently_active_status AS (\n    SELECT\n        cif_id,\n        MAX(TRY_CAST(SUBSTR(last_customer_induced_transaction_date,\
          \ 1, 10) AS DATE)) AS last_transaction_date,\n        MAX(TRY_CAST(SUBSTR(dormant_date,\
          \ 1, 10) AS DATE)) AS last_dormant_date,\n        CASE\n            WHEN\
          \ MAX(TRY_CAST(SUBSTR(dormant_date, 1, 10) AS DATE)) < MAX(TRY_CAST(SUBSTR(last_customer_induced_transaction_date,\
          \ 1, 10) AS DATE)) THEN 'Y'\n            ELSE 'N'\n        END AS Recently_Active\n\
          \    FROM\n        gold.dim_deposit_accounts\n    GROUP BY\n        cif_id\n\
          \    )\n\n    SELECT DISTINCT\n    dcs.cif_id,\n    a.total_accounts,\n\
          \    a.total_saving_accounts,\n    a.total_fixed_accounts,\n    a.total_loan_accounts,\n\
          \    a.total_overdraft_accounts,\n    dcs.schm_type AS scheme_type,\n  \
          \  dcs.total_debit_transaction AS total_debit_transaction_count,\n    dcs.total_credit_transaction\
          \ AS total_credit_transaction_count,\n    dcs.average_debit_transaction_count,\n\
          \    dcs.average_credit_transaction_count,\n    dcs.dr_amount_count AS total_debit_transaction_amount,\n\
          \    dcs.cr_amount_count AS total_credit_transaction_amount,\n    dcs.dr_average\
          \ AS average_debit_amount,\n    dcs.cr_average AS average_credit_amount,\n\
          \    dcs.average_yearly_saving,\n    dcs.total_yearly_saving,\n    ci.employment_status,\n\
          \    ci.marital_status,\n    ci.occupation,\n    ci.gender,\n    ci.age_group,\n\
          \    -- sr.salary_range,\n    wat.withdrawal_trends,\n    ad.total_dormant_days,\n\
          \    ad.total_dormant_account,\n    ad.total_active_account,\n    fa.first_account_opened_days,\n\
          \    fa.last_account_opened_days,\n    fa.first_account_date,\n    fa.last_account_date,\n\
          \    ra.last_transaction_date,\n    ra.last_dormant_date,\n    ra.Recently_Active\n\
          \    FROM\n    debit_credit_summary dcs\n    JOIN customer_info ci ON dcs.cif_id\
          \ = ci.cif_id\n    JOIN withdrawal_amount_trends wat ON dcs.cif_id = wat.cif_id\n\
          \    JOIN salary_range sr ON dcs.cif_id = sr.cif_id\n    JOIN account_details\
          \ a ON dcs.cif_id = a.cif_id\n    LEFT JOIN dormant_table ad ON dcs.cif_id\
          \ = ad.cif_id\n    LEFT JOIN first_ac_open_last_ac_open fa ON dcs.cif_id\
          \ = fa.cif_id\n    LEFT JOIN recently_active_status ra ON dcs.cif_id = ra.cif_id\n\
          \    ORDER BY dcs.cif_id\n    \"\"\"\n    try:\n            print(\"Connecting\
          \ to Trino...\")\n            conn = connect(\n                host=TRINO_HOST,\n\
          \                port=TRINO_PORT,\n                user=TRINO_USER,\n  \
          \              # auth=BasicAuthentication(TRINO_USER, TRINO_PASSWORD), \n\
          \                catalog=TRINO_CATALOG,\n                schema=TRINO_SCHEMA\n\
          \                # http_scheme=\"https\"\n            )\n            cursor\
          \ = conn.cursor()\n\n            print(\"Executing query...\")\n       \
          \     cursor.execute(SQL_QUERY)\n\n            # Fetch data\n          \
          \  rows = cursor.fetchall()\n            columns = [desc[0] for desc in\
          \ cursor.description]\n\n            conn.close()\n            print(\"\
          Query executed successfully.\")\n\n            # Convert to JSON\n     \
          \       results = [dict(zip(columns, row)) for row in rows]\n\n        \
          \    # Save JSON file\n              # Save to CSV file\n            output_file\
          \ = \"trino_data.csv\"\n            with open(output_file, \"w\", newline=\"\
          \") as f:\n                writer = csv.writer(f)\n                writer.writerow(columns)\
          \  # Write header\n                writer.writerows(rows)    # Write data\
          \ rows\n\n            print(f\"Data saved to {output_file}\")\n        \
          \    # return output_file  # Return the file path\n\n    except Exception\
          \ as e:\n            print(f\"Error: {str(e)}\")\n            return None\n\
          \    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n    saving_file_name\
          \ = f\"data/AI360FDREC_raw_data_{current_date}.csv\"\n    os.makedirs(saving_file_name,\
          \ exist_ok=True)\n    minio_client.upload_file(output_file, minio_bucket,\
          \ saving_file_name)\n\n    print(\"CSV file uploaded to MinIO successfully.\"\
          )\n    return saving_file_name\n\n"
        image: bitnami/spark:3.5
    exec-predict-new-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - predict_new_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef predict_new_data( minio_bucket: str, \n    processed_data: str,\
          \  # Fix: Expect string path\n    trained_model: str,\n    pipeline_model:\
          \ str\n    )->None:\n    import boto3\n    import shutil\n    import os\n\
          \    from pyspark.sql import SparkSession\n    from pyspark.ml import PipelineModel\n\
          \    from pyspark.ml.classification import LogisticRegressionModel\n   \
          \ from pyspark.sql import Row\n    from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\
          \    import boto3\n    import shutil\n    import os\n    from pyspark.sql\
          \ import SparkSession\n    from pyspark.ml import PipelineModel\n    from\
          \ pyspark.ml.classification import LogisticRegressionModel\n    from pyspark.sql.functions\
          \ import lit, col, when\n\n    # Initialize Spark session\n     # variables\n\
          \    # variables\n    access_key_id = 'admin'\n    secret_access_key = 'dlyticaD123'\n\
          \    minio_endpoint = 'dn-minio-tenant-hl.dn-minio-tenant.svc.cluster.local:9000'\
          \  # Replace with your MinIO server endpoint\n    data_bucket = 'ctzn360'\n\
          \    # apps_bucket = 'dn-apps'\n    hive_metastore_uri = \"thrift://dn-hive-metastore.dn-hive-metastore.svc.cluster.local:9083\"\
          \n    iceberg_warehouse_location = f\"s3a://{data_bucket}/data/\"\n    custom_catalog\
          \ = \"iceberg_catalog\"\n    # spark_eventlog_dir = f\"s3a://{apps_bucket}/logs/spark/\"\
          \n    # master_url=\"spark://dn-spark-master-svc.dn-spark.svc.cluster.local:7077\"\
          \n    # spark_driver_host=\"notebook-five-spark-driver-headless.kubeflow-user-example-com.svc.cluster.local\"\
          \n    app_name=\"All Data Generation\"\n    additional_packages=\"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.spark:spark-avro_2.12:3.5.0,com.crealytics:spark-excel_2.12:0.13.7,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.hadoop:hadoop-common:3.3.4\"\
          \n\n    spark = SparkSession.builder \\\n    .appName(app_name) \\\n   \
          \ .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\"\
          ) \\\n    .config(\"spark.hadoop.fs.s3a.endpoint\", minio_endpoint) \\\n\
          \    .config(\"spark.hadoop.fs.s3a.access.key\", access_key_id) \\\n   \
          \ .config(\"spark.hadoop.fs.s3a.secret.key\", secret_access_key) \\\n  \
          \  .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n   \
          \ .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n\
          \    .config(\"spark.hadoop.hive.metastore.uris\", hive_metastore_uri) \\\
          \n    .config(\"spark.sql.warehouse.dir\", iceberg_warehouse_location) \\\
          \n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\"\
          ) \\\n    .config(f\"spark.sql.catalog.{custom_catalog}\", \"org.apache.iceberg.spark.SparkCatalog\"\
          ) \\\n    .config(\"spark.jars.packages\", additional_packages)\\\n    .config(\"\
          spark.sql.repl.eagerEval.enabled\", True)\\\n    .config(\"spark.sql.debug.maxToStringFields\"\
          , 1000) \\\n    .enableHiveSupport() \\\n    .getOrCreate()\n\n\n    # Initialize\
          \ MinIO client\n    minio_client = boto3.client(\n        's3',\n      \
          \  endpoint_url=\"http://192.168.80.155:32000\",\n        aws_access_key_id=\"\
          admin\",\n        aws_secret_access_key=\"dlyticaD123\"\n    )\n\n    local_raw_data_path\
          \ = \"/tmp/raw_data.csv\"\n    minio_client.download_file(minio_bucket,\
          \ processed_data, local_raw_data_path)\n\n    # Read CSV file into Spark\
          \ DataFrame\n    data = spark.read.csv(local_raw_data_path, header=True,\
          \ inferSchema=True)\n    data.show()\n\n\n    numerical_col = [\n      \
          \  'total_accounts', 'total_saving_accounts', 'total_fixed_accounts', 'total_loan_accounts',\n\
          \        'total_overdraft_accounts', 'total_debit_transaction_count', 'total_credit_transaction_count',\n\
          \        'total_debit_transaction_amount', 'total_credit_transaction_amount',\
          \ 'average_debit_transaction_count',\n        'average_credit_transaction_count',\
          \ 'average_debit_amount', 'average_credit_amount',\n        'average_yearly_saving',\
          \ 'total_yearly_saving', 'total_dormant_days', 'total_dormant_account',\n\
          \        'total_active_account', 'first_account_opened_days', 'last_account_opened_days','recommendation_fd_target'\n\
          \    ]\n\n    # Convert each column to `double`, replacing non-numeric values\
          \ with NULL\n    for col_name in numerical_col:\n        data = data.withColumn(\n\
          \            col_name,\n            when(col(col_name).rlike(\"^[0-9]+(\\\
          \\.[0-9]+)?$\"), col(col_name).cast(\"double\"))\n            .otherwise(None)\
          \  # Convert invalid values (e.g., text) to NULL\n        )\n\n    categorical_cols\
          \ = ['gender', 'employment_status', 'marital_status', 'occupation', 'age_group',\
          \ 'withdrawal_trends']\n    numerical_cols = [\n        'total_accounts',\
          \ 'total_saving_accounts', 'total_fixed_accounts', 'total_loan_accounts',\n\
          \        'total_overdraft_accounts', 'total_debit_transaction_count', 'total_credit_transaction_count',\n\
          \        'total_debit_transaction_amount', 'total_credit_transaction_amount',\
          \ 'average_debit_transaction_count',\n        'average_credit_transaction_count',\
          \ 'average_debit_amount', 'average_credit_amount',\n        'average_yearly_saving',\
          \ 'total_yearly_saving', 'total_dormant_days', 'total_dormant_account',\n\
          \        'total_active_account', 'first_account_opened_days', 'last_account_opened_days','recommendation_fd_target'\n\
          \    ]\n\n    # Handle missing values:\n    data = data.fillna(0, subset=numerical_cols)\n\
          \    data = data.fillna(\"Unknown\", subset=categorical_cols)\n\n    # Define\
          \ paths\n    local_model_zip = \"/tmp/lr_model.zip\"\n    local_pipeline_zip\
          \ = \"/tmp/fitted_pipeline_model.zip\"\n    local_model_dir = \"/tmp/lr_model\"\
          \n    local_pipeline_dir = \"/tmp/fitted_pipeline_model\"\n    trained_model\
          \ = trained_model + \".zip\"\n    pipeline_model = pipeline_model + \".zip\"\
          \n    print(trained_model)\n    # Download model and pipeline from MinIO\
          \ using the paths from `train_model`\n    minio_client.download_file(minio_bucket,\
          \ trained_model, local_model_zip)\n    minio_client.download_file(minio_bucket,\
          \ pipeline_model, local_pipeline_zip)\n\n    # Unzip model and pipeline\n\
          \    shutil.unpack_archive(local_model_zip, local_model_dir)\n    shutil.unpack_archive(local_pipeline_zip,\
          \ local_pipeline_dir)\n\n    # Load the fitted pipeline and model\n    fitted_pipeline_model\
          \ = PipelineModel.load(local_pipeline_dir)\n    loaded_lr_model = LogisticRegressionModel.load(local_model_dir)\n\
          \n    # Assign artifact paths to output\n    fitted_pipeline_model.uri =\
          \ local_pipeline_dir\n    loaded_lr_model.uri = local_model_dir\n    print(\"\
          Model and pipeline successfully loaded!\")\n    # Create new sample data\n\
          \n\n    # Convert to DataFrame\n    # new_data_df = spark.createDataFrame(new_data)\n\
          \n    # Apply the fitted pipeline to transform new data\n    processed_new_data\
          \ = fitted_pipeline_model.transform(data)\n\n    # Make predictions\n\n\
          \    predictions = loaded_lr_model.transform(processed_new_data)\n    df_selected\
          \ = predictions.select(\"cif_id\", \"prediction\")\n    df_selected.writeTo(f\"\
          {custom_catalog}.gold.predictions\").createOrReplace()\n    # # Evaluate\n\
          \    # evaluator = BinaryClassificationEvaluator(labelCol=\"recommendation_fd_target\"\
          , metricName=\"areaUnderROC\")\n    # roc_auc = evaluator.evaluate(predictions)\n\
          \    # print(f\"ROC AUC: {roc_auc}\")\n\n    # Display predictions\n   \
          \ predictions.select(\"cif_id\", \"probability\", \"prediction\").show(truncate=False)\n\
          \n    return None\n\n"
        image: quay.io/datanature_dev/jupyternotebook:java_home13
    exec-train-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_model(feature_data_path: str, minio_bucket: str) -> NamedTuple('Outputs',\
          \ [('model_path', str), ('pipeline_path', str)]):\n    import pandas as\
          \ pd\n    from pyspark.sql import SparkSession\n    from pyspark.ml.feature\
          \ import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n\
          \    from pyspark.ml import Pipeline\n    from pyspark.ml.classification\
          \ import LogisticRegression\n    from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\
          \    import boto3\n    import os\n    import shutil\n    from datetime import\
          \ datetime\n    from pyspark.sql.functions import lit, col, when\n\n   \
          \ # variables\n    # variables\n    # access_key_id = 'admin'\n    # secret_access_key\
          \ = 'dlyticaD123'\n    # minio_endpoint = 'dn-minio-tenant-hl.dn-minio-tenant.svc.cluster.local:9000'\
          \  # Replace with your MinIO server endpoint\n    # data_bucket = 'ai360fd-recommendation'\n\
          \    # apps_bucket = ''\n    # hive_metastore_uri = \"thrift://dn-hive-metastore.dn-hive-metastore.svc.cluster.local:9083\"\
          \n    # iceberg_warehouse_location = f\"s3a://{data_bucket}/data/\"\n  \
          \  # custom_catalog = \"iceberg_catalog\"\n    # spark_eventlog_dir = f\"\
          s3a://{apps_bucket}/logs/spark/\"\n    # master_url=\"spark://dn-spark-master-svc.dn-spark.svc.cluster.local:7077\"\
          \n    # spark_driver_host=\"notebook-five-spark-driver-headless.kubeflow-user-example-com.svc.cluster.local\"\
          \n    # app_name=\"All Data Generation\"\n    # additional_packages=\"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.spark:spark-avro_2.12:3.5.0,com.crealytics:spark-excel_2.12:0.13.7,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.hadoop:hadoop-common:3.3.4\"\
          \n\n    access_key_id = 'admin'\n    secret_access_key = 'dlyticaD123'\n\
          \    minio_endpoint = 'dn-minio-tenant-hl.dn-minio-tenant.svc.cluster.local:9000'\
          \  # Replace with your MinIO server endpoint\n    data_bucket = 'ai360fd-recommendation'\n\
          \    # apps_bucket = 'dn-apps'\n    hive_metastore_uri = \"thrift://dn-hive-metastore.dn-hive-metastore.svc.cluster.local:9083\"\
          \n    iceberg_warehouse_location = f\"s3a://{data_bucket}/data/\"\n    custom_catalog\
          \ = \"iceberg_catalog\"\n    # spark_eventlog_dir = f\"s3a://{apps_bucket}/logs/spark/\"\
          \n    # master_url=\"spark://dn-spark-master-svc.dn-spark.svc.cluster.local:7077\"\
          \n    # spark_driver_host=\"notebook-five-spark-driver-headless.kubeflow-user-example-com.svc.cluster.local\"\
          \n    app_name=\"All Data Generation\"\n    additional_packages=\"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.spark:spark-avro_2.12:3.5.0,com.crealytics:spark-excel_2.12:0.13.7,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.hadoop:hadoop-common:3.3.4\"\
          \n\n    spark = SparkSession.builder \\\n        .appName(app_name) \\\n\
          \        .config(\"spark.driver.memory\", \"8g\") \\\n        .config(\"\
          spark.executor.memory\", \"8g\") \\\n        .config(\"spark.executor.memoryOverhead\"\
          , \"2g\") \\\n        .config(\"spark.driver.maxResultSize\", \"2g\") \\\
          \n        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\"\
          ) \\\n        .config(\"spark.hadoop.fs.s3a.endpoint\", minio_endpoint)\
          \ \\\n        .config(\"spark.hadoop.fs.s3a.access.key\", access_key_id)\
          \ \\\n        .config(\"spark.hadoop.fs.s3a.secret.key\", secret_access_key)\
          \ \\\n        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\"\
          ) \\\n        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"\
          false\") \\\n        .config(\"spark.hadoop.hive.metastore.uris\", hive_metastore_uri)\
          \ \\\n        .config(\"spark.sql.warehouse.dir\", iceberg_warehouse_location)\
          \ \\\n        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\"\
          ) \\\n        .config(f\"spark.sql.catalog.{custom_catalog}\", \"org.apache.iceberg.spark.SparkCatalog\"\
          ) \\\n        .config(\"spark.jars.packages\", additional_packages) \\\n\
          \        .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n      \
          \  .config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n        .enableHiveSupport()\
          \ \\\n        .getOrCreate()\n\n    # spark.sql(f\"\"\"\n    # show tables\
          \ in {custom_catalog}.gold\n    # \"\"\").show(100,False)\n    # print(\"\
          train_model is called\")\n    # Initialize Spark session\n    # spark =\
          \ SparkSession.builder.appName(\"Feature Engineering\").getOrCreate()\n\n\
          \    # Initialize MinIO client using boto3\n    minio_client = boto3.client(\n\
          \        's3',\n        endpoint_url=\"http://192.168.80.155:32000\",\n\
          \        aws_access_key_id=\"admin\",\n        aws_secret_access_key=\"\
          dlyticaD123\"\n    )\n\n    # Download the raw data from MinIO\n    local_raw_data_path\
          \ = \"/tmp/raw_data.csv\"\n    minio_client.download_file(minio_bucket,\
          \ feature_data_path, local_raw_data_path)\n\n    # Read CSV file into Spark\
          \ DataFrame\n    data = spark.read.csv(local_raw_data_path, header=True,\
          \ inferSchema=True)\n    data.show()\n    # Define numerical columns\n \
          \   numerical_col = [\n        'total_accounts', 'total_saving_accounts',\
          \ 'total_fixed_accounts', 'total_loan_accounts',\n        'total_overdraft_accounts',\
          \ 'total_debit_transaction_count', 'total_credit_transaction_count',\n \
          \       'total_debit_transaction_amount', 'total_credit_transaction_amount',\
          \ 'average_debit_transaction_count',\n        'average_credit_transaction_count',\
          \ 'average_debit_amount', 'average_credit_amount',\n        'average_yearly_saving',\
          \ 'total_yearly_saving', 'total_dormant_days', 'total_dormant_account',\n\
          \        'total_active_account', 'first_account_opened_days', 'last_account_opened_days','recommendation_fd_target'\n\
          \    ]\n\n    # Convert each column to `double`, replacing non-numeric values\
          \ with NULL\n    for col_name in numerical_col:\n        data = data.withColumn(\n\
          \            col_name,\n            when(col(col_name).rlike(\"^[0-9]+(\\\
          \\.[0-9]+)?$\"), col(col_name).cast(\"double\"))\n            .otherwise(None)\
          \  # Convert invalid values (e.g., text) to NULL\n        )\n\n    # Define\
          \ categorical and numerical columns\n    categorical_cols = ['gender', 'employment_status',\
          \ 'marital_status', 'occupation', 'age_group', 'withdrawal_trends']\n  \
          \  numerical_cols = [\n        'total_accounts', 'total_saving_accounts',\
          \ 'total_fixed_accounts', 'total_loan_accounts',\n        'total_overdraft_accounts',\
          \ 'total_debit_transaction_count', 'total_credit_transaction_count',\n \
          \       'total_debit_transaction_amount', 'total_credit_transaction_amount',\
          \ 'average_debit_transaction_count',\n        'average_credit_transaction_count',\
          \ 'average_debit_amount', 'average_credit_amount',\n        'average_yearly_saving',\
          \ 'total_yearly_saving', 'total_dormant_days', 'total_dormant_account',\n\
          \        'total_active_account', 'first_account_opened_days', 'last_account_opened_days','recommendation_fd_target'\n\
          \    ]\n\n    # Handle missing values:\n    data = data.fillna(0, subset=numerical_cols)\n\
          \    data = data.fillna(\"Unknown\", subset=categorical_cols)\n\n    # Preprocessing\
          \ pipeline\n    indexers = [StringIndexer(inputCol=col, outputCol=col +\
          \ \"_index\", handleInvalid=\"keep\") for col in categorical_cols]\n   \
          \ encoders = [OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"\
          _vec\") for col in categorical_cols]\n    numerical_assembler = VectorAssembler(inputCols=numerical_cols,\
          \ outputCol=\"numerical_features\")\n    scaler = StandardScaler(inputCol=\"\
          numerical_features\", outputCol=\"scaled_numerical_features\")\n    all_features\
          \ = [col + \"_vec\" for col in categorical_cols] + [\"scaled_numerical_features\"\
          ]\n    feature_assembler = VectorAssembler(inputCols=all_features, outputCol=\"\
          features\", handleInvalid=\"skip\")\n\n    pipeline = Pipeline(stages=indexers\
          \ + encoders + [numerical_assembler, scaler, feature_assembler])\n\n   \
          \ # Fit the pipeline\n    fitted_pipeline = pipeline.fit(data)\n    processed_data\
          \ = fitted_pipeline.transform(data)\n\n    # Save processed data as Parquet\
          \ instead of CSV\n    local_processed_data_path = \"/tmp/processed_data.parquet\"\
          \n    processed_data.write.mode(\"overwrite\").parquet(local_processed_data_path)\n\
          \n    # Upload processed data to MinIO\n    processed_data_zip = \"/tmp/processed_data.zip\"\
          \n    shutil.make_archive(\"/tmp/processed_data\", 'zip', \"/tmp/processed_data.parquet\"\
          )\n    minio_client.upload_file(processed_data_zip, minio_bucket, \"data/processed_data.zip\"\
          )\n\n    # Train model\n    train_data, test_data = processed_data.randomSplit([0.8,\
          \ 0.2], seed=42)\n    lr = LogisticRegression(featuresCol=\"features\",\
          \ labelCol=\"recommendation_fd_target\")\n    lr_model = lr.fit(train_data)\n\
          \n    current_datetime = datetime.now().strftime(\"%Y-%m-%d\")\n    training_data_path\
          \ = f\"data/AI360FDREC_training_dataset_{current_datetime}.zip\"\n    test_data_path\
          \ = f\"data/AI360FDREC_test_dataset_{current_datetime}.zip\"\n    #path\
          \ to store training data in local directory\n    local_training_data_path\
          \  = \"/tmp/train_data.parquet\"\n    train_data.write.mode(\"overwrite\"\
          ).parquet(local_training_data_path)\n\n    #uploading trainig data into\
          \ minio\n    training_data_zip = \"/tmp/train_data.zip\"\n    shutil.make_archive(\"\
          /tmp/train_data\", 'zip', local_training_data_path)\n    minio_client.upload_file(training_data_zip,\
          \ minio_bucket, training_data_path)\n\n    #path to store test data in local\n\
          \    local_test_data_path = \"/tmp/test_data.parquet\"\n    test_data.write.mode(\"\
          overwrite\").parquet(local_test_data_path)\n\n    #uploading test data into\
          \ minio\n    processed_data_zip = \"/tmp/test_data.zip\"\n    shutil.make_archive(\"\
          /tmp/test_data\", 'zip', local_test_data_path)\n    minio_client.upload_file(processed_data_zip,\
          \ minio_bucket, test_data_path)\n    # current_time = datetime.now().\n\
          \    # Evaluate the model\n    predictions = lr_model.transform(test_data)\n\
          \    print('hello')\n\n    #selecting  cif_id and predictions from the dataframe\n\
          \    df_selected = predictions.select(\"cif_id\", \"prediction\")\n\n  \
          \  # writin into the predictions of gold schema in iceberg catalog format\n\
          \    df_selected.writeTo(f\"{custom_catalog}.gold.predictions\").createOrReplace()\n\
          \n    # spark.sql(f\"\"\"\n    # select * from {custom_catalog}.gold.predictions\n\
          \    # \"\"\").show(100,False)\n\n    # Try to fetch all the tables from\
          \ the \"gold\" schema in the \"iceberg\" catalog\n    # df = spark.sql(f\"\
          SHOW TABLES IN {custom_catalog}.gold\")\n    # df.show()\n    # # Try to\
          \ fetch all the tables from the \"gold\" schema in the \"iceberg\" catalog\n\
          \    # df = spark.sql(f\"select * from {custom_catalog}.gold.predictions\"\
          )\n    # df.show()\n\n    predictions.count()\n    evaluator = BinaryClassificationEvaluator(labelCol=\"\
          recommendation_fd_target\", metricName=\"areaUnderROC\")\n    roc_auc =\
          \ evaluator.evaluate(predictions)\n    print(f\"ROC AUC: {roc_auc}\")\n\n\
          \    # predictions.filter(predictions.prediction == 1).select(\"cif_id\"\
          , \"recommendation_fd_target\", \"probability\", \"prediction\").show(1000,\
          \ truncate=False)\n\n    current_date = datetime.now().strftime(\"%Y-%m-%d\"\
          )\n    saving_model_name = f\"/tmp/AI360FDREC_MODEL_{current_date}\"\n \
          \   saving_model_name1 = f\"models/AI360FDREC_MODEL_{current_date}\"\n \
          \   saving_model_name_zip = f\"models/AI360FDREC_MODEL_{current_date}.zip\"\
          \n    saving_model_name_zip_constant = f\"models/AI360FDREC_MODEL.zip\"\n\
          \    saving_pipeline_name = f\"/tmp/AI360FDREC_PIPELINE_{current_date}\"\
          \n    saving_pipeline_name1 = f\"models/AI360FDREC_PIPELINE_{current_date}\"\
          \n    saving_pipeline_name_zip = f\"models/AI360FDREC_PIPELINE_{current_date}.zip\"\
          \n    saving_pipeline_name_zip_constant = f\"models/AI360FDREC_PIPELINE.zip\"\
          \n    os.makedirs(saving_model_name, exist_ok=True)\n    os.makedirs(saving_pipeline_name,\
          \ exist_ok=True)\n    # Save model to MinIO\n    local_model_dir = '/tmp/lr_model'\n\
          \n    # Ensure old model directory is removed\n    if os.path.exists(local_model_dir):\n\
          \        shutil.rmtree(local_model_dir)\n\n    lr_model.save(local_model_dir)\n\
          \n    # Save fitted pipeline model\n    local_pipeline_dir = \"/tmp/fitted_pipeline_model\"\
          \n    if os.path.exists(local_pipeline_dir):\n        shutil.rmtree(local_pipeline_dir)\n\
          \n    fitted_pipeline.save(local_pipeline_dir)\n\n    # Zip and upload model\n\
          \    model_zip_path = \"/tmp/lr_model.zip\"\n    shutil.make_archive('/tmp/lr_model',\
          \ 'zip', local_model_dir)\n    minio_client.upload_file(model_zip_path,\
          \ minio_bucket, saving_model_name_zip)\n    minio_client.upload_file(model_zip_path,\
          \ minio_bucket, saving_model_name_zip_constant)\n\n    # Zip and upload\
          \ pipeline\n    pipeline_zip_path = \"/tmp/fitted_pipeline_model.zip\"\n\
          \    shutil.make_archive(\"/tmp/fitted_pipeline_model\", 'zip', local_pipeline_dir)\n\
          \    minio_client.upload_file(pipeline_zip_path, minio_bucket, saving_pipeline_name_zip)\n\
          \    minio_client.upload_file(pipeline_zip_path, minio_bucket, saving_pipeline_name_zip_constant)\n\
          \n    model_url = saving_model_name1\n    pipeline_url = saving_pipeline_name1\n\
          \n    print(f\"Uploaded model to: {model_url}\")\n    print(f\"Uploaded\
          \ pipeline to: {pipeline_url}\")\n\n    return model_url,pipeline_url\n\n"
        image: quay.io/datanature_dev/jupyternotebook:java_home14
pipelineInfo:
  name: fixed-deposit-recommendation-pipeline
root:
  dag:
    tasks:
      feature-engineering:
        cachingOptions: {}
        componentRef:
          name: comp-feature-engineering
        dependentTasks:
        - fetch-data
        inputs:
          parameters:
            file_path:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: fetch-data
            minio_bucket:
              componentInputParameter: minio_bucket
        taskInfo:
          name: feature-engineering
      fetch-data:
        cachingOptions: {}
        componentRef:
          name: comp-fetch-data
        inputs:
          parameters:
            minio_bucket:
              componentInputParameter: minio_bucket
        taskInfo:
          name: fetch-data
      predict-new-data:
        cachingOptions: {}
        componentRef:
          name: comp-predict-new-data
        dependentTasks:
        - feature-engineering
        - train-model
        inputs:
          parameters:
            minio_bucket:
              componentInputParameter: minio_bucket
            pipeline_model:
              taskOutputParameter:
                outputParameterKey: pipeline_path
                producerTask: train-model
            processed_data:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: feature-engineering
            trained_model:
              taskOutputParameter:
                outputParameterKey: model_path
                producerTask: train-model
        taskInfo:
          name: predict-new-data
      train-model:
        cachingOptions: {}
        componentRef:
          name: comp-train-model
        dependentTasks:
        - feature-engineering
        inputs:
          parameters:
            feature_data_path:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: feature-engineering
            minio_bucket:
              componentInputParameter: minio_bucket
        taskInfo:
          name: train-model
  inputDefinitions:
    parameters:
      minio_bucket:
        defaultValue: ai360fd-recommendation
        isOptional: true
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.12.1

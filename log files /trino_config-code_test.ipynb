{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c6cad29-380a-4021-899f-cbc0082c0f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp in /opt/conda/lib/python3.11/site-packages (2.12.1)\n",
      "Requirement already satisfied: minio in /opt/conda/lib/python3.11/site-packages (7.2.15)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.37.22-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: click<9,>=8.0.0 in /opt/conda/lib/python3.11/site-packages (from kfp) (8.1.8)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.11/site-packages (from kfp) (0.16)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.11/site-packages (from kfp) (2.24.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /opt/conda/lib/python3.11/site-packages (from kfp) (2.38.0)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in /opt/conda/lib/python3.11/site-packages (from kfp) (2.19.0)\n",
      "Requirement already satisfied: kfp-pipeline-spec==0.6.0 in /opt/conda/lib/python3.11/site-packages (from kfp) (0.6.0)\n",
      "Requirement already satisfied: kfp-server-api<2.5.0,>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from kfp) (2.4.0)\n",
      "Requirement already satisfied: kubernetes<31,>=8.0.0 in /opt/conda/lib/python3.11/site-packages (from kfp) (30.1.0)\n",
      "Requirement already satisfied: protobuf<5,>=4.21.1 in /opt/conda/lib/python3.11/site-packages (from kfp) (4.25.6)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.11/site-packages (from kfp) (6.0.1)\n",
      "Requirement already satisfied: requests-toolbelt<2,>=0.8.0 in /opt/conda/lib/python3.11/site-packages (from kfp) (1.0.0)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.11/site-packages (from kfp) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2.0.0 in /opt/conda/lib/python3.11/site-packages (from kfp) (1.26.20)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from minio) (2024.6.2)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.11/site-packages (from minio) (23.1.0)\n",
      "Requirement already satisfied: pycryptodome in /opt/conda/lib/python3.11/site-packages (from minio) (3.22.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from minio) (4.12.2)\n",
      "Collecting numpy>=1.23.2 (from pandas)\n",
      "  Using cached numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting botocore<1.38.0,>=1.37.22 (from boto3)\n",
      "  Downloading botocore-1.37.22-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3)\n",
      "  Using cached s3transfer-0.11.4-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/conda/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.69.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/conda/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.26.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /opt/conda/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from google-auth<3,>=1.6.1->kfp) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from google-auth<3,>=1.6.1->kfp) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.11/site-packages (from google-auth<3,>=1.6.1->kfp) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-storage<3,>=2.2.1->kfp) (2.4.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in /opt/conda/lib/python3.11/site-packages (from google-cloud-storage<3,>=2.2.1->kfp) (2.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-storage<3,>=2.2.1->kfp) (1.7.1)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.11/site-packages (from kfp-server-api<2.5.0,>=2.1.0->kfp) (1.16.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.11/site-packages (from kubernetes<31,>=8.0.0->kfp) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.11/site-packages (from kubernetes<31,>=8.0.0->kfp) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.11/site-packages (from kubernetes<31,>=8.0.0->kfp) (3.2.2)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.11/site-packages (from argon2-cffi->minio) (21.2.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.7)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from argon2-cffi-bindings->argon2-cffi->minio) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->minio) (2.22)\n",
      "Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Downloading boto3-1.37.22-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.6/139.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.37.22-py3-none-any.whl (13.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "Using cached s3transfer-0.11.4-py3-none-any.whl (84 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: tzdata, numpy, jmespath, pandas, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.37.22 botocore-1.37.22 jmespath-1.0.1 numpy-2.2.4 pandas-2.2.3 s3transfer-0.11.4 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install kfp minio pandas boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28875818-9641-4358-b46f-5d42bf87e917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/07 09:33:25 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import boto3\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import lit, col, when\n",
    "\n",
    "# variables\n",
    "# variables\n",
    "# access_key_id = 'admin'\n",
    "# secret_access_key = 'dlyticaD123'\n",
    "# minio_endpoint = 'dn-minio-tenant-hl.dn-minio-tenant.svc.cluster.local:9000'  # Replace with your MinIO server endpoint\n",
    "# data_bucket = 'ai360fd-recommendation'\n",
    "# apps_bucket = ''\n",
    "# hive_metastore_uri = \"thrift://dn-hive-metastore.dn-hive-metastore.svc.cluster.local:9083\"\n",
    "# iceberg_warehouse_location = f\"s3a://{data_bucket}/data/\"\n",
    "# custom_catalog = \"iceberg_catalog\"\n",
    "# spark_eventlog_dir = f\"s3a://{apps_bucket}/logs/spark/\"\n",
    "# master_url=\"spark://dn-spark-master-svc.dn-spark.svc.cluster.local:7077\"\n",
    "# spark_driver_host=\"notebook-five-spark-driver-headless.kubeflow-user-example-com.svc.cluster.local\"\n",
    "# app_name=\"All Data Generation\"\n",
    "# additional_packages=\"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.spark:spark-avro_2.12:3.5.0,com.crealytics:spark-excel_2.12:0.13.7,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.hadoop:hadoop-common:3.3.4\"\n",
    "\n",
    "access_key_id = 'admin'\n",
    "secret_access_key = 'dlyticaD123'\n",
    "minio_endpoint = 'dn-minio-tenant-hl.dn-minio-tenant.svc.cluster.local:9000'  # Replace with your MinIO server endpoint\n",
    "data_bucket = 'ai360fd-recommendation'\n",
    "# apps_bucket = 'dn-apps'\n",
    "hive_metastore_uri = \"thrift://dn-hive-metastore.dn-hive-metastore.svc.cluster.local:9083\"\n",
    "iceberg_warehouse_location = f\"s3a://{data_bucket}/data/\"\n",
    "custom_catalog = \"iceberg_catalog\"\n",
    "# spark_eventlog_dir = f\"s3a://{apps_bucket}/logs/spark/\"\n",
    "# master_url=\"spark://dn-spark-master-svc.dn-spark.svc.cluster.local:7077\"\n",
    "# spark_driver_host=\"notebook-five-spark-driver-headless.kubeflow-user-example-com.svc.cluster.local\"\n",
    "app_name=\"All Data Generation\"\n",
    "additional_packages=\"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.spark:spark-avro_2.12:3.5.0,com.crealytics:spark-excel_2.12:0.13.7,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.hadoop:hadoop-common:3.3.4\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(app_name) \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", minio_endpoint) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", access_key_id) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", secret_access_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.hive.metastore.uris\", hive_metastore_uri) \\\n",
    "    .config(\"spark.sql.warehouse.dir\", iceberg_warehouse_location) \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "    .config(f\"spark.sql.catalog.{custom_catalog}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", additional_packages) \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# spark.sql(f\"\"\"\n",
    "# show tables in {custom_catalog}.gold\n",
    "# \"\"\").show(100,False)\n",
    "# print(\"train_model is called\")\n",
    "# Initialize Spark session\n",
    "# spark = SparkSession.builder.appName(\"Feature Engineering\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66c10bfa-4d7f-492c-8eac-fe8d1c281f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------------------+-----------+\n",
      "|namespace|tableName                        |isTemporary|\n",
      "+---------+---------------------------------+-----------+\n",
      "|gold     |customer                         |false      |\n",
      "|gold     |dim_mob_profile                  |false      |\n",
      "|gold     |dim_mob_request_info             |false      |\n",
      "|gold     |dim_mob_bank_account             |false      |\n",
      "|gold     |dim_mob_customer_login           |false      |\n",
      "|gold     |dim_mob_application_user         |false      |\n",
      "|gold     |dim_mob_customers                |false      |\n",
      "|gold     |dim_mob_merchant                 |false      |\n",
      "|gold     |dim_communications               |false      |\n",
      "|gold     |fact_eod_balance_details         |false      |\n",
      "|gold     |fact_interest_rate_change_history|false      |\n",
      "|gold     |dim_mob_logged_in_activity       |false      |\n",
      "|gold     |dim_lien                         |false      |\n",
      "|gold     |currency_rates                   |false      |\n",
      "|gold     |fact_transaction                 |false      |\n",
      "|gold     |transaction_with_type            |false      |\n",
      "|gold     |dim_product                      |false      |\n",
      "|gold     |rec_final_table                  |false      |\n",
      "|gold     |exchange_rate                    |false      |\n",
      "|gold     |dim_customers_with_avg_balance   |false      |\n",
      "|gold     |dim_customers                    |false      |\n",
      "|gold     |dim_average_balance              |false      |\n",
      "|gold     |dim_signatories__dbt_tmp         |false      |\n",
      "|gold     |predictions                      |false      |\n",
      "|gold     |dim_customer_legal_entity_details|false      |\n",
      "|gold     |dim_branch                       |false      |\n",
      "|gold     |dim_gam                          |false      |\n",
      "|gold     |dim_cust_address                 |false      |\n",
      "|gold     |dim_document                     |false      |\n",
      "|gold     |fd_and_mob_prediction            |false      |\n",
      "|gold     |dim_nominee                      |false      |\n",
      "|gold     |dim_signatories                  |false      |\n",
      "|gold     |fact_mob_login_access_channel    |false      |\n",
      "|gold     |fact_mob_transactions            |false      |\n",
      "|gold     |fact_mob_transactions_iso        |false      |\n",
      "|gold     |exchange_rate_all                |false      |\n",
      "|gold     |dim_date                         |false      |\n",
      "|gold     |dim_deposit_accounts             |false      |\n",
      "|gold     |roles                            |false      |\n",
      "|gold     |customer_segments                |false      |\n",
      "|gold     |fact_transaction_details         |false      |\n",
      "|gold     |customer_segments1               |false      |\n",
      "+---------+---------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "show tables in {custom_catalog}.gold\n",
    "\"\"\").show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c14a8100-07bb-498a-8b35-2caa31c75d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/07 09:36:06 WARN CheckAllocator: More than one DefaultAllocationManager on classpath. Choosing first found\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>cif_id</th><th>cluster_id</th><th>intreapretation</th></tr>\n",
       "<tr><td>R000372359</td><td>6</td><td>Moderate Spender;...</td></tr>\n",
       "<tr><td>R000371858</td><td>4</td><td>Moderate Spender;...</td></tr>\n",
       "<tr><td>R000372322</td><td>6</td><td>Moderate Spender;...</td></tr>\n",
       "<tr><td>R000371446</td><td>6</td><td>Moderate Spender;...</td></tr>\n",
       "<tr><td>R000370704</td><td>6</td><td>Moderate Spender;...</td></tr>\n",
       "<tr><td>R000371574</td><td>3</td><td>Moderate Spender;...</td></tr>\n",
       "<tr><td>R000372430</td><td>6</td><td>Moderate Spender;...</td></tr>\n",
       "<tr><td>R000370804</td><td>9</td><td>Moderate Spender;...</td></tr>\n",
       "<tr><td>R000371010</td><td>1</td><td>Moderate Spender;...</td></tr>\n",
       "<tr><td>R000371018</td><td>6</td><td>Moderate Spender;...</td></tr>\n",
       "<tr><td>R000371531</td><td>6</td><td>Moderate Spender;...</td></tr>\n",
       "<tr><td>R000370940</td><td>6</td><td>Moderate Spender;...</td></tr>\n",
       "<tr><td>R000371116</td><td>6</td><td>Moderate Spender;...</td></tr>\n",
       "<tr><td>R000371233</td><td>6</td><td>Moderate Spender;...</td></tr>\n",
       "<tr><td>R000371447</td><td>6</td><td>Moderate Spender;...</td></tr>\n",
       "<tr><td>R000371453</td><td>6</td><td>Moderate Spender;...</td></tr>\n",
       "<tr><td>R000371992</td><td>6</td><td>Moderate Spender;...</td></tr>\n",
       "<tr><td>R000371004</td><td>6</td><td>Moderate Spender;...</td></tr>\n",
       "<tr><td>R000370970</td><td>6</td><td>Moderate Spender;...</td></tr>\n",
       "<tr><td>R000370718</td><td>6</td><td>Moderate Spender;...</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+----------+----------+--------------------+\n",
       "|    cif_id|cluster_id|     intreapretation|\n",
       "+----------+----------+--------------------+\n",
       "|R000372322|         6|Moderate Spender;...|\n",
       "|R000372359|         6|Moderate Spender;...|\n",
       "|R000371858|         4|Moderate Spender;...|\n",
       "|R000370704|         6|Moderate Spender;...|\n",
       "|R000371446|         6|Moderate Spender;...|\n",
       "|R000371574|         3|Moderate Spender;...|\n",
       "|R000372430|         6|Moderate Spender;...|\n",
       "|R000371010|         1|Moderate Spender;...|\n",
       "|R000370804|         9|Moderate Spender;...|\n",
       "|R000371531|         6|Moderate Spender;...|\n",
       "|R000370940|         6|Moderate Spender;...|\n",
       "|R000371018|         6|Moderate Spender;...|\n",
       "|R000371447|         6|Moderate Spender;...|\n",
       "|R000371233|         6|Moderate Spender;...|\n",
       "|R000371116|         6|Moderate Spender;...|\n",
       "|R000371004|         6|Moderate Spender;...|\n",
       "|R000371992|         6|Moderate Spender;...|\n",
       "|R000371453|         6|Moderate Spender;...|\n",
       "|R000370970|         6|Moderate Spender;...|\n",
       "|R000370718|         6|Moderate Spender;...|\n",
       "+----------+----------+--------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f'SELECT * from  {custom_catalog}.gold.customer_segments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53e1a88e-2057-4d9c-8a79-2907e4b21900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {custom_catalog}.gold.customer_segments (\n",
    "            cif_id VARCHAR(50),\n",
    "            cluster_id INTEGER,\n",
    "            intrepretation VARCHAR(300)\n",
    "        )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd67819b-4705-40ae-b85a-6d19372d1d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "results -> csv -> 04_03.csv\n",
    "csv -> read 04_03.csv -> iceberg write (overwrite)\n",
    "trino table\n",
    "move csv -> 04_03.csv -> archive folder\n",
    "\n",
    "\n",
    "first write -> 04_02 -> csv\n",
    "csv spark read -> iceberg\n",
    "Move csv -> archive\n",
    "csv -> empty\n",
    "archive -> 04_02\n",
    "\n",
    "04_03 -> csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16d43ee5-458a-4db3-b265-e0cd998cf1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://high-memory-one-0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>All Data Generation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa4aab73fd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff0579df-5e22-4090-b5ea-d641278f99b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1268538"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_s3_path = f\"s3a://ai360ctzn-customer-segmentation/results/final_segmented_customers_2025-04-02.csv\"\n",
    " \n",
    "   #read from a folder in s3 bucket\n",
    "df = spark.read.csv(input_s3_path, header=True,inferSchema=True)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc668bfd-dbc5-415a-9d28-069946f80620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|cif_id    |cluster|interpretations                                                                                                             |\n",
      "+----------+-------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|C000000056|2      |Moderate Spender; Low Credit Utilization; Aggressive Overpayer; Low Financial Risk                                          |\n",
      "|C000000291|1      |Moderate Spender; Very High Credit Utilization; Aggressive Overpayer; Extremely High Financial Risk                         |\n",
      "|C000000586|0      |High Spender; Intensive Cash Advance User; Very High Credit Utilization; Aggressive Overpayer; Extremely High Financial Risk|\n",
      "|C000000660|1      |Moderate Spender; Very High Credit Utilization; Aggressive Overpayer; Extremely High Financial Risk                         |\n",
      "|C000000726|4      |Moderate Spender; Rare Cash Advance User; Low Credit Utilization; Aggressive Overpayer; Low Financial Risk                  |\n",
      "+----------+-------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0d91b65-880e-4c5b-8dcb-130e9f963a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.writeTo(f\"{custom_catalog}.gold.customer_segments1\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa8d9a-9795-41b8-821c-aca609efc1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting  cif_id and predictions from the dataframe\n",
    "df_selected = predictions.select(\"cif_id\", \"prediction\")\n",
    " \n",
    "# writin into the predictions of gold schema in iceberg catalog format\n",
    "df_selected.writeTo(f\"{custom_catalog}.gold.predictions\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5c97d0a-782b-4654-8f37-13069f40e40e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cursor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# After creating the connection and cursor, add this:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcursor\u001b[49m\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT column_name FROM information_schema.columns WHERE table_schema = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgold\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m AND table_name = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_segments\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m columns \u001b[38;5;241m=\u001b[39m cursor\u001b[38;5;241m.\u001b[39mfetchall()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable columns:\u001b[39m\u001b[38;5;124m\"\u001b[39m, columns)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cursor' is not defined"
     ]
    }
   ],
   "source": [
    "# # After creating the connection and cursor, add this:\n",
    "# cursor.execute(\"SELECT column_name FROM information_schema.columns WHERE table_schema = 'gold' AND table_name = 'customer_segments'\")\n",
    "# columns = cursor.fetchall()\n",
    "# print(\"Available columns:\", columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc2a574-685f-4e23-a6d5-d2882ffa0555",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image='quay.io/datanature_dev/jupyternotebook:java_home14',\n",
    "    packages_to_install=[\n",
    "        'trino',\n",
    "        'pandas<2.0.0',\n",
    "        'boto3',\n",
    "        'urllib3',\n",
    "        'sqlalchemy',\n",
    "        'sqlalchemy-trino',\n",
    "        'pyarrow==12.0.1',\n",
    "        'numpy<2.0.0'\n",
    "    ]\n",
    ")\n",
    "\n",
    "def save_predictions_to_trino(\n",
    "    segmentation_path: str,\n",
    "    minio_bucket: str\n",
    ") -> str:\n",
    "    import trino\n",
    "    from trino.auth import BasicAuthentication\n",
    "    import pandas as pd\n",
    "    import boto3\n",
    "    import urllib3\n",
    "    import os\n",
    "    import time\n",
    "    from io import StringIO\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "    # Disable SSL warnings\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "    try:\n",
    "        # Handle input path\n",
    "        object_key = segmentation_path.split(minio_bucket + '/')[-1] if minio_bucket in segmentation_path else segmentation_path\n",
    "        print(f\"MinIO bucket: {minio_bucket}\")\n",
    "        print(f\"Object key: {object_key}\")\n",
    "        \n",
    "        # 1. Download CSV from MinIO\n",
    "        minio_client = boto3.client(\n",
    "            's3',\n",
    "            endpoint_url=\"http://192.168.80.155:32000\",\n",
    "            aws_access_key_id=\"admin\",\n",
    "            aws_secret_access_key=\"dlyticaD123\",\n",
    "            verify=False,\n",
    "            config=boto3.session.Config(\n",
    "                connect_timeout=30,\n",
    "                read_timeout=60,\n",
    "                retries={'max_attempts': 3}\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        local_path = \"/tmp/predictions.csv\"\n",
    "        print(\"Downloading file from MinIO...\")\n",
    "        minio_client.download_file(minio_bucket, object_key, local_path)\n",
    "        \n",
    "        # 2. Read CSV in chunks to handle large files\n",
    "        print(\"Reading CSV file...\")\n",
    "        chunksize = 100000\n",
    "        dfs = []\n",
    "        for chunk in pd.read_csv(local_path, chunksize=chunksize):\n",
    "            # Rename columns to match Trino table\n",
    "            chunk = chunk.rename(columns={\n",
    "                \"cluster\": \"cluster_id\",\n",
    "                \"interpretations\": \"intreapretation\"  # Matching the table column\n",
    "            })\n",
    "            dfs.append(chunk)\n",
    "        \n",
    "        df = pd.concat(dfs)\n",
    "        print(f\"Read {len(df)} records from CSV\")\n",
    "        \n",
    "        # 3. Connect to Trino\n",
    "        trino_host = \"192.168.80.155\"\n",
    "        trino_port = \"30071\"\n",
    "        trino_user = \"ctzn.bank\"\n",
    "        trino_password = \"ctzn.bank_123\"\n",
    "        trino_catalog = \"iceberg\"\n",
    "        trino_schema = \"gold\"\n",
    "        \n",
    "        # Create connection with timeout\n",
    "        conn = trino.dbapi.connect(\n",
    "            host=trino_host,\n",
    "            port=trino_port,\n",
    "            user=trino_user,\n",
    "            auth=BasicAuthentication(trino_user, trino_password),\n",
    "            catalog=trino_catalog,\n",
    "            schema=trino_schema,\n",
    "            http_scheme=\"https\",\n",
    "            verify=False,\n",
    "            request_timeout=600  # 10 minute timeout\n",
    "        )\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # 4. Prepare table (create if not exists)\n",
    "        print(\"Preparing table...\")\n",
    "        cursor.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS iceberg.gold.customer_segments (\n",
    "            cif_id VARCHAR,\n",
    "            cluster_id INTEGER,\n",
    "            intreapretation VARCHAR\n",
    "        )\n",
    "        \"\"\")\n",
    "        \n",
    "        # 5. Clear existing data\n",
    "        print(\"Clearing existing data...\")\n",
    "        cursor.execute(\"DELETE FROM iceberg.gold.customer_segments\")\n",
    "        \n",
    "        # 6. Insert data in parallel batches\n",
    "        print(\"Starting data insertion...\")\n",
    "        batch_size = 5000\n",
    "        total_rows = len(df)\n",
    "        batches = [(i, min(i + batch_size, total_rows)) \n",
    "                  for i in range(0, total_rows, batch_size)]\n",
    "        \n",
    "        def insert_batch(start, end):\n",
    "            batch_conn = trino.dbapi.connect(\n",
    "                host=trino_host,\n",
    "                port=trino_port,\n",
    "                user=trino_user,\n",
    "                auth=BasicAuthentication(trino_user, trino_password),\n",
    "                catalog=trino_catalog,\n",
    "                schema=trino_schema,\n",
    "                http_scheme=\"https\",\n",
    "                verify=False\n",
    "            )\n",
    "            batch_cursor = batch_conn.cursor()\n",
    "            \n",
    "            try:\n",
    "                # Prepare batch data\n",
    "                batch_df = df.iloc[start:end]\n",
    "                values = []\n",
    "                for _, row in batch_df.iterrows():\n",
    "                    values.append((\n",
    "                        str(row['cif_id']) if pd.notna(row['cif_id']) else None,\n",
    "                        int(row['cluster_id']) if pd.notna(row['cluster_id']) else None,\n",
    "                        str(row['intreapretation']) if pd.notna(row['intreapretation']) else None\n",
    "                    ))\n",
    "                \n",
    "                # Build query\n",
    "                query = \"INSERT INTO iceberg.gold.customer_segments (cif_id, cluster_id, intreapretation) VALUES \"\n",
    "                query += \", \".join([\"(%s, %s, %s)\"] * len(values))\n",
    "                query = query % tuple([item for sublist in values for item in sublist])\n",
    "                \n",
    "                # Execute\n",
    "                batch_cursor.execute(query)\n",
    "                return (start, end, True)\n",
    "            except Exception as e:\n",
    "                print(f\"Batch {start}-{end} failed: {str(e)}\")\n",
    "                return (start, end, False)\n",
    "            finally:\n",
    "                batch_cursor.close()\n",
    "                batch_conn.close()\n",
    "        \n",
    "        # Use ThreadPoolExecutor for parallel inserts\n",
    "        success_count = 0\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = [executor.submit(insert_batch, start, end) for start, end in batches]\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                start, end, success = future.result()\n",
    "                if success:\n",
    "                    success_count += 1\n",
    "                    print(f\"Successfully inserted batch {start}-{end}\")\n",
    "                else:\n",
    "                    print(f\"Failed to insert batch {start}-{end}\")\n",
    "        \n",
    "        # Verify insertion\n",
    "        print(\"Verifying data...\")\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM iceberg.gold.customer_segments\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f\"Verified {count} records in Trino table\")\n",
    "        \n",
    "        # Close connections\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        if success_count == len(batches):\n",
    "            return f\"Successfully loaded {count} records to Trino table iceberg.gold.customer_segments\"\n",
    "        else:\n",
    "            raise Exception(f\"Partial success: {success_count}/{len(batches)} batches inserted\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in save_predictions_to_trino: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        # Clean up\n",
    "        if 'local_path' in locals() and os.path.exists(local_path):\n",
    "            os.remove(local_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247498fc-83ae-4e79-a22f-6343c54159a4",
   "metadata": {},
   "source": [
    "# working pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ea58f54-3b17-4005-8154-9b8642118caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.dsl import pipeline, component\n",
    "from datetime import datetime\n",
    "from typing import NamedTuple\n",
    "\n",
    "# Step 1: Fetch data from Trino and save it to MinIO\n",
    "@component(\n",
    "    base_image='bitnami/spark:3.5', \n",
    "    packages_to_install=['trino', 'pandas', 'pyarrow', 's3fs', 'boto3', 'urllib3']\n",
    ")\n",
    "def fetch_data_trino(minio_bucket: str) -> str:\n",
    "    import trino\n",
    "    from trino.auth import BasicAuthentication\n",
    "    import csv\n",
    "    import boto3\n",
    "    import os\n",
    "    import urllib3\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Disable SSL warnings\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "    # Trino Connection Details\n",
    "    TRINO_HOST = \"192.168.80.155\"\n",
    "    TRINO_PORT = \"30071\"\n",
    "    TRINO_USER = \"ctzn.bank\"\n",
    "    TRINO_PASSWORD = \"ctzn.bank_123\"\n",
    "    TRINO_CATALOG = \"iceberg\"\n",
    "    TRINO_SCHEMA = \"silver_crmuser\"\n",
    "    TRINO_HTTP_SCHEME = \"https\"\n",
    "\n",
    "    # Output file\n",
    "    OUTPUT_FILE = \"/tmp/ctzn_10_years_data.csv\"\n",
    "\n",
    "    # SQL Query\n",
    "    SQL_QUERY = \"\"\"\n",
    "    WITH recent_customers AS (\n",
    "        SELECT DISTINCT g.cif_id\n",
    "        FROM gold.dim_gam AS g\n",
    "        WHERE CAST(DATE_PARSE(SUBSTRING(g.acct_opn_date, 1, 19), '%Y-%m-%dT%H:%i:%s') AS DATE) >= CURRENT_DATE - INTERVAL '10' YEAR\n",
    "    ),\n",
    "    account_activity AS (\n",
    "        SELECT \n",
    "            a.cif_id,\n",
    "            SUM(COALESCE(a.total_credit_tran_vol, 0) - COALESCE(a.total_debit_tran_vol, 0)) AS balance,\n",
    "            COUNT(DISTINCT a.nepali_month)/6.0 AS balance_frequency,\n",
    "            SUM(COALESCE(a.total_debit_tran_vol, 0)) AS purchases,\n",
    "            MAX(COALESCE(a.total_debit_tran_vol, 0)) AS oneoff_purchases,\n",
    "            SUM(COALESCE(a.total_debit_tran_vol, 0)) - MAX(COALESCE(a.total_debit_tran_vol, 0)) AS installments_purchases,\n",
    "            SUM(CASE WHEN COALESCE(a.total_credit_tran_vol, 0) > 0 AND COALESCE(a.total_debit_tran_vol, 0) = 0 \n",
    "                    THEN COALESCE(a.total_credit_tran_vol, 0) ELSE 0 END) AS cash_advance,\n",
    "            COUNT(DISTINCT a.foracid)/6.0 AS purchases_frequency,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_debit_tran_vol, 0) > 100000 THEN a.foracid END)/6.0 AS oneoff_purchases_frequency,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_debit_tran_vol, 0) BETWEEN 1 AND 100000 THEN a.foracid END)/6.0 AS purchases_installments_frequency,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_credit_tran_vol, 0) > 0 THEN a.foracid END)/6.0 AS cash_advance_frequency,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_credit_tran_vol, 0) > 0 THEN a.foracid END) AS cash_advance_trx,\n",
    "            COUNT(DISTINCT a.foracid) AS purchases_trx,\n",
    "            SUM(COALESCE(a.total_credit_tran_vol, 0)) AS payments,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_credit_tran_vol, 0) >= COALESCE(a.total_debit_tran_vol, 0) \n",
    "                                THEN a.nepali_month END)/6.0 AS prc_full_payment\n",
    "        FROM gold.mv_fact_deposit_account_insights a\n",
    "        JOIN recent_customers rc ON a.cif_id = rc.cif_id\n",
    "        GROUP BY a.cif_id\n",
    "    ),\n",
    "    salary_stats AS (\n",
    "        SELECT \n",
    "            APPROX_PERCENTILE(COALESCE(salary_per_month, 0), 0.5) AS median_salary,\n",
    "            APPROX_PERCENTILE(COALESCE(salary_per_month, 0), 0.05) AS fifth_percentile_salary\n",
    "        FROM gold.dim_customers\n",
    "    ),\n",
    "    customer_profile AS (\n",
    "        SELECT \n",
    "            g.cif_id,\n",
    "            DATE_DIFF('year', \n",
    "                     CAST(DATE_PARSE(SUBSTRING(MIN(g.acct_opn_date), 1, 19), '%Y-%m-%dT%H:%i:%s') AS DATE), \n",
    "                     CURRENT_DATE) AS tenure,\n",
    "            (SELECT fifth_percentile_salary FROM salary_stats) AS minimum_payments\n",
    "        FROM gold.dim_gam g\n",
    "        LEFT JOIN gold.dim_customers c ON g.cif_id = c.cif_id\n",
    "        GROUP BY g.cif_id\n",
    "    )\n",
    "    SELECT \n",
    "        aa.cif_id AS custid,\n",
    "        aa.balance,\n",
    "        aa.balance_frequency,\n",
    "        aa.purchases,\n",
    "        aa.oneoff_purchases,\n",
    "        aa.installments_purchases,\n",
    "        aa.cash_advance,\n",
    "        aa.purchases_frequency,\n",
    "        aa.oneoff_purchases_frequency,\n",
    "        aa.purchases_installments_frequency,\n",
    "        aa.cash_advance_frequency,\n",
    "        aa.cash_advance_trx,\n",
    "        aa.purchases_trx,\n",
    "        (SELECT median_salary * 3 FROM salary_stats) AS credit_limit,\n",
    "        aa.payments,\n",
    "        cp.minimum_payments,\n",
    "        aa.prc_full_payment,\n",
    "        cp.tenure\n",
    "    FROM account_activity aa\n",
    "    JOIN customer_profile cp ON aa.cif_id = cp.cif_id\n",
    "    ORDER BY aa.cif_id\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Connect to Trino\n",
    "        conn = trino.dbapi.connect(\n",
    "            host=TRINO_HOST,\n",
    "            port=TRINO_PORT,\n",
    "            user=TRINO_USER,\n",
    "            auth=BasicAuthentication(TRINO_USER, TRINO_PASSWORD),\n",
    "            catalog=TRINO_CATALOG,\n",
    "            schema=TRINO_SCHEMA,\n",
    "            http_scheme=TRINO_HTTP_SCHEME,\n",
    "            request_timeout=600,\n",
    "            verify=False\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Execute query and fetch data\n",
    "        cursor.execute(SQL_QUERY)\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        \n",
    "        # Write to CSV in batches\n",
    "        with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(columns)\n",
    "            \n",
    "            while True:\n",
    "                rows = cursor.fetchmany(1000)\n",
    "                if not rows:\n",
    "                    break\n",
    "                writer.writerows(rows)\n",
    "        \n",
    "        # Upload to MinIO\n",
    "        minio_client = boto3.client(\n",
    "            's3',\n",
    "            endpoint_url=\"http://192.168.80.155:32000\",\n",
    "            aws_access_key_id=\"admin\",\n",
    "            aws_secret_access_key=\"dlyticaD123\",\n",
    "            verify=False\n",
    "        )\n",
    "        \n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        minio_path = f\"data/customer_segmentation_raw_{current_date}.csv\"\n",
    "        minio_client.upload_file(OUTPUT_FILE, minio_bucket, minio_path)\n",
    "        \n",
    "        return minio_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in fetch_data_trino: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11788abf-6a58-416b-bd25-f7e4216487b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.dsl import pipeline, component\n",
    "from datetime import datetime\n",
    "from typing import NamedTuple\n",
    "\n",
    "# Step 1: Fetch data from Trino and save it to MinIO\n",
    "@component(\n",
    "    base_image='bitnami/spark:3.5', \n",
    "    packages_to_install=['trino', 'pandas', 'pyarrow', 's3fs', 'boto3', 'urllib3']\n",
    ")\n",
    "def fetch_data_trino(minio_bucket: str) -> str:\n",
    "    import trino\n",
    "    from trino.auth import BasicAuthentication\n",
    "    import csv\n",
    "    import boto3\n",
    "    import os\n",
    "    import urllib3\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Disable SSL warnings\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "    # Trino Connection Details\n",
    "    TRINO_HOST = \"192.168.80.155\"\n",
    "    TRINO_PORT = \"30071\"\n",
    "    TRINO_USER = \"ctzn.bank\"\n",
    "    TRINO_PASSWORD = \"ctzn.bank_123\"\n",
    "    TRINO_CATALOG = \"iceberg\"\n",
    "    TRINO_SCHEMA = \"gold\"\n",
    "    TRINO_HTTP_SCHEME = \"https\"\n",
    "\n",
    "    # Output file\n",
    "    OUTPUT_FILE = \"/tmp/ctzn_10_years_data.csv\"\n",
    "\n",
    "    # SQL Query\n",
    "    SQL_QUERY = \"\"\"\n",
    "    WITH recent_customers AS (\n",
    "        SELECT DISTINCT g.cif_id\n",
    "        FROM gold.dim_gam AS g\n",
    "        WHERE CAST(DATE_PARSE(SUBSTRING(g.acct_opn_date, 1, 19), '%Y-%m-%dT%H:%i:%s') AS DATE) >= CURRENT_DATE - INTERVAL '10' YEAR\n",
    "    ),\n",
    "    account_activity AS (\n",
    "        SELECT \n",
    "            a.cif_id,\n",
    "            SUM(COALESCE(a.total_credit_tran_vol, 0) - COALESCE(a.total_debit_tran_vol, 0)) AS balance,\n",
    "            COUNT(DISTINCT a.nepali_month)/6.0 AS balance_frequency,\n",
    "            SUM(COALESCE(a.total_debit_tran_vol, 0)) AS purchases,\n",
    "            MAX(COALESCE(a.total_debit_tran_vol, 0)) AS oneoff_purchases,\n",
    "            SUM(COALESCE(a.total_debit_tran_vol, 0)) - MAX(COALESCE(a.total_debit_tran_vol, 0)) AS installments_purchases,\n",
    "            SUM(CASE WHEN COALESCE(a.total_credit_tran_vol, 0) > 0 AND COALESCE(a.total_debit_tran_vol, 0) = 0 \n",
    "                    THEN COALESCE(a.total_credit_tran_vol, 0) ELSE 0 END) AS cash_advance,\n",
    "            COUNT(DISTINCT a.foracid)/6.0 AS purchases_frequency,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_debit_tran_vol, 0) > 100000 THEN a.foracid END)/6.0 AS oneoff_purchases_frequency,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_debit_tran_vol, 0) BETWEEN 1 AND 100000 THEN a.foracid END)/6.0 AS purchases_installments_frequency,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_credit_tran_vol, 0) > 0 THEN a.foracid END)/6.0 AS cash_advance_frequency,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_credit_tran_vol, 0) > 0 THEN a.foracid END) AS cash_advance_trx,\n",
    "            COUNT(DISTINCT a.foracid) AS purchases_trx,\n",
    "            SUM(COALESCE(a.total_credit_tran_vol, 0)) AS payments,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_credit_tran_vol, 0) >= COALESCE(a.total_debit_tran_vol, 0) \n",
    "                                THEN a.nepali_month END)/6.0 AS prc_full_payment\n",
    "        FROM gold.mv_fact_deposit_account_insights a\n",
    "        JOIN recent_customers rc ON a.cif_id = rc.cif_id\n",
    "        GROUP BY a.cif_id\n",
    "    ),\n",
    "    salary_stats AS (\n",
    "        SELECT \n",
    "            APPROX_PERCENTILE(COALESCE(salary_per_month, 0), 0.5) AS median_salary,\n",
    "            APPROX_PERCENTILE(COALESCE(salary_per_month, 0), 0.05) AS fifth_percentile_salary\n",
    "        FROM gold.dim_customers\n",
    "    ),\n",
    "    customer_profile AS (\n",
    "        SELECT \n",
    "            g.cif_id,\n",
    "            DATE_DIFF('year', \n",
    "                     CAST(DATE_PARSE(SUBSTRING(MIN(g.acct_opn_date), 1, 19), '%Y-%m-%dT%H:%i:%s') AS DATE), \n",
    "                     CURRENT_DATE) AS tenure,\n",
    "            (SELECT fifth_percentile_salary FROM salary_stats) AS minimum_payments\n",
    "        FROM gold.dim_gam g\n",
    "        LEFT JOIN gold.dim_customers c ON g.cif_id = c.cif_id\n",
    "        GROUP BY g.cif_id\n",
    "    )\n",
    "    SELECT \n",
    "        aa.cif_id AS custid,\n",
    "        aa.balance,\n",
    "        aa.balance_frequency,\n",
    "        aa.purchases,\n",
    "        aa.oneoff_purchases,\n",
    "        aa.installments_purchases,\n",
    "        aa.cash_advance,\n",
    "        aa.purchases_frequency,\n",
    "        aa.oneoff_purchases_frequency,\n",
    "        aa.purchases_installments_frequency,\n",
    "        aa.cash_advance_frequency,\n",
    "        aa.cash_advance_trx,\n",
    "        aa.purchases_trx,\n",
    "        (SELECT median_salary * 3 FROM salary_stats) AS credit_limit,\n",
    "        aa.payments,\n",
    "        cp.minimum_payments,\n",
    "        aa.prc_full_payment,\n",
    "        cp.tenure\n",
    "    FROM account_activity aa\n",
    "    JOIN customer_profile cp ON aa.cif_id = cp.cif_id\n",
    "    ORDER BY aa.cif_id\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Connect to Trino\n",
    "        conn = trino.dbapi.connect(\n",
    "            host=TRINO_HOST,\n",
    "            port=TRINO_PORT,\n",
    "            user=TRINO_USER,\n",
    "            auth=BasicAuthentication(TRINO_USER, TRINO_PASSWORD),\n",
    "            catalog=TRINO_CATALOG,\n",
    "            schema=TRINO_SCHEMA,\n",
    "            http_scheme=TRINO_HTTP_SCHEME,\n",
    "            request_timeout=600,\n",
    "            verify=False\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Execute query and fetch data\n",
    "        cursor.execute(SQL_QUERY)\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        \n",
    "        # Write to CSV in batches\n",
    "        with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(columns)\n",
    "            \n",
    "            while True:\n",
    "                rows = cursor.fetchmany(1000)\n",
    "                if not rows:\n",
    "                    break\n",
    "                writer.writerows(rows)\n",
    "        \n",
    "        # Upload to MinIO\n",
    "        minio_client = boto3.client(\n",
    "            's3',\n",
    "            endpoint_url=\"http://192.168.80.155:32000\",\n",
    "            aws_access_key_id=\"admin\",\n",
    "            aws_secret_access_key=\"dlyticaD123\",\n",
    "            verify=False\n",
    "        )\n",
    "        \n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        minio_path = f\"data/customer_segmentation_raw_{current_date}.csv\"\n",
    "        minio_client.upload_file(OUTPUT_FILE, minio_bucket, minio_path)\n",
    "        \n",
    "        return minio_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in fetch_data_trino: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "@component(\n",
    "    base_image='quay.io/datanature_dev/jupyternotebook:java_home14',\n",
    "    packages_to_install=[\n",
    "        'pyspark==3.5.0',\n",
    "        'pandas==2.0.3',\n",
    "        'numpy==1.24.4',\n",
    "        'boto3==1.28.57',\n",
    "        'scikit-learn==1.3.0',\n",
    "        'matplotlib==3.7.2',\n",
    "        'pyarrow==12.0.1',\n",
    "        'urllib3==2.0.4'\n",
    "    ]\n",
    ")\n",
    "def feature_engineering_and_segmentation(\n",
    "    file_path: str, \n",
    "    minio_bucket: str\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('segmentation_path', str),\n",
    "    ('model_path', str),\n",
    "    ('cluster_plot_path', str),\n",
    "    ('cluster_interpretations', str),\n",
    "    ('final_output_path', str)\n",
    "]):\n",
    "    # First ensure numpy is imported with the correct version\n",
    "    import numpy as np\n",
    "    np.__version__  # This helps catch version issues early\n",
    "    \n",
    "    # Then import other packages\n",
    "    import boto3\n",
    "    from botocore.config import Config\n",
    "    import pandas as pd\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')  # Set non-interactive backend\n",
    "    import matplotlib.pyplot as plt\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "    import json\n",
    "    import tempfile\n",
    "    import time\n",
    "    from collections import namedtuple\n",
    "    from io import StringIO\n",
    "    \n",
    "    # Now import pyspark components\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col, when, mean, monotonically_increasing_id, stddev, count, abs\n",
    "    from pyspark.sql.types import DoubleType, StringType\n",
    "    from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\n",
    "    from pyspark.ml.clustering import BisectingKMeans\n",
    "    from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "    from pyspark.sql.functions import udf\n",
    "\n",
    "    def calculate_feature_distinctiveness(cluster_profiles):\n",
    "        \"\"\"Enhanced feature distinctiveness calculation\"\"\"\n",
    "        feature_variance = cluster_profiles.std()\n",
    "        distinctiveness = feature_variance / (feature_variance.max() + 1e-10)\n",
    "        return distinctiveness\n",
    "\n",
    "    def interpret_clusters(cluster_profiles):\n",
    "        interpretations = {}\n",
    "        distinctiveness = calculate_feature_distinctiveness(cluster_profiles)\n",
    "        \n",
    "        for cluster in cluster_profiles.index:\n",
    "            features = cluster_profiles.columns.tolist()\n",
    "            profile_components = []\n",
    "            \n",
    "            if 'purchases' in features:\n",
    "                spend_level = cluster_profiles.loc[cluster, 'purchases']\n",
    "                mean_spend = cluster_profiles['purchases'].mean()\n",
    "                std_spend = cluster_profiles['purchases'].std()\n",
    "                \n",
    "                if spend_level > mean_spend + 1.5 * std_spend:\n",
    "                    profile_components.append(\"High Spender\")\n",
    "                elif spend_level < mean_spend - 1.5 * std_spend:\n",
    "                    profile_components.append(\"Low Spender\")\n",
    "                else:\n",
    "                    profile_components.append(\"Moderate Spender\")\n",
    "            \n",
    "            if 'cash_advance' in features and 'cash_advance_frequency' in features:\n",
    "                cash_advance = cluster_profiles.loc[cluster, 'cash_advance']\n",
    "                cash_freq = cluster_profiles.loc[cluster, 'cash_advance_frequency']\n",
    "                mean_cash = cluster_profiles['cash_advance'].mean()\n",
    "                mean_freq = cluster_profiles['cash_advance_frequency'].mean()\n",
    "                \n",
    "                if cash_advance > mean_cash * 2 and cash_freq > mean_freq * 2:\n",
    "                    profile_components.append(\"Intensive Cash Advance User\")\n",
    "                elif cash_advance < mean_cash * 0.5 and cash_freq < mean_freq * 0.5:\n",
    "                    profile_components.append(\"Rare Cash Advance User\")\n",
    "            \n",
    "            if 'balance' in features and 'credit_limit' in features:\n",
    "                balance_ratio = cluster_profiles.loc[cluster, 'balance'] / (cluster_profiles.loc[cluster, 'credit_limit'] + 1e-10)\n",
    "                \n",
    "                if balance_ratio > 0.8:\n",
    "                    profile_components.append(\"Very High Credit Utilization\")\n",
    "                elif balance_ratio > 0.5:\n",
    "                    profile_components.append(\"High Credit Utilization\")\n",
    "                elif balance_ratio < 0.2:\n",
    "                    profile_components.append(\"Low Credit Utilization\")\n",
    "                else:\n",
    "                    profile_components.append(\"Moderate Credit Utilization\")\n",
    "            \n",
    "            if 'payments' in features and 'minimum_payments' in features:\n",
    "                payment_ratio = cluster_profiles.loc[cluster, 'payments'] / (cluster_profiles.loc[cluster, 'minimum_payments'] + 1e-10)\n",
    "                \n",
    "                if payment_ratio > 3:\n",
    "                    profile_components.append(\"Aggressive Overpayer\")\n",
    "                elif payment_ratio > 2:\n",
    "                    profile_components.append(\"Consistent Overpayer\")\n",
    "                elif payment_ratio < 1.2:\n",
    "                    profile_components.append(\"Minimum Payment User\")\n",
    "            \n",
    "            if 'balance' in features and 'credit_limit' in features and 'cash_advance' in features:\n",
    "                risk_score = (\n",
    "                    cluster_profiles.loc[cluster, 'balance'] / (cluster_profiles.loc[cluster, 'credit_limit'] + 1e-10) * 0.4 +\n",
    "                    cluster_profiles.loc[cluster, 'cash_advance'] / (cluster_profiles.loc[cluster, 'credit_limit'] + 1e-10) * 0.6\n",
    "                )\n",
    "                \n",
    "                if risk_score > 0.7:\n",
    "                    profile_components.append(\"Extremely High Financial Risk\")\n",
    "                elif risk_score > 0.5:\n",
    "                    profile_components.append(\"High Financial Risk\")\n",
    "                elif risk_score < 0.2:\n",
    "                    profile_components.append(\"Low Financial Risk\")\n",
    "                else:\n",
    "                    profile_components.append(\"Moderate Financial Risk\")\n",
    "            \n",
    "            if profile_components:\n",
    "                interpretations[cluster] = \"; \".join(profile_components)\n",
    "            else:\n",
    "                interpretations[cluster] = \"Undefined Customer Segment\"\n",
    "        \n",
    "        return interpretations\n",
    "\n",
    "    def upload_to_minio(local_path, bucket, object_key, max_retries=5):\n",
    "        \"\"\"Uploads a file to MinIO with improved error handling and content-length issues fixed\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Create a fresh client for each upload attempt\n",
    "                client = boto3.client(\n",
    "                    's3',\n",
    "                    endpoint_url=\"http://192.168.80.155:32000\",\n",
    "                    aws_access_key_id=\"admin\",\n",
    "                    aws_secret_access_key=\"dlyticaD123\",\n",
    "                    verify=False,\n",
    "                    config=Config(\n",
    "                        connect_timeout=30,\n",
    "                        read_timeout=60,\n",
    "                        retries={'max_attempts': 3}\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                # Get the file size\n",
    "                file_size = os.path.getsize(local_path)\n",
    "                \n",
    "                # For CSV files, we'll read and write with pandas to ensure proper formatting\n",
    "                if object_key.endswith('.csv'):\n",
    "                    df = pd.read_csv(local_path)\n",
    "                    csv_buffer = StringIO()\n",
    "                    df.to_csv(csv_buffer, index=False)\n",
    "                    client.put_object(\n",
    "                        Bucket=bucket,\n",
    "                        Key=object_key,\n",
    "                        Body=csv_buffer.getvalue(),\n",
    "                        ContentType='text/csv'\n",
    "                    )\n",
    "                else:\n",
    "                    # For non-CSV files, use standard upload\n",
    "                    with open(local_path, 'rb') as file_data:\n",
    "                        client.put_object(\n",
    "                            Bucket=bucket,\n",
    "                            Key=object_key,\n",
    "                            Body=file_data,\n",
    "                            ContentLength=file_size\n",
    "                        )\n",
    "                    \n",
    "                print(f\"Successfully uploaded {local_path} to {bucket}/{object_key}\")\n",
    "                return f\"{bucket}/{object_key}\"\n",
    "                \n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = 2 ** attempt  # Exponential backoff\n",
    "                    print(f\"Upload attempt {attempt+1} failed: {str(e)}. Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Upload failed after {max_retries} attempts: {str(e)}\")\n",
    "                    raise\n",
    "\n",
    "    # Initialize Spark with optimized settings\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"AdvancedCustomerSegmentation\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "        .config(\"spark.executor.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Initialize MinIO client with improved configuration\n",
    "    boto_config = Config(\n",
    "        connect_timeout=30,\n",
    "        read_timeout=60,\n",
    "        retries={'max_attempts': 3}\n",
    "    )\n",
    "    \n",
    "    minio_client = boto3.client(\n",
    "        's3',\n",
    "        endpoint_url=\"http://192.168.80.155:32000\",\n",
    "        aws_access_key_id=\"admin\",\n",
    "        aws_secret_access_key=\"dlyticaD123\",\n",
    "        verify=False,\n",
    "        config=boto_config\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        today = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        local_path = \"/tmp/raw_data.csv\"\n",
    "        \n",
    "        # Download with retry logic\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                minio_client.download_file(minio_bucket, file_path, local_path)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise\n",
    "                time.sleep(5)\n",
    "\n",
    "        # Read data with explicit schema\n",
    "        df = spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .csv(local_path)\n",
    "\n",
    "        if 'custid' in df.columns:\n",
    "            df = df.withColumnRenamed(\"custid\", \"cif_id\")\n",
    "\n",
    "        # Feature engineering\n",
    "        df = df.withColumn(\"balance_utilization_ratio\", \n",
    "            when(col(\"credit_limit\") != 0, col(\"balance\") / col(\"credit_limit\")).otherwise(0)\n",
    "        )\n",
    "        df = df.withColumn(\"cash_advance_intensity\", \n",
    "            when(col(\"credit_limit\") != 0, col(\"cash_advance\") / col(\"credit_limit\")).otherwise(0)\n",
    "        )\n",
    "        df = df.withColumn(\"payment_effort_ratio\", \n",
    "            when(col(\"minimum_payments\") != 0, col(\"payments\") / col(\"minimum_payments\")).otherwise(0)\n",
    "        )\n",
    "        df = df.withColumn(\"purchase_diversity\", \n",
    "            abs(col(\"oneoff_purchases\") - col(\"installments_purchases\")) / \n",
    "            (abs(col(\"oneoff_purchases\") + col(\"installments_purchases\")) + 1e-10)\n",
    "        )\n",
    "        df = df.na.fill(0)\n",
    "\n",
    "        numeric_cols = [\n",
    "            'purchases', 'oneoff_purchases', 'installments_purchases', \n",
    "            'cash_advance', 'cash_advance_frequency', \n",
    "            'balance', 'credit_limit', \n",
    "            'payments', 'minimum_payments',\n",
    "            'balance_utilization_ratio', \n",
    "            'cash_advance_intensity', \n",
    "            'payment_effort_ratio',\n",
    "            'purchase_diversity'\n",
    "        ]\n",
    "\n",
    "        # Feature transformation\n",
    "        assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "        assembled = assembler.transform(df.drop(\"cif_id\"))\n",
    "        scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "        scaled_data = scaler.fit(assembled).transform(assembled)\n",
    "\n",
    "        # Dimensionality reduction\n",
    "        pca = PCA(k=5, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
    "        pca_data = pca.fit(scaled_data).transform(scaled_data)\n",
    "\n",
    "        # Clustering optimization\n",
    "        evaluator = ClusteringEvaluator(featuresCol='pca_features')\n",
    "        k_range = range(6, 15)\n",
    "        k_metrics = []\n",
    "\n",
    "        for k in k_range:\n",
    "            kmeans = BisectingKMeans(k=k, seed=42, featuresCol=\"pca_features\")\n",
    "            model = kmeans.fit(pca_data)\n",
    "            predictions = model.transform(pca_data)\n",
    "            \n",
    "            cost = model.summary.trainingCost if hasattr(model.summary, 'trainingCost') else 0\n",
    "            silhouette = evaluator.evaluate(predictions)\n",
    "            \n",
    "            centers = model.clusterCenters()\n",
    "            separation = np.mean([np.min(np.linalg.norm(centers[i] - centers[j])) \n",
    "                                  for i in range(len(centers)) \n",
    "                                  for j in range(i+1, len(centers))])\n",
    "            \n",
    "            k_metrics.append({\n",
    "                'k': k,\n",
    "                'cost': cost,\n",
    "                'silhouette': silhouette,\n",
    "                'separation': separation\n",
    "            })\n",
    "\n",
    "        # Determine optimal k\n",
    "        k_metrics_df = pd.DataFrame(k_metrics)\n",
    "        k_metrics_df['normalized_cost'] = (k_metrics_df['cost'] - k_metrics_df['cost'].min()) / (k_metrics_df['cost'].max() - k_metrics_df['cost'].min())\n",
    "        k_metrics_df['normalized_silhouette'] = (k_metrics_df['silhouette'] - k_metrics_df['silhouette'].min()) / (k_metrics_df['silhouette'].max() - k_metrics_df['silhouette'].min())\n",
    "        k_metrics_df['normalized_separation'] = (k_metrics_df['separation'] - k_metrics_df['separation'].min()) / (k_metrics_df['separation'].max() - k_metrics_df['separation'].min())\n",
    "        \n",
    "        k_metrics_df['composite_score'] = (\n",
    "            0.3 * (1 - k_metrics_df['normalized_cost']) +\n",
    "            0.4 * k_metrics_df['normalized_silhouette'] +\n",
    "            0.3 * k_metrics_df['normalized_separation']\n",
    "        )\n",
    "        \n",
    "        optimal_k = k_metrics_df.loc[k_metrics_df['composite_score'].idxmax(), 'k']\n",
    "\n",
    "        # Plotting\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15))\n",
    "        ax1.plot(k_metrics_df['k'], k_metrics_df['cost'], marker='o', color='blue')\n",
    "        ax1.set_xlabel('Number of Clusters (k)')\n",
    "        ax1.set_ylabel('Cost', color='blue')\n",
    "        ax1.grid(True)\n",
    "\n",
    "        ax2.plot(k_metrics_df['k'], k_metrics_df['silhouette'], marker='x', color='green')\n",
    "        ax2.set_xlabel('Number of Clusters (k)')\n",
    "        ax2.set_ylabel('Silhouette Score', color='green')\n",
    "        ax2.grid(True)\n",
    "\n",
    "        ax3.plot(k_metrics_df['k'], k_metrics_df['separation'], marker='^', color='red')\n",
    "        ax3.set_xlabel('Number of Clusters (k)')\n",
    "        ax3.set_ylabel('Cluster Separation', color='red')\n",
    "        ax3.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        cluster_plot_path = f\"/tmp/cluster_diagnostics_{today}.png\"\n",
    "        plt.savefig(cluster_plot_path)\n",
    "        plt.close()\n",
    "        \n",
    "        # Final clustering\n",
    "        kmeans = BisectingKMeans(k=optimal_k, seed=42, featuresCol=\"pca_features\")\n",
    "        model = kmeans.fit(pca_data)\n",
    "        clustered_data = model.transform(pca_data)\n",
    "\n",
    "        # Add cluster information\n",
    "        df_with_id = df.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "        clustered_data = clustered_data.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "        clustered_data = clustered_data.join(df_with_id.select(\"row_id\", \"cif_id\"), on=\"row_id\").drop(\"row_id\")\n",
    "\n",
    "        # Generate cluster profiles\n",
    "        summary = clustered_data.groupBy(\"prediction\").agg(\n",
    "            *[mean(col(c)).alias(c) for c in numeric_cols],\n",
    "            *[stddev(col(c)).alias(f\"{c}_std\") for c in numeric_cols],\n",
    "            count(\"cif_id\").alias(\"cluster_size\")\n",
    "        )\n",
    "        \n",
    "        # Convert to Pandas safely\n",
    "        cluster_data = summary.collect()\n",
    "        cluster_profiles = pd.DataFrame([row.asDict() for row in cluster_data]).set_index(\"prediction\")\n",
    "        \n",
    "        # Get interpretations\n",
    "        interpretations = interpret_clusters(cluster_profiles)\n",
    "        \n",
    "        # Add cluster interpretation to dataframe\n",
    "        interpret_udf = udf(lambda x: interpretations.get(x, \"Unknown\"), StringType())\n",
    "        clustered_data = clustered_data.withColumn(\"interpretation\", interpret_udf(col(\"prediction\")))\n",
    "\n",
    "        # Prepare final output with all relevant columns\n",
    "        output_columns = [\"cif_id\", \"prediction\", \"interpretation\"] + numeric_cols\n",
    "        final_output = clustered_data.select(\n",
    "            col(\"cif_id\").alias(\"cif_id\"),\n",
    "            col(\"prediction\").alias(\"cluster\"),\n",
    "            col(\"interpretation\").alias(\"interpretations\")\n",
    "        )\n",
    "\n",
    "        # Convert to Pandas DataFrame for CSV export\n",
    "        final_pdf = pd.DataFrame(\n",
    "            final_output.rdd.map(lambda row: row.asDict()).collect()\n",
    "        )\n",
    "\n",
    "        # Get current date in YYYY-MM-DD format\n",
    "        current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Define output paths\n",
    "        output_paths = {\n",
    "            'final_output': f\"/tmp/final_segmented_customers_{current_date}.csv\",\n",
    "            'interpretations': f\"/tmp/cluster_interpretations_{current_date}.json\",\n",
    "            'plot': f\"/tmp/cluster_diagnostics_{current_date}.png\",\n",
    "            'model': f\"/tmp/segmentation_model_{current_date}\"\n",
    "        }\n",
    "        \n",
    "        # Save final output (only the 3 columns)\n",
    "        final_pdf.to_csv(output_paths['final_output'], index=False)\n",
    "        print(f\"Final output sample:\\n{final_pdf.head()}\")\n",
    "        \n",
    "        # Save interpretations\n",
    "        with open(output_paths['interpretations'], 'w') as f:\n",
    "            json.dump(interpretations, f, indent=2)\n",
    "        \n",
    "        # Save plot\n",
    "        plt.savefig(output_paths['plot'])\n",
    "        plt.close()\n",
    "        \n",
    "        # Save model\n",
    "        model.write().overwrite().save(output_paths['model'])\n",
    "        \n",
    "        # Upload to MinIO with specified folder structure\n",
    "        minio_paths = {}\n",
    "        \n",
    "        # Upload CSV files to results/\n",
    "        csv_files = {\n",
    "            'final_output': f\"results/final_segmented_customers_{current_date}.csv\",\n",
    "            'interpretations': f\"results/cluster_interpretations_{current_date}.json\"\n",
    "        }\n",
    "        \n",
    "        for name, minio_key in csv_files.items():\n",
    "            try:\n",
    "                uploaded_path = upload_to_minio(output_paths[name], minio_bucket, minio_key)\n",
    "                minio_paths[name] = uploaded_path\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading {name}: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "        # Upload plot to images/\n",
    "        plot_key = f\"images/cluster_diagnostics_{current_date}.png\"\n",
    "        try:\n",
    "            uploaded_path = upload_to_minio(output_paths['plot'], minio_bucket, plot_key)\n",
    "            minio_paths['plot'] = uploaded_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading plot: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # Upload model to models/\n",
    "        model_key_prefix = f\"models/segmentation_model_{current_date}\"\n",
    "        try:\n",
    "            for root, _, files in os.walk(output_paths['model']):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    relative_path = os.path.relpath(file_path, output_paths['model'])\n",
    "                    s3_key = f\"{model_key_prefix}/{relative_path}\"\n",
    "                    upload_to_minio(file_path, minio_bucket, s3_key)\n",
    "            minio_paths['model'] = f\"{minio_bucket}/{model_key_prefix}\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading model files: {str(e)}\")\n",
    "            minio_paths['model'] = f\"{minio_bucket}/{model_key_prefix}_partial\"\n",
    "\n",
    "        output = namedtuple('Outputs', [\n",
    "            'segmentation_path',\n",
    "            'model_path',\n",
    "            'cluster_plot_path',\n",
    "            'cluster_interpretations',\n",
    "            'final_output_path'\n",
    "        ])\n",
    "        \n",
    "        return output(\n",
    "            f\"{minio_bucket}/{csv_files['final_output']}\",  # segmentation_path\n",
    "            f\"{minio_bucket}/{model_key_prefix}\",           # model_path\n",
    "            f\"{minio_bucket}/{plot_key}\",                   # cluster_plot_path\n",
    "            f\"{minio_bucket}/{csv_files['interpretations']}\",  # cluster_interpretations\n",
    "            f\"{minio_bucket}/{csv_files['final_output']}\"   # final_output_path\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in feature_engineering_and_segmentation: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if spark:\n",
    "            spark.stop()\n",
    "\n",
    "@component(\n",
    "    base_image='quay.io/datanature_dev/jupyternotebook:java_home14',\n",
    "    packages_to_install=[\n",
    "        'pyspark==3.5.0',\n",
    "        'pandas==2.0.3',\n",
    "        'numpy==1.24.4',\n",
    "        'boto3==1.28.57',\n",
    "        'pyarrow==12.0.1',\n",
    "        'urllib3==2.0.4'\n",
    "    ]\n",
    ")\n",
    "def save_predictions_to_trino(\n",
    "    segmentation_path: str,\n",
    "    minio_bucket: str\n",
    ") -> str:\n",
    "    import os\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col\n",
    "    from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "    # Configuration\n",
    "    access_key_id = 'admin'\n",
    "    secret_access_key = 'dlyticaD123'\n",
    "    minio_endpoint = 'dn-minio-tenant-hl.dn-minio-tenant.svc.cluster.local:9000'\n",
    "    data_bucket = 'ai360fd-recommendation'\n",
    "    hive_metastore_uri = \"thrift://dn-hive-metastore.dn-hive-metastore.svc.cluster.local:9083\"\n",
    "    iceberg_warehouse_location = f\"s3a://{data_bucket}/data/\"\n",
    "    custom_catalog = \"iceberg_catalog\"\n",
    "    app_name = \"Customer Segmentation Loader\"\n",
    "    additional_packages = \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.spark:spark-avro_2.12:3.5.0,com.crealytics:spark-excel_2.12:0.13.7,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.hadoop:hadoop-common:3.3.4\"\n",
    "    \n",
    "    # Initialize Spark Session with more compatible settings\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", minio_endpoint) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", access_key_id) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", secret_access_key) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .config(\"spark.hadoop.hive.metastore.uris\", hive_metastore_uri) \\\n",
    "        .config(\"spark.sql.warehouse.dir\", iceberg_warehouse_location) \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "        .config(f\"spark.sql.catalog.{custom_catalog}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "        .config(f\"spark.sql.catalog.{custom_catalog}.warehouse\", iceberg_warehouse_location) \\\n",
    "        .config(f\"spark.sql.catalog.{custom_catalog}.s3.endpoint\", minio_endpoint) \\\n",
    "        .config(f\"spark.sql.catalog.{custom_catalog}.s3.access-key\", access_key_id) \\\n",
    "        .config(f\"spark.sql.catalog.{custom_catalog}.s3.secret-key\", secret_access_key) \\\n",
    "        .config(f\"spark.sql.catalog.{custom_catalog}.s3.path-style-access\", \"true\") \\\n",
    "        .config(\"spark.jars.packages\", additional_packages) \\\n",
    "        .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "        .config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n",
    "        .config(\"spark.sql.legacy.createHiveTableByDefault\", \"false\") \\\n",
    "        .config(\"spark.sql.iceberg.handle-timestamp-without-timezone\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"100\") \\\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    try:\n",
    "        # Extract the object key from the segmentation_path\n",
    "        if segmentation_path.startswith(f\"{minio_bucket}/\"):\n",
    "            object_key = segmentation_path[len(minio_bucket)+1:]\n",
    "        else:\n",
    "            object_key = segmentation_path\n",
    "        \n",
    "        # Construct the S3 path for Spark to read\n",
    "        input_s3_path = f\"s3a://{minio_bucket}/{object_key}\"\n",
    "        print(f\"Reading data from: {input_s3_path}\")\n",
    "        \n",
    "        # Read the segmentation data\n",
    "        df = spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .csv(input_s3_path)\n",
    "        \n",
    "        # Print input schema for debugging\n",
    "        print(\"Input DataFrame Schema:\")\n",
    "        df.printSchema()\n",
    "        \n",
    "        # Rename columns to match our desired schema\n",
    "        column_mapping = {\n",
    "            \"cluster\": \"cluster_id\",\n",
    "            \"interpretations\": \"interpretation\"  # Corrected spelling\n",
    "        }\n",
    "        \n",
    "        for old_name, new_name in column_mapping.items():\n",
    "            if old_name in df.columns:\n",
    "                df = df.withColumnRenamed(old_name, new_name)\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        required_columns = [\"cif_id\", \"cluster_id\", \"interpretation\"]\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "        \n",
    "        # Select only the required columns and cache to minimize re-computation\n",
    "        df = df.select(*required_columns).cache()\n",
    "        \n",
    "        # Sample a small subset for testing if dataset is large\n",
    "        # This helps with storage full issues - remove in production\n",
    "        row_count = df.count()\n",
    "        if row_count > 1000:\n",
    "            print(f\"Dataset has {row_count} rows, sampling for testing due to storage constraints\")\n",
    "            df = df.sample(fraction=1000.0/row_count).cache()\n",
    "        \n",
    "        # Define the table name\n",
    "        table_name = f\"{custom_catalog}.gold.customer_segments\"\n",
    "        temp_table_name = f\"{custom_catalog}.gold.customer_segments_temp\"\n",
    "        \n",
    "        # Handle existing table - this is crucial as we're getting an \"already exists\" error\n",
    "        try:\n",
    "            # Check if table exists by attempting to access it directly\n",
    "            print(f\"Checking if {table_name} exists\")\n",
    "            \n",
    "            # Use more direct DataFrame API approach to check table existence\n",
    "            tables = spark.catalog.listTables(f\"{custom_catalog}.gold\")\n",
    "            table_exists = any(t.name.lower() == \"customer_segments\" for t in tables)\n",
    "            \n",
    "            if table_exists:\n",
    "                print(f\"Table {table_name} exists, will use 'overwrite' mode instead of 'create'\")\n",
    "                # Get sample of existing data to verify schema\n",
    "                try:\n",
    "                    existing_df = spark.table(table_name).limit(1)\n",
    "                    print(\"Existing table schema:\")\n",
    "                    existing_df.printSchema()\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not read existing table: {str(e)}\")\n",
    "            else:\n",
    "                print(f\"Table {table_name} does not exist\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error checking table existence: {str(e)}\")\n",
    "            print(\"Proceeding with assumption that table does not exist\")\n",
    "            table_exists = False\n",
    "        \n",
    "        # Write data - handle both cases (exists or not)\n",
    "        print(\"Writing data to table...\")\n",
    "        try:\n",
    "            if table_exists:\n",
    "                # Try overwrite mode\n",
    "                print(f\"Using overwrite mode for existing table {table_name}\")\n",
    "                # df.writeTo(table_name).overwrite()\n",
    "\n",
    "                df.writeTo(table_name).using(\"iceberg\").partitionedBy(\"cluster_id\").createOrReplace()\n",
    "\n",
    "            else:\n",
    "                # Try create mode with IF NOT EXISTS to avoid conflicts\n",
    "                print(f\"Creating new table {table_name}\")\n",
    "                try:\n",
    "                    df.writeTo(table_name).using(\"iceberg\").createOrReplace()\n",
    "                except Exception as e:\n",
    "                    # Last resort - try different approach if createOrReplace fails\n",
    "                    print(f\"Error with createOrReplace: {str(e)}\")\n",
    "                    print(\"Trying alternative approach with temporary table\")\n",
    "                    \n",
    "                    # Create with temporary name first\n",
    "                    df.writeTo(temp_table_name).using(\"iceberg\").create()\n",
    "                    \n",
    "                    # Then rename or copy data if rename not supported\n",
    "                    tables = spark.catalog.listTables(f\"{custom_catalog}.gold\")\n",
    "                    if any(t.name.lower() == \"customer_segments\" for t in tables):\n",
    "                        # If target exists, drop it first\n",
    "                        spark.catalog.dropTable(table_name)\n",
    "                    \n",
    "                    # Now rename temp to target\n",
    "                    spark._jsparkSession.catalog().renameTable(temp_table_name, table_name)\n",
    "            \n",
    "            # Verify the data was written\n",
    "            verification_df = spark.table(table_name)\n",
    "            count = verification_df.count()\n",
    "            print(f\"Successfully wrote {count} records to {table_name}\")\n",
    "            return f\"Successfully loaded {count} records to {table_name}\"\n",
    "            \n",
    "        except Exception as write_error:\n",
    "            if \"XMinioStorageFull\" in str(write_error):\n",
    "                print(\"Storage full error detected\")\n",
    "                # Handle storage full error by using compact format and reducing data size\n",
    "                print(\"Attempting to write with more compact format and reduced dataset\")\n",
    "                \n",
    "                # Sample smaller dataset as emergency fallback\n",
    "                small_df = df.sample(fraction=0.1).cache()\n",
    "                \n",
    "                # Try write with most efficient settings\n",
    "                small_df.coalesce(1).writeTo(table_name).using(\"iceberg\").option(\"write-format\", \"parquet\").option(\"compression\", \"snappy\").createOrReplace()\n",
    "                \n",
    "                warn_msg = \"WARNING: Storage was full - only wrote 10% sample of data\"\n",
    "                print(warn_msg)\n",
    "                return warn_msg\n",
    "            else:\n",
    "                # Re-raise other errors\n",
    "                raise\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error in save_predictions_to_trino: {str(e)}\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "    finally:\n",
    "        if 'spark' in locals():\n",
    "            spark.stop()\n",
    "\n",
    "# Pipeline definition\n",
    "@pipeline(name=\"Customer Segmentation Pipeline\")\n",
    "def customer_segmentation_pipeline(\n",
    "    minio_bucket: str = \"ai360ctzn-customer-segmentation\"\n",
    "):\n",
    "    # Step 1: Fetch data\n",
    "    fetch_task = fetch_data_trino(minio_bucket=minio_bucket)\n",
    "    # fetch_task.set_caching_options(False)\n",
    "    \n",
    "    # Step 2: Process data and create segments\n",
    "    segmentation_task = feature_engineering_and_segmentation(\n",
    "        file_path=fetch_task.output,\n",
    "        minio_bucket=minio_bucket\n",
    "    )\n",
    "    # segmentation_task.set_caching_options(False)\n",
    "    \n",
    "    # Step 3: Save results to Trino\n",
    "    save_to_trino = save_predictions_to_trino(\n",
    "        segmentation_path=segmentation_task.outputs['final_output_path'],\n",
    "        minio_bucket=minio_bucket\n",
    "    )\n",
    "    save_to_trino.set_caching_options(False)\n",
    "\n",
    "    \n",
    "\n",
    "    save_to_trino.set_caching_options(False)\n",
    "# Compile the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    from kfp import compiler\n",
    "    compiler.Compiler().compile(\n",
    "        customer_segmentation_pipeline,\n",
    "        \"customer_segmentation_pipeline.yaml\"\n",
    "    )\n",
    "    print(\"Pipeline compiled successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8779068-b438-4998-89d8-cf64e413a173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Save predictions to Trino\n",
    "@component(\n",
    "    base_image='quay.io/datanature_dev/jupyternotebook:java_home14',\n",
    "    packages_to_install=['trino', 'pandas', 'boto3', 'urllib3']\n",
    ")\n",
    "def save_predictions_to_trino(\n",
    "    segmentation_path: str,\n",
    "    minio_bucket: str\n",
    ") -> str:\n",
    "    import trino\n",
    "    from trino.auth import BasicAuthentication\n",
    "    import pandas as pd\n",
    "    import boto3\n",
    "    import urllib3\n",
    "    import pandas as pd\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "    from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "    import os\n",
    "    import shutil\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql.functions import lit, col, when\n",
    "    \n",
    "    # variables\n",
    "    # variables\n",
    "    # access_key_id = 'admin'\n",
    "    # secret_access_key = 'dlyticaD123'\n",
    "    # minio_endpoint = 'dn-minio-tenant-hl.dn-minio-tenant.svc.cluster.local:9000'  # Replace with your MinIO server endpoint\n",
    "    # data_bucket = 'ai360fd-recommendation'\n",
    "    # apps_bucket = ''\n",
    "    # hive_metastore_uri = \"thrift://dn-hive-metastore.dn-hive-metastore.svc.cluster.local:9083\"\n",
    "    # iceberg_warehouse_location = f\"s3a://{data_bucket}/data/\"\n",
    "    # custom_catalog = \"iceberg_catalog\"\n",
    "    # spark_eventlog_dir = f\"s3a://{apps_bucket}/logs/spark/\"\n",
    "    # master_url=\"spark://dn-spark-master-svc.dn-spark.svc.cluster.local:7077\"\n",
    "    # spark_driver_host=\"notebook-five-spark-driver-headless.kubeflow-user-example-com.svc.cluster.local\"\n",
    "    # app_name=\"All Data Generation\"\n",
    "    # additional_packages=\"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.spark:spark-avro_2.12:3.5.0,com.crealytics:spark-excel_2.12:0.13.7,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.hadoop:hadoop-common:3.3.4\"\n",
    "\n",
    "    \n",
    "    access_key_id = 'admin'\n",
    "    secret_access_key = 'dlyticaD123'\n",
    "    minio_endpoint = 'dn-minio-tenant-hl.dn-minio-tenant.svc.cluster.local:9000'  # Replace with your MinIO server endpoint\n",
    "    data_bucket = 'ai360fd-recommendation'\n",
    "    # apps_bucket = 'dn-apps'\n",
    "    hive_metastore_uri = \"thrift://dn-hive-metastore.dn-hive-metastore.svc.cluster.local:9083\"\n",
    "    iceberg_warehouse_location = f\"s3a://{data_bucket}/data/\"\n",
    "    custom_catalog = \"iceberg_catalog\"\n",
    "    # spark_eventlog_dir = f\"s3a://{apps_bucket}/logs/spark/\"\n",
    "    # master_url=\"spark://dn-spark-master-svc.dn-spark.svc.cluster.local:7077\"\n",
    "    # spark_driver_host=\"notebook-five-spark-driver-headless.kubeflow-user-example-com.svc.cluster.local\"\n",
    "    app_name=\"All Data Generation\"\n",
    "    additional_packages=\"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.spark:spark-avro_2.12:3.5.0,com.crealytics:spark-excel_2.12:0.13.7,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.hadoop:hadoop-common:3.3.4\"\n",
    "\n",
    "    #spark configuration\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", minio_endpoint) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", access_key_id) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", secret_access_key) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .config(\"spark.hadoop.hive.metastore.uris\", hive_metastore_uri) \\\n",
    "        .config(\"spark.sql.warehouse.dir\", iceberg_warehouse_location) \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "        .config(f\"spark.sql.catalog.{custom_catalog}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "        .config(\"spark.jars.packages\", additional_packages) \\\n",
    "        .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "        .config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # spark.sql(f\"\"\"\n",
    "    # show tables in {custom_catalog}.gold\n",
    "    # \"\"\").show(100,False)\n",
    "    # print(\"train_model is called\")\n",
    "    # Initialize Spark session\n",
    "    # spark = SparkSession.builder.appName(\"Feature Engineering\").getOrCreate()\n",
    "    \n",
    "        \n",
    "    # Disable SSL warnings\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "    try:\n",
    "        # Download predictions from MinIO\n",
    "        minio_client = boto3.client(\n",
    "            's3',\n",
    "            endpoint_url=\"http://192.168.80.155:32000\",\n",
    "            aws_access_key_id=\"admin\",\n",
    "            aws_secret_access_key=\"dlyticaD123\",\n",
    "            verify=False\n",
    "        )\n",
    "        \n",
    "        local_path = \"/tmp/predictions.csv\"\n",
    "        minio_client.download_file(minio_bucket, segmentation_path, local_path)\n",
    "        predictions = pd.read_csv(local_path)\n",
    "        \n",
    "        # # Connect to Trino\n",
    "        # conn = trino.dbapi.connect(\n",
    "        #     host=\"192.168.80.155\",\n",
    "        #     port=30071,\n",
    "        #     user=\"ctzn.bank\",\n",
    "        #     auth=BasicAuthentication(\"ctzn.bank\", \"ctzn.bank_123\"),\n",
    "        #     catalog=\"iceberg\",\n",
    "        #     schema=\"silver_crmuser\",\n",
    "        #     http_scheme=\"https\",\n",
    "        #     verify=False\n",
    "        # )\n",
    "        # cursor = conn.cursor()\n",
    "\n",
    "        # # Create table if not exists\n",
    "        # cursor.execute(\"\"\"\n",
    "        # CREATE TABLE IF NOT EXISTS iceberg.silver_crmuser.customer_segments (\n",
    "        #     cif_id VARCHAR,\n",
    "        #     cluster_id INTEGER,\n",
    "        #     processed_date TIMESTAMP\n",
    "        # ) WITH (\n",
    "        #     partitioning = ARRAY['processed_date']\n",
    "        # )\n",
    "        # \"\"\")\n",
    "\n",
    "        # Insert predictions in batches\n",
    "        batch_size = 1000\n",
    "        for i in range(0, len(predictions), batch_size):\n",
    "            batch = predictions[i:i+batch_size]\n",
    "            values = [(str(row['custid']), int(row['prediction'])) for _, row in batch.iterrows()]\n",
    "            \n",
    "            insert_sql = \"\"\"\n",
    "            INSERT INTO iceberg.silver_crmuser.customer_segments \n",
    "            (cif_id, cluster_id, processed_date) \n",
    "            VALUES (?, ?, CURRENT_TIMESTAMP)\n",
    "            \"\"\"\n",
    "            \n",
    "            cursor.executemany(insert_sql, values)\n",
    "            conn.commit()\n",
    "\n",
    "        return f\"Successfully saved {len(predictions)} predictions to Trino\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in save_predictions_to_trino: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "# Pipeline definition\n",
    "@pipeline(name=\"Customer Segmentation Pipeline\")\n",
    "def customer_segmentation_pipeline(\n",
    "    minio_bucket: str = \"ai360ctzn-customer-segmentation\"\n",
    "):\n",
    "    # Step 1: Fetch data\n",
    "    fetch_task = fetch_data_trino(minio_bucket=minio_bucket)\n",
    "    \n",
    "    # Step 2: Process data and create segments\n",
    "    segmentation_task = feature_engineering_and_segmentation(\n",
    "        file_path=fetch_task.output,\n",
    "        minio_bucket=minio_bucket\n",
    "    )\n",
    "    \n",
    "    # Step 3: Save results to Trino\n",
    "    save_task = save_predictions_to_trino(\n",
    "        segmentation_path=segmentation_task.outputs['segmentation_path'],\n",
    "        minio_bucket=minio_bucket\n",
    "    )\n",
    "\n",
    "# Compile the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    from kfp import compiler\n",
    "    compiler.Compiler().compile(\n",
    "        customer_segmentation_pipeline,\n",
    "        \"customer_segmentation_pipeline.yaml\"\n",
    "    )\n",
    "    print(\"Pipeline compiled successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef23b2-b5d4-4fc2-b71c-cef61d17046c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

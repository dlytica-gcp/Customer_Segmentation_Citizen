{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c2c4a6a-a6bb-4ef9-a31f-d27c978d64a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "# import kfp\n",
    "# from kfp.dsl import pipeline, component, Output, Dataset, Input\n",
    "# from minio import Minio\n",
    "# from typing import NamedTuple\n",
    "# from datetime import datetime\n",
    "\n",
    "# # Step 1: Fetch data from Trino and save it to MinIO\n",
    "# @component(base_image='bitnami/spark:3.5', packages_to_install=['trino', 'pandas', 'pyarrow', 's3fs','scikit-learn','numpy','matplotlib','seaborn','pyspark','boto3'])\n",
    "# def fetch_data_trino(minio_bucket: str) -> str:\n",
    "#     from trino.dbapi import connect\n",
    "#     import csv\n",
    "#     import boto3\n",
    "\n",
    "#     # Trino Connection Details\n",
    "#     TRINO_HOST = \"192.168.80.155\"\n",
    "#     TRINO_PORT = \"30071\"\n",
    "#     TRINO_USER = \"ctzn.bank\"\n",
    "#     TRINO_PASSWORD = \"ctzn.bank_123\"\n",
    "#     TRINO_CATALOG = \"iceberg\"\n",
    "#     TRINO_SCHEMA = \"silver_crmuser\"\n",
    "    \n",
    "#     # Output CSV filename\n",
    "#     OUTPUT_FILE = \"/tmp/ctzn_10_years_data.csv\"\n",
    "\n",
    "#     # SQL Query for customer segmentation data\n",
    "#     SQL_QUERY = \"\"\" \n",
    "#     WITH recent_customers AS (\n",
    "#         SELECT DISTINCT \n",
    "#             g.cif_id\n",
    "#         FROM \n",
    "#             gold.dim_gam AS g\n",
    "#         WHERE \n",
    "#             CAST(DATE_PARSE(SUBSTRING(g.acct_opn_date, 1, 19), '%Y-%m-%dT%H:%i:%s') AS DATE) >= CURRENT_DATE - INTERVAL '5' YEAR\n",
    "#     ),\n",
    "    \n",
    "#     account_activity AS (\n",
    "#         SELECT \n",
    "#             a.cif_id,\n",
    "#             -- Balance calculations\n",
    "#             SUM(COALESCE(a.total_credit_tran_vol, 0) - COALESCE(a.total_debit_tran_vol, 0)) AS balance,\n",
    "#             COUNT(DISTINCT a.nepali_month)/6.0 AS balance_frequency,\n",
    "            \n",
    "#             -- Purchase metrics\n",
    "#             SUM(COALESCE(a.total_debit_tran_vol, 0)) AS purchases,\n",
    "#             MAX(COALESCE(a.total_debit_tran_vol, 0)) AS oneoff_purchases,\n",
    "#             SUM(COALESCE(a.total_debit_tran_vol, 0)) - MAX(COALESCE(a.total_debit_tran_vol, 0)) AS installments_purchases,\n",
    "            \n",
    "#             -- Cash advance calculations\n",
    "#             SUM(CASE WHEN COALESCE(a.total_credit_tran_vol, 0) > 0 AND COALESCE(a.total_debit_tran_vol, 0) = 0 \n",
    "#                      THEN COALESCE(a.total_credit_tran_vol, 0) ELSE 0 END) AS cash_advance,\n",
    "            \n",
    "#             -- Frequency calculations\n",
    "#             COUNT(DISTINCT a.foracid)/6.0 AS purchases_frequency,\n",
    "#             COUNT(DISTINCT CASE WHEN COALESCE(a.total_debit_tran_vol, 0) > 100000 THEN a.foracid END)/6.0 AS oneoff_purchases_frequency,\n",
    "#             COUNT(DISTINCT CASE WHEN COALESCE(a.total_debit_tran_vol, 0) BETWEEN 1 AND 100000 THEN a.foracid END)/6.0 AS purchases_installments_frequency,\n",
    "#             COUNT(DISTINCT CASE WHEN COALESCE(a.total_credit_tran_vol, 0) > 0 THEN a.foracid END)/6.0 AS cash_advance_frequency,\n",
    "            \n",
    "#             -- Transaction counts\n",
    "#             COUNT(DISTINCT CASE WHEN COALESCE(a.total_credit_tran_vol, 0) > 0 THEN a.foracid END) AS cash_advance_trx,\n",
    "#             COUNT(DISTINCT a.foracid) AS purchases_trx,\n",
    "            \n",
    "#             -- Payment behavior\n",
    "#             SUM(COALESCE(a.total_credit_tran_vol, 0)) AS payments,\n",
    "#             COUNT(DISTINCT CASE WHEN COALESCE(a.total_credit_tran_vol, 0) >= COALESCE(a.total_debit_tran_vol, 0) \n",
    "#                                 THEN a.nepali_month END)/6.0 AS prc_full_payment\n",
    "#         FROM \n",
    "#             gold.mv_fact_deposit_account_insights a\n",
    "#         JOIN \n",
    "#             recent_customers rc ON a.cif_id = rc.cif_id\n",
    "#         GROUP BY \n",
    "#             a.cif_id\n",
    "#     ),\n",
    "    \n",
    "#     salary_stats AS (\n",
    "#         SELECT \n",
    "#             APPROX_PERCENTILE(COALESCE(salary_per_month, 0), 0.5) AS median_salary,\n",
    "#             APPROX_PERCENTILE(COALESCE(salary_per_month, 0), 0.05) AS fifth_percentile_salary\n",
    "#         FROM \n",
    "#             gold.dim_customers\n",
    "#     ),\n",
    "    \n",
    "#     customer_profile AS (\n",
    "#         SELECT \n",
    "#             g.cif_id,\n",
    "#             -- Tenure calculation from account opening date\n",
    "#             DATE_DIFF('year', \n",
    "#                      CAST(DATE_PARSE(SUBSTRING(MIN(g.acct_opn_date), 1, 19), '%Y-%m-%dT%H:%i:%s') AS DATE), \n",
    "#                      CURRENT_DATE) AS tenure,\n",
    "            \n",
    "#             -- Estimated minimum payments (5% of median salary)\n",
    "#             (SELECT fifth_percentile_salary FROM salary_stats) AS minimum_payments\n",
    "#         FROM \n",
    "#             gold.dim_gam g\n",
    "#         LEFT JOIN \n",
    "#             gold.dim_customers c ON g.cif_id = c.cif_id\n",
    "#         GROUP BY \n",
    "#             g.cif_id\n",
    "#     )\n",
    "    \n",
    "#     SELECT \n",
    "#         aa.cif_id AS custid,\n",
    "#         aa.balance,\n",
    "#         aa.balance_frequency,\n",
    "#         aa.purchases,\n",
    "#         aa.oneoff_purchases,\n",
    "#         aa.installments_purchases,\n",
    "#         aa.cash_advance,\n",
    "#         aa.purchases_frequency,\n",
    "#         aa.oneoff_purchases_frequency,\n",
    "#         aa.purchases_installments_frequency,\n",
    "#         aa.cash_advance_frequency,\n",
    "#         aa.cash_advance_trx,\n",
    "#         aa.purchases_trx,\n",
    "#         -- Using estimated credit limit (3x median salary)\n",
    "#         (SELECT median_salary * 3 FROM salary_stats) AS credit_limit,\n",
    "#         aa.payments,\n",
    "#         cp.minimum_payments,\n",
    "#         aa.prc_full_payment,\n",
    "#         cp.tenure\n",
    "#     FROM \n",
    "#         account_activity aa\n",
    "#     JOIN \n",
    "#         customer_profile cp ON aa.cif_id = cp.cif_id\n",
    "#     ORDER BY \n",
    "#         aa.cif_id;\n",
    "#     \"\"\"\n",
    "\n",
    "#     try:\n",
    "#         # Connect to Trino\n",
    "#         conn = connect(\n",
    "#             host=TRINO_HOST,\n",
    "#             port=TRINO_PORT,\n",
    "#             user=TRINO_USER,\n",
    "#             password=TRINO_PASSWORD,\n",
    "#             catalog=TRINO_CATALOG,\n",
    "#             schema=TRINO_SCHEMA,\n",
    "#         )\n",
    "#         cursor = conn.cursor()\n",
    "        \n",
    "#         # Execute the query\n",
    "#         cursor.execute(SQL_QUERY)\n",
    "        \n",
    "#         # Save the result to a CSV file\n",
    "#         columns = [desc[0] for desc in cursor.description]\n",
    "#         with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "#             writer = csv.writer(f)\n",
    "#             writer.writerow(columns)  # Write header\n",
    "#             rows = cursor.fetchall()\n",
    "#             writer.writerows(rows)  # Write data\n",
    "\n",
    "#         conn.close()\n",
    "        \n",
    "#         # Upload CSV to MinIO\n",
    "#         minio_client = boto3.client(\n",
    "#             's3',\n",
    "#             endpoint_url=\"http://192.168.80.155:32000\",\n",
    "#             aws_access_key_id=\"admin\",\n",
    "#             aws_secret_access_key=\"dlyticaD123\"\n",
    "#         )\n",
    "#         current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "#         saving_file_name = f\"data/customer_segmentation_raw_data_{current_date}.csv\"\n",
    "#         minio_client.upload_file(OUTPUT_FILE, minio_bucket, saving_file_name)\n",
    "        \n",
    "#         return saving_file_name\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error occurred: {str(e)}\")\n",
    "#         return None\n",
    "\n",
    "# # Step 2: Perform Feature Engineering and Customer Segmentation\n",
    "# # Step 2: Perform Feature Engineering and Customer Segmentation\n",
    "# @component(base_image='quay.io/datanature_dev/jupyternotebook:java_home14', packages_to_install=['trino', 'pandas', 'pyarrow', 's3fs','scikit-learn','numpy','matplotlib','seaborn','pyspark','boto3'])\n",
    "# def feature_engineering_and_segmentation(file_path: str, minio_bucket: str) -> str:\n",
    "#     # Initialize Spark session\n",
    "#     spark = SparkSession.builder \\\n",
    "#         .appName(\"CustomerSegmentation\") \\\n",
    "#         .config(\"spark.driver.memory\", \"30g\") \\\n",
    "#         .config(\"spark.executor.memory\", \"30g\") \\\n",
    "#         .config(\"spark.executor.memoryOverhead\", \"18g\") \\\n",
    "#         .config(\"spark.driver.maxResultSize\", \"18g\") \\\n",
    "#         .config(\"spark.executor.cores\", \"4\") \\\n",
    "#         .config(\"spark.executor.instances\", \"4\") \\\n",
    "#         .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "#         .config(\"spark.dynamicAllocation.minExecutors\", \"2\") \\\n",
    "#         .config(\"spark.dynamicAllocation.maxExecutors\", \"10\") \\\n",
    "#         .config(\"spark.dynamicAllocation.initialExecutors\", \"4\") \\\n",
    "#         .config(\"spark.default.parallelism\", \"200\") \\\n",
    "#         .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "#         .enableHiveSupport() \\\n",
    "#         .getOrCreate()\n",
    "\n",
    "#     # Initialize MinIO client\n",
    "#     minio_client = boto3.client(\n",
    "#         's3',\n",
    "#         endpoint_url=\"http://192.168.80.155:32000\",\n",
    "#         aws_access_key_id=\"admin\",\n",
    "#         aws_secret_access_key=\"dlyticaD123\"\n",
    "#     )\n",
    "    \n",
    "#     # Download raw data from MinIO\n",
    "#     local_raw_data_path = \"/tmp/raw_data.csv\"\n",
    "#     minio_client.download_file(minio_bucket, file_path, local_raw_data_path)\n",
    "    \n",
    "#     # Load Data\n",
    "#     df = spark.read.csv(local_raw_data_path, header=True, inferSchema=True)\n",
    "\n",
    "#     # Display basic information\n",
    "#     print(\"Total number of records:\", df.count())\n",
    "#     print(\"Schema:\")\n",
    "#     df.printSchema()\n",
    "\n",
    "#     # Handle missing values (fill with mean for numerical columns, \"Unknown\" for categorical columns)\n",
    "#     def handle_missing_values(df):\n",
    "#         categorical_cols = ['gender', 'employment_status', 'marital_status', 'occupation', 'age_group', 'withdrawal_trends']\n",
    "#         numerical_cols = [\n",
    "#             'total_accounts', 'total_saving_accounts', 'total_fixed_accounts', 'total_loan_accounts',\n",
    "#             'total_overdraft_accounts', 'total_debit_transaction_count', 'total_credit_transaction_count',\n",
    "#             'total_debit_transaction_amount', 'total_credit_transaction_amount', 'average_debit_transaction_count',\n",
    "#             'average_credit_transaction_count', 'average_debit_amount', 'average_credit_amount',\n",
    "#             'average_yearly_saving', 'total_yearly_saving', 'total_dormant_days', 'total_dormant_account',\n",
    "#             'total_active_account', 'first_account_opened_days', 'last_account_opened_days'\n",
    "#         ]\n",
    "        \n",
    "#         # Fill missing values\n",
    "#         df = df.fillna(0, subset=numerical_cols)\n",
    "#         df = df.fillna(\"Unknown\", subset=categorical_cols)\n",
    "        \n",
    "#         return df\n",
    "\n",
    "#     df = handle_missing_values(df)\n",
    "\n",
    "#     # Handle outliers using Z-score method (already implemented in your code)\n",
    "#     def handle_outliers_zscore(df, cols, threshold=3.0):\n",
    "#         \"\"\"Handle outliers in spark dataframe using z-score method\"\"\"\n",
    "#         for c in cols:\n",
    "#             stats = df.select(mean(col(c)).alias(\"mean\"), stddev(col(c)).alias(\"stddev\")).collect()[0]\n",
    "#             mean_val, stddev_val = stats[\"mean\"], stats[\"stddev\"]\n",
    "#             if stddev_val is None or stddev_val == 0:\n",
    "#                 continue\n",
    "#             df = df.withColumn(\n",
    "#                 c,\n",
    "#                 when((col(c) - mean_val) / stddev_val > threshold, mean_val + threshold * stddev_val)\n",
    "#                 .when((col(c) - mean_val) / stddev_val < -threshold, mean_val - threshold * stddev_val)\n",
    "#                 .otherwise(col(c))\n",
    "#             )\n",
    "#         return df\n",
    "\n",
    "#     numeric_cols = [field.name for field in df.schema.fields \n",
    "#                     if isinstance(df.schema[field.name].dataType, DoubleType) \n",
    "#                     or str(df.schema[field.name].dataType).startswith('IntegerType') \n",
    "#                     or str(df.schema[field.name].dataType).startswith('LongType')]\n",
    "#     df = handle_outliers_zscore(df, numeric_cols)\n",
    "\n",
    "#     # Create feature engineering columns (as per your provided code)\n",
    "#     df = df.withColumn(\"new_balance_balance_frequency\", col(\"balance\") * col(\"balance_frequency\"))\n",
    "#     df = df.withColumn(\"new_oneoff_purchases_purchases\", \n",
    "#                       when(col(\"purchases\") != 0, col(\"oneoff_purchases\") / col(\"purchases\")).otherwise(0))\n",
    "#     df = df.withColumn(\"new_installments_purchases_purchases\", \n",
    "#                       when(col(\"purchases\") != 0, col(\"installments_purchases\") / col(\"purchases\")).otherwise(0))\n",
    "#     df = df.withColumn(\"new_cash_advance_purchases_purchases\", col(\"cash_advance\") * col(\"cash_advance_frequency\"))\n",
    "#     df = df.withColumn(\"new_purchases_purchases_frequency\", col(\"purchases\") * col(\"purchases_frequency\"))\n",
    "#     df = df.withColumn(\"new_purchases_oneoff_purchases_frequency\", col(\"purchases\") * col(\"oneoff_purchases_frequency\"))\n",
    "#     df = df.withColumn(\"new_purchases_purchases_trx\", \n",
    "#                       when(col(\"purchases_trx\") != 0, col(\"purchases\") / col(\"purchases_trx\")).otherwise(0))\n",
    "#     df = df.withColumn(\"new_cash_advance_cash_advance_trx\", \n",
    "#                       when(col(\"cash_advance_trx\") != 0, col(\"cash_advance\") / col(\"cash_advance_trx\")).otherwise(0))\n",
    "#     df = df.withColumn(\"new_balance_credit_limit\", \n",
    "#                       when(col(\"credit_limit\") != 0, col(\"balance\") / col(\"credit_limit\")).otherwise(0))\n",
    "#     df = df.withColumn(\"new_payments_credit_limit\", \n",
    "#                       when(col(\"minimum_payments\") != 0, col(\"payments\") / col(\"minimum_payments\")).otherwise(0))\n",
    "\n",
    "#     # Fill remaining NaN values with 0\n",
    "#     df = df.na.fill(0)\n",
    "\n",
    "#     # Prepare features for clustering (drop cif_id)\n",
    "#     if \"cif_id\" in df.columns:\n",
    "#         features_df = df.drop(\"cif_id\")\n",
    "#     else:\n",
    "#         features_df = df\n",
    "\n",
    "#     # Filter out non-numeric columns for the assembler\n",
    "#     numeric_cols = [col_name for col_name, dtype in features_df.dtypes \n",
    "#                     if dtype != 'string' and not dtype.startswith('array') \n",
    "#                     and not col_name == 'cif_id']\n",
    "\n",
    "#     # Assemble features into a vector\n",
    "#     assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "#     assembled_data = assembler.transform(features_df)\n",
    "\n",
    "#     # Standardize features\n",
    "#     scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "#     scaler_model = scaler.fit(assembled_data)\n",
    "#     scaled_data = scaler_model.transform(assembled_data)\n",
    "\n",
    "#     # Apply PCA for dimensionality reduction\n",
    "#     pca = PCA(k=2, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
    "#     pca_model = pca.fit(scaled_data)\n",
    "#     pca_data = pca_model.transform(scaled_data)\n",
    "\n",
    "#     # Explained variance ratio\n",
    "#     explained_variance = pca_model.explainedVariance.toArray()\n",
    "#     print(\"Explained variance ratio:\", explained_variance)\n",
    "#     print(\"Total variance explained:\", sum(explained_variance))\n",
    "\n",
    "#     # Find optimal number of clusters using Silhouette score and Elbow method\n",
    "#     def find_optimal_clusters(data, max_k=10):\n",
    "#         costs = []\n",
    "#         silhouettes = []\n",
    "#         evaluator = ClusteringEvaluator(predictionCol=\"prediction\", featuresCol=\"pca_features\", \n",
    "#                                        metricName=\"silhouette\")\n",
    "        \n",
    "#         for k in range(2, max_k + 1):\n",
    "#             kmeans = KMeans(k=k, seed=42, featuresCol=\"pca_features\")\n",
    "#             model = kmeans.fit(data)\n",
    "#             cost = model.summary.trainingCost\n",
    "#             costs.append(cost)\n",
    "            \n",
    "#             # Make predictions\n",
    "#             predictions = model.transform(data)\n",
    "#             silhouette = evaluator.evaluate(predictions)\n",
    "#             silhouettes.append(silhouette)\n",
    "            \n",
    "#             print(f\"k={k}, Cost={cost}, Silhouette Score={silhouette}\")\n",
    "        \n",
    "#         return costs, silhouettes\n",
    "\n",
    "#     costs, silhouettes = find_optimal_clusters(pca_data, max_k=10)\n",
    "\n",
    "#     # Determine optimal k based on silhouette score\n",
    "#     optimal_k = silhouette_df.loc[silhouette_df['silhouette'].idxmax(), 'k']\n",
    "#     print(f\"Optimal number of clusters based on silhouette score: {optimal_k}\")\n",
    "\n",
    "#     kmeans = KMeans(k=optimal_k, seed=42, featuresCol=\"pca_features\")\n",
    "#     kmeans_model = kmeans.fit(pca_data)\n",
    "#     clustered_data = kmeans_model.transform(pca_data)\n",
    "\n",
    "#     # Prepare results for cluster visualization and analysis\n",
    "#     pandas_df = clustered_data.select(\"pca_features\", \"prediction\").toPandas()\n",
    "#     pandas_df['pca1'] = pandas_df['pca_features'].apply(lambda x: float(x[0]))\n",
    "#     pandas_df['pca2'] = pandas_df['pca_features'].apply(lambda x: float(x[1]))\n",
    "\n",
    "#     # Plot clusters\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     scatter = plt.scatter(pandas_df['pca1'], pandas_df['pca2'], c=pandas_df['prediction'], cmap='viridis', alpha=0.5)\n",
    "#     plt.colorbar(scatter, label='Cluster')\n",
    "#     plt.title('Customer Segmentation - PCA Visualization')\n",
    "#     plt.xlabel('Principal Component 1')\n",
    "#     plt.ylabel('Principal Component 2')\n",
    "#     cluster_image_path = \"/tmp/cluster_visualization.png\"\n",
    "#     plt.savefig(cluster_image_path)\n",
    "\n",
    "#     # Save visualization to MinIO\n",
    "#     # Save visualization to MinIO\n",
    "#     minio_client.upload_file(cluster_image_path, minio_bucket, f\"images/customer_segmentation_{datetime.now().strftime('%Y-%m-%d')}_cluster_visualization.png\")\n",
    "\n",
    "#     # Save clustering results to CSV\n",
    "#     final_output_path = f\"/tmp/customer_segmentation_output_{datetime.now().strftime('%Y-%m-%d')}.csv\"\n",
    "#     clustered_data.toPandas().to_csv(final_output_path, index=False)\n",
    "\n",
    "#     # Upload the output file to MinIO\n",
    "#     minio_client.upload_file(final_output_path, minio_bucket, f\"data/customer_segmentation_output_{datetime.now().strftime('%Y-%m-%d')}.csv\")\n",
    "\n",
    "#     return final_output_path\n",
    "\n",
    "# # Step 3: Save final cluster predictions into Trino\n",
    "# @component(base_image='bitnami/spark:3.5', packages_to_install=['trino','minio','boto3'])\n",
    "# def save_predictions_to_trino(prediction_file: str) -> None:\n",
    "#     from trino.dbapi import connect\n",
    "#     import pandas as pd\n",
    "\n",
    "#     # Trino Connection Details\n",
    "#     TRINO_HOST = \"192.168.80.155\"\n",
    "#     TRINO_PORT = \"30071\"\n",
    "#     TRINO_USER = \"ctzn.bank\"\n",
    "#     TRINO_PASSWORD = \"ctzn.bank_123\"\n",
    "#     TRINO_CATALOG = \"iceberg\"\n",
    "#     TRINO_SCHEMA = \"silver_crmuser\"\n",
    "\n",
    "#     # Load the predictions data from MinIO (already saved in previous step)\n",
    "#     predictions_df = pd.read_csv(prediction_file)\n",
    "\n",
    "#     # Save predictions to Trino (Iceberg catalog)\n",
    "#     conn = connect(\n",
    "#         host=TRINO_HOST,\n",
    "#         port=TRINO_PORT,\n",
    "#         user=TRINO_USER,\n",
    "#         password=TRINO_PASSWORD,\n",
    "#         catalog=TRINO_CATALOG,\n",
    "#         schema=TRINO_SCHEMA,\n",
    "#     )\n",
    "#     cursor = conn.cursor()\n",
    "\n",
    "#     # Assume predictions_df contains 'cif_id' and 'prediction' columns\n",
    "#     for index, row in predictions_df.iterrows():\n",
    "#         cif_id = row['cif_id']\n",
    "#         prediction = row['prediction']\n",
    "        \n",
    "#         # Insert prediction data into Trino\n",
    "#         cursor.execute(\n",
    "#             f\"INSERT INTO {TRINO_CATALOG}.{TRINO_SCHEMA}.customer_predictions (cif_id, prediction) VALUES ({cif_id}, {prediction})\"\n",
    "#         )\n",
    "\n",
    "#     conn.commit()\n",
    "#     conn.close()\n",
    "\n",
    "# # Step 4: Define the full pipeline\n",
    "# @pipeline(name=\"Customer Segmentation Pipeline\")\n",
    "# def customer_segmentation_pipeline(minio_bucket: str = \"ai360ctzn-customer-segmentation\"):\n",
    "#     # Step 1: Fetch Data from Trino and save it to MinIO\n",
    "#     data_task = fetch_data_trino(minio_bucket=minio_bucket)\n",
    "\n",
    "#     # Step 2: Perform Feature Engineering and Customer Segmentation\n",
    "#     segmentation_task = feature_engineering_and_segmentation(file_path=data_task.output, minio_bucket=minio_bucket)\n",
    "\n",
    "#     # Step 3: Save predictions into Trino\n",
    "#     save_predictions_task = save_predictions_to_trino(prediction_file=segmentation_task.output)\n",
    "\n",
    "# # Compile the pipeline\n",
    "# from kfp import compiler\n",
    "# compiler.Compiler().compile(customer_segmentation_pipeline, \"customer_segmentation_pipeline.yaml\")\n",
    "\n",
    "# print(\"Pipeline compiled successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5409186b-a5c7-4ba1-b34d-1cf68b5cd79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp in /opt/conda/lib/python3.11/site-packages (2.12.1)\n",
      "Collecting minio\n",
      "  Using cached minio-7.2.15-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting trino\n",
      "  Using cached trino-0.333.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement matlpotlib (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for matlpotlib\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kfp minio trino pandas numpy matlpotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22a55af-7a0b-422e-bf83-fb2cb9570189",
   "metadata": {},
   "source": [
    "### Final corrected pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "91fc1604-7ff7-47ae-afec-dd60187f144e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp.dsl import pipeline, component\n",
    "from datetime import datetime\n",
    "from typing import NamedTuple\n",
    "\n",
    "# Step 1: Fetch data from Trino and save it to MinIO\n",
    "@component(\n",
    "    base_image='bitnami/spark:3.5', \n",
    "    packages_to_install=['trino', 'pandas', 'pyarrow', 's3fs', 'boto3', 'urllib3']\n",
    ")\n",
    "def fetch_data_trino(minio_bucket: str) -> str:\n",
    "    import trino\n",
    "    from trino.auth import BasicAuthentication\n",
    "    import csv\n",
    "    import boto3\n",
    "    import os\n",
    "    import urllib3\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Disable SSL warnings\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "    # Trino Connection Details\n",
    "    TRINO_HOST = \"192.168.80.155\"\n",
    "    TRINO_PORT = \"30071\"\n",
    "    TRINO_USER = \"ctzn.bank\"\n",
    "    TRINO_PASSWORD = \"ctzn.bank_123\"\n",
    "    TRINO_CATALOG = \"iceberg\"\n",
    "    TRINO_SCHEMA = \"silver_crmuser\"\n",
    "    TRINO_HTTP_SCHEME = \"https\"\n",
    "\n",
    "    # Output file\n",
    "    OUTPUT_FILE = \"/tmp/ctzn_10_years_data.csv\"\n",
    "\n",
    "    # SQL Query\n",
    "    SQL_QUERY = \"\"\"\n",
    "    WITH recent_customers AS (\n",
    "        SELECT DISTINCT g.cif_id\n",
    "        FROM gold.dim_gam AS g\n",
    "        WHERE CAST(DATE_PARSE(SUBSTRING(g.acct_opn_date, 1, 19), '%Y-%m-%dT%H:%i:%s') AS DATE) >= CURRENT_DATE - INTERVAL '10' YEAR\n",
    "    ),\n",
    "    account_activity AS (\n",
    "        SELECT \n",
    "            a.cif_id,\n",
    "            SUM(COALESCE(a.total_credit_tran_vol, 0) - COALESCE(a.total_debit_tran_vol, 0)) AS balance,\n",
    "            COUNT(DISTINCT a.nepali_month)/6.0 AS balance_frequency,\n",
    "            SUM(COALESCE(a.total_debit_tran_vol, 0)) AS purchases,\n",
    "            MAX(COALESCE(a.total_debit_tran_vol, 0)) AS oneoff_purchases,\n",
    "            SUM(COALESCE(a.total_debit_tran_vol, 0)) - MAX(COALESCE(a.total_debit_tran_vol, 0)) AS installments_purchases,\n",
    "            SUM(CASE WHEN COALESCE(a.total_credit_tran_vol, 0) > 0 AND COALESCE(a.total_debit_tran_vol, 0) = 0 \n",
    "                    THEN COALESCE(a.total_credit_tran_vol, 0) ELSE 0 END) AS cash_advance,\n",
    "            COUNT(DISTINCT a.foracid)/6.0 AS purchases_frequency,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_debit_tran_vol, 0) > 100000 THEN a.foracid END)/6.0 AS oneoff_purchases_frequency,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_debit_tran_vol, 0) BETWEEN 1 AND 100000 THEN a.foracid END)/6.0 AS purchases_installments_frequency,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_credit_tran_vol, 0) > 0 THEN a.foracid END)/6.0 AS cash_advance_frequency,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_credit_tran_vol, 0) > 0 THEN a.foracid END) AS cash_advance_trx,\n",
    "            COUNT(DISTINCT a.foracid) AS purchases_trx,\n",
    "            SUM(COALESCE(a.total_credit_tran_vol, 0)) AS payments,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_credit_tran_vol, 0) >= COALESCE(a.total_debit_tran_vol, 0) \n",
    "                                THEN a.nepali_month END)/6.0 AS prc_full_payment\n",
    "        FROM gold.mv_fact_deposit_account_insights a\n",
    "        JOIN recent_customers rc ON a.cif_id = rc.cif_id\n",
    "        GROUP BY a.cif_id\n",
    "    ),\n",
    "    salary_stats AS (\n",
    "        SELECT \n",
    "            APPROX_PERCENTILE(COALESCE(salary_per_month, 0), 0.5) AS median_salary,\n",
    "            APPROX_PERCENTILE(COALESCE(salary_per_month, 0), 0.05) AS fifth_percentile_salary\n",
    "        FROM gold.dim_customers\n",
    "    ),\n",
    "    customer_profile AS (\n",
    "        SELECT \n",
    "            g.cif_id,\n",
    "            DATE_DIFF('year', \n",
    "                     CAST(DATE_PARSE(SUBSTRING(MIN(g.acct_opn_date), 1, 19), '%Y-%m-%dT%H:%i:%s') AS DATE), \n",
    "                     CURRENT_DATE) AS tenure,\n",
    "            (SELECT fifth_percentile_salary FROM salary_stats) AS minimum_payments\n",
    "        FROM gold.dim_gam g\n",
    "        LEFT JOIN gold.dim_customers c ON g.cif_id = c.cif_id\n",
    "        GROUP BY g.cif_id\n",
    "    )\n",
    "    SELECT \n",
    "        aa.cif_id AS custid,\n",
    "        aa.balance,\n",
    "        aa.balance_frequency,\n",
    "        aa.purchases,\n",
    "        aa.oneoff_purchases,\n",
    "        aa.installments_purchases,\n",
    "        aa.cash_advance,\n",
    "        aa.purchases_frequency,\n",
    "        aa.oneoff_purchases_frequency,\n",
    "        aa.purchases_installments_frequency,\n",
    "        aa.cash_advance_frequency,\n",
    "        aa.cash_advance_trx,\n",
    "        aa.purchases_trx,\n",
    "        (SELECT median_salary * 3 FROM salary_stats) AS credit_limit,\n",
    "        aa.payments,\n",
    "        cp.minimum_payments,\n",
    "        aa.prc_full_payment,\n",
    "        cp.tenure\n",
    "    FROM account_activity aa\n",
    "    JOIN customer_profile cp ON aa.cif_id = cp.cif_id\n",
    "    ORDER BY aa.cif_id\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Connect to Trino\n",
    "        conn = trino.dbapi.connect(\n",
    "            host=TRINO_HOST,\n",
    "            port=TRINO_PORT,\n",
    "            user=TRINO_USER,\n",
    "            auth=BasicAuthentication(TRINO_USER, TRINO_PASSWORD),\n",
    "            catalog=TRINO_CATALOG,\n",
    "            schema=TRINO_SCHEMA,\n",
    "            http_scheme=TRINO_HTTP_SCHEME,\n",
    "            request_timeout=600,\n",
    "            verify=False\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Execute query and fetch data\n",
    "        cursor.execute(SQL_QUERY)\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        \n",
    "        # Write to CSV in batches\n",
    "        with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(columns)\n",
    "            \n",
    "            while True:\n",
    "                rows = cursor.fetchmany(1000)\n",
    "                if not rows:\n",
    "                    break\n",
    "                writer.writerows(rows)\n",
    "        \n",
    "        # Upload to MinIO\n",
    "        minio_client = boto3.client(\n",
    "            's3',\n",
    "            endpoint_url=\"http://192.168.80.155:32000\",\n",
    "            aws_access_key_id=\"admin\",\n",
    "            aws_secret_access_key=\"dlyticaD123\",\n",
    "            verify=False\n",
    "        )\n",
    "        \n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        minio_path = f\"data/customer_segmentation_raw_{current_date}.csv\"\n",
    "        minio_client.upload_file(OUTPUT_FILE, minio_bucket, minio_path)\n",
    "        \n",
    "        return minio_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in fetch_data_trino: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "\n",
    "@component(\n",
    "    base_image='quay.io/datanature_dev/jupyternotebook:java_home14',\n",
    "    packages_to_install=[\n",
    "        'pyspark', 'pandas', 'boto3', 'scikit-learn', \n",
    "        'matplotlib', 'numpy', 'trino', 'urllib3'\n",
    "    ]\n",
    ")\n",
    "def feature_engineering_and_segmentation(\n",
    "    file_path: str, \n",
    "    minio_bucket: str\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('segmentation_path', str),\n",
    "    ('model_path', str),\n",
    "    ('cluster_plot_path', str)\n",
    "]):\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col, when, mean, monotonically_increasing_id, stddev, count, abs, lit\n",
    "    from pyspark.sql.types import DoubleType, StringType\n",
    "    from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\n",
    "    from pyspark.ml.clustering import BisectingKMeans\n",
    "    from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "    from pyspark.sql.functions import udf\n",
    "    import boto3\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from datetime import datetime\n",
    "    from collections import namedtuple\n",
    "    import urllib3\n",
    "    import os\n",
    "\n",
    "    # Disable SSL warnings\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "    # Initialize Spark with optimized settings\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"AdvancedCustomerSegmentation\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Initialize MinIO client\n",
    "    minio_client = boto3.client(\n",
    "        's3',\n",
    "        endpoint_url=\"http://192.168.80.155:32000\",\n",
    "        aws_access_key_id=\"admin\",\n",
    "        aws_secret_access_key=\"dlyticaD123\",\n",
    "        verify=False\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        today = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Download data from MinIO\n",
    "        local_path = \"/tmp/raw_data.csv\"\n",
    "        minio_client.download_file(minio_bucket, file_path, local_path)\n",
    "        df = spark.read.csv(local_path, header=True, inferSchema=True)\n",
    "\n",
    "        if 'custid' in df.columns:\n",
    "            df = df.withColumnRenamed(\"custid\", \"cif_id\")\n",
    "\n",
    "        # Enhanced feature engineering\n",
    "        df = df.withColumn(\"balance_utilization_ratio\", \n",
    "            when(col(\"credit_limit\") != 0, col(\"balance\") / col(\"credit_limit\")).otherwise(0))\n",
    "        df = df.withColumn(\"cash_advance_intensity\", \n",
    "            when(col(\"credit_limit\") != 0, col(\"cash_advance\") / col(\"credit_limit\")).otherwise(0))\n",
    "        df = df.withColumn(\"payment_effort_ratio\", \n",
    "            when(col(\"minimum_payments\") != 0, col(\"payments\") / col(\"minimum_payments\")).otherwise(0))\n",
    "        df = df.withColumn(\"purchase_diversity\", \n",
    "            abs(col(\"oneoff_purchases\") - col(\"installments_purchases\")) / \n",
    "            (abs(col(\"oneoff_purchases\") + col(\"installments_purchases\")) + 1e-10))\n",
    "        df = df.na.fill(0)\n",
    "\n",
    "        # Feature selection\n",
    "        numeric_cols = [\n",
    "            'purchases', 'oneoff_purchases', 'installments_purchases', \n",
    "            'cash_advance', 'cash_advance_frequency', 'balance', 'credit_limit', \n",
    "            'payments', 'minimum_payments', 'balance_utilization_ratio',\n",
    "            'cash_advance_intensity', 'payment_effort_ratio', 'purchase_diversity'\n",
    "        ]\n",
    "\n",
    "        # Feature transformation pipeline\n",
    "        assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "        assembled = assembler.transform(df.drop(\"cif_id\"))\n",
    "        scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "        scaled_data = scaler.fit(assembled).transform(assembled)\n",
    "\n",
    "        # Dimensionality reduction\n",
    "        pca = PCA(k=5, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
    "        pca_data = pca.fit(scaled_data).transform(scaled_data)\n",
    "\n",
    "        # Clustering optimization\n",
    "        evaluator = ClusteringEvaluator(featuresCol='pca_features')\n",
    "        k_range = range(6, 15)\n",
    "        k_metrics = []\n",
    "\n",
    "        for k in k_range:\n",
    "            kmeans = BisectingKMeans(k=k, seed=42, featuresCol=\"pca_features\")\n",
    "            model = kmeans.fit(pca_data)\n",
    "            predictions = model.transform(pca_data)\n",
    "            \n",
    "            cost = model.summary.trainingCost if hasattr(model.summary, 'trainingCost') else 0\n",
    "            silhouette = evaluator.evaluate(predictions)\n",
    "            \n",
    "            centers = model.clusterCenters()\n",
    "            separation = np.mean([np.min(np.linalg.norm(centers[i] - centers[j])) \n",
    "                               for i in range(len(centers)) \n",
    "                               for j in range(i+1, len(centers))])\n",
    "            \n",
    "            k_metrics.append({'k': k, 'cost': cost, 'silhouette': silhouette, 'separation': separation})\n",
    "\n",
    "        k_metrics = pd.DataFrame(k_metrics)\n",
    "        k_metrics['normalized_cost'] = (k_metrics['cost'] - k_metrics['cost'].min()) / (k_metrics['cost'].max() - k_metrics['cost'].min())\n",
    "        k_metrics['normalized_silhouette'] = (k_metrics['silhouette'] - k_metrics['silhouette'].min()) / (k_metrics['silhouette'].max() - k_metrics['silhouette'].min())\n",
    "        k_metrics['normalized_separation'] = (k_metrics['separation'] - k_metrics['separation'].min()) / (k_metrics['separation'].max() - k_metrics['separation'].min())\n",
    "        k_metrics['composite_score'] = (\n",
    "            0.3 * (1 - k_metrics['normalized_cost']) +\n",
    "            0.4 * k_metrics['normalized_silhouette'] +\n",
    "            0.3 * k_metrics['normalized_separation']\n",
    "        )\n",
    "        \n",
    "        optimal_k = k_metrics.loc[k_metrics['composite_score'].idxmax(), 'k']\n",
    "\n",
    "        # Plot cluster diagnostics\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15))\n",
    "        ax1.plot(k_metrics['k'], k_metrics['cost'], marker='o', color='blue')\n",
    "        ax1.set_xlabel('Number of Clusters (k)')\n",
    "        ax1.set_ylabel('Cost', color='blue')\n",
    "        ax1.grid(True)\n",
    "\n",
    "        ax2.plot(k_metrics['k'], k_metrics['silhouette'], marker='x', color='green')\n",
    "        ax2.set_xlabel('Number of Clusters (k)')\n",
    "        ax2.set_ylabel('Silhouette Score', color='green')\n",
    "        ax2.grid(True)\n",
    "\n",
    "        ax3.plot(k_metrics['k'], k_metrics['separation'], marker='^', color='red')\n",
    "        ax3.set_xlabel('Number of Clusters (k)')\n",
    "        ax3.set_ylabel('Cluster Separation', color='red')\n",
    "        ax3.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        cluster_plot_path = \"/tmp/cluster_diagnostics.png\"\n",
    "        plt.savefig(cluster_plot_path)\n",
    "        plt.close()\n",
    "\n",
    "        # Final clustering\n",
    "        kmeans = BisectingKMeans(k=optimal_k, seed=42, featuresCol=\"pca_features\")\n",
    "        model = kmeans.fit(pca_data)\n",
    "        clustered_data = model.transform(pca_data)\n",
    "\n",
    "        # Add cluster info to original data\n",
    "        df_with_id = df.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "        clustered_data = clustered_data.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "        clustered_data = clustered_data.join(df_with_id.select(\"row_id\", \"cif_id\"), on=\"row_id\").drop(\"row_id\")\n",
    "\n",
    "        # Generate cluster profiles and interpretations\n",
    "        summary = clustered_data.groupBy(\"prediction\").agg(\n",
    "            *[mean(col(c)).alias(c) for c in numeric_cols],\n",
    "            count(\"cif_id\").alias(\"cluster_size\")\n",
    "        )\n",
    "        cluster_profiles = summary.toPandas().set_index(\"prediction\")\n",
    "        \n",
    "        # Create a mapping dictionary for cluster interpretations\n",
    "        interpretations = {}\n",
    "        for cluster in cluster_profiles.index:\n",
    "            components = []\n",
    "            if 'purchases' in cluster_profiles.columns:\n",
    "                spend = cluster_profiles.loc[cluster, 'purchases']\n",
    "                mean_spend = cluster_profiles['purchases'].mean()\n",
    "                if spend > mean_spend + 1.5 * cluster_profiles['purchases'].std():\n",
    "                    components.append(\"High Spender\")\n",
    "                elif spend < mean_spend - 1.5 * cluster_profiles['purchases'].std():\n",
    "                    components.append(\"Low Spender\")\n",
    "                else:\n",
    "                    components.append(\"Moderate Spender\")\n",
    "            \n",
    "            if 'cash_advance' in cluster_profiles.columns:\n",
    "                cash = cluster_profiles.loc[cluster, 'cash_advance']\n",
    "                if cash > cluster_profiles['cash_advance'].mean() * 2:\n",
    "                    components.append(\"Heavy Cash Advance User\")\n",
    "            \n",
    "            interpretations[cluster] = \"; \".join(components) if components else \"Standard Customer\"\n",
    "\n",
    "        # Create a broadcast variable for the interpretations\n",
    "        interpretations_bc = spark.sparkContext.broadcast(interpretations)\n",
    "\n",
    "        # Define a function to get interpretation that will work with Spark\n",
    "        def get_interpretation(cluster_id):\n",
    "            return interpretations_bc.value.get(cluster_id, \"Unknown\")\n",
    "\n",
    "        # Register the UDF\n",
    "        from pyspark.sql.types import StringType\n",
    "        get_interpretation_udf = udf(get_interpretation, StringType())\n",
    "\n",
    "        # Apply the interpretation\n",
    "        clustered_data = clustered_data.withColumn(\n",
    "            \"interpretation\", \n",
    "            get_interpretation_udf(col(\"prediction\"))\n",
    "        )\n",
    "\n",
    "        # Save results to MinIO\n",
    "        final_df = pd.DataFrame(\n",
    "            clustered.select(\"cif_id\", \"prediction\", \"interpretation\")\n",
    "                 .rdd.map(lambda row: row.asDict())\n",
    "                 .collect()\n",
    "            )\n",
    "\n",
    "        # Save CSV to MinIO\n",
    "        final_csv = f\"/tmp/segmentation_{today}.csv\"\n",
    "        final_df.to_csv(final_csv, index=False)\n",
    "        minio_seg_path = f\"results/segmentation_{today}.csv\"\n",
    "        minio_client.upload_file(final_csv, minio_bucket, minio_seg_path)\n",
    "        \n",
    "        # Save plot to MinIO\n",
    "        minio_plot_path = f\"results/cluster_diagnostics_{today}.png\"\n",
    "        minio_client.upload_file(cluster_plot_path, minio_bucket, minio_plot_path)\n",
    "        \n",
    "        # Save model to MinIO\n",
    "        model_path = f\"/tmp/segmentation_model_{today}\"\n",
    "        model.write().overwrite().save(model_path)\n",
    "        minio_model_path = f\"models/segmentation_model_{today}\"\n",
    "        for root, _, files in os.walk(model_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(file_path, model_path)\n",
    "                minio_client.upload_file(\n",
    "                    file_path, \n",
    "                    minio_bucket, \n",
    "                    f\"{minio_model_path}/{relative_path}\"\n",
    "                )\n",
    "\n",
    "        # Save to Trino\n",
    "        from trino.dbapi import connect\n",
    "        from trino.auth import BasicAuthentication\n",
    "        \n",
    "        conn = connect(\n",
    "            host=\"192.168.80.155\",\n",
    "            port=30071,\n",
    "            user=\"ctzn.bank\",\n",
    "            auth=BasicAuthentication(\"ctzn.bank\", \"ctzn.bank_123\"),\n",
    "            catalog=\"iceberg\",\n",
    "            schema=\"silver_crmuser\",\n",
    "            http_scheme=\"https\",\n",
    "            verify=False\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Create table if not exists\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS iceberg.silver_crmuser.cluster_predictions (\n",
    "            cif_id VARCHAR,\n",
    "            cluster_id INTEGER,\n",
    "            interpretation VARCHAR,\n",
    "            processed_date TIMESTAMP\n",
    "        ) WITH (\n",
    "            partitioning = ARRAY['processed_date']\n",
    "        )\n",
    "        \"\"\")\n",
    "\n",
    "        # Insert predictions in batches\n",
    "        batch_size = 1000\n",
    "        for i in range(0, len(final_df), batch_size):\n",
    "            batch = final_df[i:i+batch_size]\n",
    "            values = [\n",
    "                (str(row['cif_id']), int(row['prediction']), str(row['interpretation']))\n",
    "                for _, row in batch.iterrows()\n",
    "            ]\n",
    "            \n",
    "            cursor.executemany(\"\"\"\n",
    "            INSERT INTO iceberg.silver_crmuser.cluster_predictions \n",
    "            (cif_id, cluster_id, interpretation, processed_date) \n",
    "            VALUES (?, ?, ?, CURRENT_TIMESTAMP)\n",
    "            \"\"\", values)\n",
    "            conn.commit()\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        output = namedtuple('Outputs', ['segmentation_path', 'model_path', 'cluster_plot_path'])\n",
    "        return output(minio_seg_path, minio_model_path, minio_plot_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in feature_engineering_and_segmentation: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "@component(\n",
    "    base_image='bitnami/spark:3.5',\n",
    "    packages_to_install=['trino', 'pandas', 'boto3', 'urllib3']\n",
    ")\n",
    "def save_predictions_to_trino(\n",
    "    segmentation_path: str,\n",
    "    minio_bucket: str\n",
    ") -> str:\n",
    "    import trino\n",
    "    from trino.auth import BasicAuthentication\n",
    "    import pandas as pd\n",
    "    import boto3\n",
    "    import urllib3\n",
    "    \n",
    "    # Disable SSL warnings\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "    try:\n",
    "        # Download predictions from MinIO\n",
    "        minio_client = boto3.client(\n",
    "            's3',\n",
    "            endpoint_url=\"http://192.168.80.155:32000\",\n",
    "            aws_access_key_id=\"admin\",\n",
    "            aws_secret_access_key=\"dlyticaD123\",\n",
    "            verify=False\n",
    "        )\n",
    "        \n",
    "        local_path = \"/tmp/predictions.csv\"\n",
    "        minio_client.download_file(minio_bucket, segmentation_path, local_path)\n",
    "        predictions = pd.read_csv(local_path)\n",
    "\n",
    "        # Connect to Trino\n",
    "        conn = trino.dbapi.connect(\n",
    "            host=\"192.168.80.155\",\n",
    "            port=30071,\n",
    "            user=\"ctzn.bank\",\n",
    "            auth=BasicAuthentication(\"ctzn.bank\", \"ctzn.bank_123\"),\n",
    "            catalog=\"iceberg\",\n",
    "            schema=\"silver_crmuser\",\n",
    "            http_scheme=\"https\",\n",
    "            verify=False\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Create table if not exists (redundant check for robustness)\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS iceberg.silver_crmuser.cluster_predictions (\n",
    "            cif_id VARCHAR,\n",
    "            cluster_id INTEGER,\n",
    "            interpretation VARCHAR,\n",
    "            processed_date TIMESTAMP\n",
    "        ) WITH (\n",
    "            partitioning = ARRAY['processed_date']\n",
    "        )\n",
    "        \"\"\")\n",
    "\n",
    "        # Insert predictions in batches\n",
    "        batch_size = 1000\n",
    "        for i in range(0, len(predictions), batch_size):\n",
    "            batch = predictions[i:i+batch_size]\n",
    "            values = [\n",
    "                (str(row['cif_id']), int(row['prediction']), str(row['interpretation']))\n",
    "                for _, row in batch.iterrows()\n",
    "            ]\n",
    "            \n",
    "            cursor.executemany(\"\"\"\n",
    "            INSERT INTO iceberg.silver_crmuser.cluster_predictions \n",
    "            (cif_id, cluster_id, interpretation, processed_date) \n",
    "            VALUES (?, ?, ?, CURRENT_TIMESTAMP)\n",
    "            \"\"\", values)\n",
    "            conn.commit()\n",
    "\n",
    "        return f\"Successfully saved {len(predictions)} predictions to Trino\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in save_predictions_to_trino: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "# Pipeline definition\n",
    "@pipeline(name=\"Customer_Segmentation_Pipeline\")\n",
    "def customer_segmentation_pipeline(\n",
    "    minio_bucket: str = \"ai360ctzn-customer-segmentation\"\n",
    "):\n",
    "    # Step 1: Fetch data\n",
    "    fetch_task = fetch_data_trino(minio_bucket=minio_bucket)\n",
    "    \n",
    "    # Step 2: Process data and create segments\n",
    "    segmentation_task = feature_engineering_and_segmentation(\n",
    "        file_path=fetch_task.output,\n",
    "        minio_bucket=minio_bucket\n",
    "    )\n",
    "    \n",
    "    # Step 3: Save results to Trino (redundant save for robustness)\n",
    "    save_task = save_predictions_to_trino(\n",
    "        segmentation_path=segmentation_task.outputs['segmentation_path'],\n",
    "        minio_bucket=minio_bucket\n",
    "    )\n",
    "\n",
    "# Compile the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    from kfp import compiler\n",
    "    compiler.Compiler().compile(\n",
    "        customer_segmentation_pipeline,\n",
    "        \"customer_segmentation_pipeline.yaml\"\n",
    "    )\n",
    "    print(\"Pipeline compiled successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6e939c2a-86ce-49f6-bd23-67c55138bf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp.dsl import pipeline, component\n",
    "from datetime import datetime\n",
    "from typing import NamedTuple\n",
    "\n",
    "# Step 1: Fetch data from Trino and save it to MinIO\n",
    "@component(\n",
    "    base_image='bitnami/spark:3.5', \n",
    "    packages_to_install=['trino', 'pandas', 'pyarrow', 's3fs', 'boto3', 'urllib3']\n",
    ")\n",
    "def fetch_data_trino(minio_bucket: str) -> str:\n",
    "    import trino\n",
    "    from trino.auth import BasicAuthentication\n",
    "    import csv\n",
    "    import boto3\n",
    "    import os\n",
    "    import urllib3\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Disable SSL warnings\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "    # Trino Connection Details\n",
    "    TRINO_HOST = \"192.168.80.155\"\n",
    "    TRINO_PORT = \"30071\"\n",
    "    TRINO_USER = \"ctzn.bank\"\n",
    "    TRINO_PASSWORD = \"ctzn.bank_123\"\n",
    "    TRINO_CATALOG = \"iceberg\"\n",
    "    TRINO_SCHEMA = \"silver_crmuser\"\n",
    "    TRINO_HTTP_SCHEME = \"https\"\n",
    "\n",
    "    # Output file\n",
    "    OUTPUT_FILE = \"/tmp/ctzn_10_years_data.csv\"\n",
    "\n",
    "    # SQL Query\n",
    "    SQL_QUERY = \"\"\"\n",
    "    WITH recent_customers AS (\n",
    "        SELECT DISTINCT g.cif_id\n",
    "        FROM gold.dim_gam AS g\n",
    "        WHERE CAST(DATE_PARSE(SUBSTRING(g.acct_opn_date, 1, 19), '%Y-%m-%dT%H:%i:%s') AS DATE) >= CURRENT_DATE - INTERVAL '10' YEAR\n",
    "    ),\n",
    "    account_activity AS (\n",
    "        SELECT \n",
    "            a.cif_id,\n",
    "            SUM(COALESCE(a.total_credit_tran_vol, 0) - COALESCE(a.total_debit_tran_vol, 0)) AS balance,\n",
    "            COUNT(DISTINCT a.nepali_month)/6.0 AS balance_frequency,\n",
    "            SUM(COALESCE(a.total_debit_tran_vol, 0)) AS purchases,\n",
    "            MAX(COALESCE(a.total_debit_tran_vol, 0)) AS oneoff_purchases,\n",
    "            SUM(COALESCE(a.total_debit_tran_vol, 0)) - MAX(COALESCE(a.total_debit_tran_vol, 0)) AS installments_purchases,\n",
    "            SUM(CASE WHEN COALESCE(a.total_credit_tran_vol, 0) > 0 AND COALESCE(a.total_debit_tran_vol, 0) = 0 \n",
    "                    THEN COALESCE(a.total_credit_tran_vol, 0) ELSE 0 END) AS cash_advance,\n",
    "            COUNT(DISTINCT a.foracid)/6.0 AS purchases_frequency,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_debit_tran_vol, 0) > 100000 THEN a.foracid END)/6.0 AS oneoff_purchases_frequency,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_debit_tran_vol, 0) BETWEEN 1 AND 100000 THEN a.foracid END)/6.0 AS purchases_installments_frequency,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_credit_tran_vol, 0) > 0 THEN a.foracid END)/6.0 AS cash_advance_frequency,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_credit_tran_vol, 0) > 0 THEN a.foracid END) AS cash_advance_trx,\n",
    "            COUNT(DISTINCT a.foracid) AS purchases_trx,\n",
    "            SUM(COALESCE(a.total_credit_tran_vol, 0)) AS payments,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_credit_tran_vol, 0) >= COALESCE(a.total_debit_tran_vol, 0) \n",
    "                                THEN a.nepali_month END)/6.0 AS prc_full_payment\n",
    "        FROM gold.mv_fact_deposit_account_insights a\n",
    "        JOIN recent_customers rc ON a.cif_id = rc.cif_id\n",
    "        GROUP BY a.cif_id\n",
    "    ),\n",
    "    salary_stats AS (\n",
    "        SELECT \n",
    "            APPROX_PERCENTILE(COALESCE(salary_per_month, 0), 0.5) AS median_salary,\n",
    "            APPROX_PERCENTILE(COALESCE(salary_per_month, 0), 0.05) AS fifth_percentile_salary\n",
    "        FROM gold.dim_customers\n",
    "    ),\n",
    "    customer_profile AS (\n",
    "        SELECT \n",
    "            g.cif_id,\n",
    "            DATE_DIFF('year', \n",
    "                     CAST(DATE_PARSE(SUBSTRING(MIN(g.acct_opn_date), 1, 19), '%Y-%m-%dT%H:%i:%s') AS DATE), \n",
    "                     CURRENT_DATE) AS tenure,\n",
    "            (SELECT fifth_percentile_salary FROM salary_stats) AS minimum_payments\n",
    "        FROM gold.dim_gam g\n",
    "        LEFT JOIN gold.dim_customers c ON g.cif_id = c.cif_id\n",
    "        GROUP BY g.cif_id\n",
    "    )\n",
    "    SELECT \n",
    "        aa.cif_id AS custid,\n",
    "        aa.balance,\n",
    "        aa.balance_frequency,\n",
    "        aa.purchases,\n",
    "        aa.oneoff_purchases,\n",
    "        aa.installments_purchases,\n",
    "        aa.cash_advance,\n",
    "        aa.purchases_frequency,\n",
    "        aa.oneoff_purchases_frequency,\n",
    "        aa.purchases_installments_frequency,\n",
    "        aa.cash_advance_frequency,\n",
    "        aa.cash_advance_trx,\n",
    "        aa.purchases_trx,\n",
    "        (SELECT median_salary * 3 FROM salary_stats) AS credit_limit,\n",
    "        aa.payments,\n",
    "        cp.minimum_payments,\n",
    "        aa.prc_full_payment,\n",
    "        cp.tenure\n",
    "    FROM account_activity aa\n",
    "    JOIN customer_profile cp ON aa.cif_id = cp.cif_id\n",
    "    ORDER BY aa.cif_id\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Connect to Trino\n",
    "        conn = trino.dbapi.connect(\n",
    "            host=TRINO_HOST,\n",
    "            port=TRINO_PORT,\n",
    "            user=TRINO_USER,\n",
    "            auth=BasicAuthentication(TRINO_USER, TRINO_PASSWORD),\n",
    "            catalog=TRINO_CATALOG,\n",
    "            schema=TRINO_SCHEMA,\n",
    "            http_scheme=TRINO_HTTP_SCHEME,\n",
    "            request_timeout=600,\n",
    "            verify=False\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Execute query and fetch data\n",
    "        cursor.execute(SQL_QUERY)\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        \n",
    "        # Write to CSV in batches\n",
    "        with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(columns)\n",
    "            \n",
    "            while True:\n",
    "                rows = cursor.fetchmany(1000)\n",
    "                if not rows:\n",
    "                    break\n",
    "                writer.writerows(rows)\n",
    "        \n",
    "        # Upload to MinIO\n",
    "        minio_client = boto3.client(\n",
    "            's3',\n",
    "            endpoint_url=\"http://192.168.80.155:32000\",\n",
    "            aws_access_key_id=\"admin\",\n",
    "            aws_secret_access_key=\"dlyticaD123\",\n",
    "            verify=False\n",
    "        )\n",
    "        \n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        minio_path = f\"data/customer_segmentation_raw_{current_date}.csv\"\n",
    "        minio_client.upload_file(OUTPUT_FILE, minio_bucket, minio_path)\n",
    "        \n",
    "        return minio_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in fetch_data_trino: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "@component(\n",
    "    base_image='quay.io/datanature_dev/jupyternotebook:java_home14',\n",
    "    packages_to_install=[\n",
    "        'pyspark==3.5.0',\n",
    "        'pandas==2.0.3',\n",
    "        'numpy==1.24.4',  # Specific version compatible with pandas 2.0.3\n",
    "        'boto3==1.28.57',\n",
    "        'scikit-learn==1.3.0',\n",
    "        'matplotlib==3.7.2',\n",
    "        'pyarrow==12.0.1',\n",
    "        'urllib3==2.0.4'\n",
    "    ]\n",
    ")\n",
    "def feature_engineering_and_segmentation(\n",
    "    file_path: str, \n",
    "    minio_bucket: str\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('segmentation_path', str),\n",
    "    ('model_path', str),\n",
    "    ('cluster_plot_path', str),\n",
    "    ('cluster_interpretations', str)\n",
    "]):\n",
    "    # First ensure numpy is imported with the correct version\n",
    "    import numpy as np\n",
    "    np.__version__  # This helps catch version issues early\n",
    "    \n",
    "    # Then import other packages\n",
    "    import boto3\n",
    "    from botocore.config import Config\n",
    "    import pandas as pd\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')  # Set non-interactive backend\n",
    "    import matplotlib.pyplot as plt\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "    import json\n",
    "    import tempfile\n",
    "    import time\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    # Now import pyspark components\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col, when, mean, monotonically_increasing_id, stddev, count, abs\n",
    "    from pyspark.sql.types import DoubleType, StringType\n",
    "    from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\n",
    "    from pyspark.ml.clustering import BisectingKMeans\n",
    "    from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "    from pyspark.sql.functions import udf\n",
    "\n",
    "    def calculate_feature_distinctiveness(cluster_profiles):\n",
    "        \"\"\"Enhanced feature distinctiveness calculation\"\"\"\n",
    "        feature_variance = cluster_profiles.std()\n",
    "        distinctiveness = feature_variance / (feature_variance.max() + 1e-10)\n",
    "        return distinctiveness\n",
    "\n",
    "    def interpret_clusters(cluster_profiles):\n",
    "        interpretations = {}\n",
    "        distinctiveness = calculate_feature_distinctiveness(cluster_profiles)\n",
    "        \n",
    "        for cluster in cluster_profiles.index:\n",
    "            features = cluster_profiles.columns.tolist()\n",
    "            profile_components = []\n",
    "            \n",
    "            if 'purchases' in features:\n",
    "                spend_level = cluster_profiles.loc[cluster, 'purchases']\n",
    "                mean_spend = cluster_profiles['purchases'].mean()\n",
    "                std_spend = cluster_profiles['purchases'].std()\n",
    "                \n",
    "                if spend_level > mean_spend + 1.5 * std_spend:\n",
    "                    profile_components.append(\"High Spender\")\n",
    "                elif spend_level < mean_spend - 1.5 * std_spend:\n",
    "                    profile_components.append(\"Low Spender\")\n",
    "                else:\n",
    "                    profile_components.append(\"Moderate Spender\")\n",
    "            \n",
    "            if 'cash_advance' in features and 'cash_advance_frequency' in features:\n",
    "                cash_advance = cluster_profiles.loc[cluster, 'cash_advance']\n",
    "                cash_freq = cluster_profiles.loc[cluster, 'cash_advance_frequency']\n",
    "                mean_cash = cluster_profiles['cash_advance'].mean()\n",
    "                mean_freq = cluster_profiles['cash_advance_frequency'].mean()\n",
    "                \n",
    "                if cash_advance > mean_cash * 2 and cash_freq > mean_freq * 2:\n",
    "                    profile_components.append(\"Intensive Cash Advance User\")\n",
    "                elif cash_advance < mean_cash * 0.5 and cash_freq < mean_freq * 0.5:\n",
    "                    profile_components.append(\"Rare Cash Advance User\")\n",
    "            \n",
    "            if 'balance' in features and 'credit_limit' in features:\n",
    "                balance_ratio = cluster_profiles.loc[cluster, 'balance'] / (cluster_profiles.loc[cluster, 'credit_limit'] + 1e-10)\n",
    "                \n",
    "                if balance_ratio > 0.8:\n",
    "                    profile_components.append(\"Very High Credit Utilization\")\n",
    "                elif balance_ratio > 0.5:\n",
    "                    profile_components.append(\"High Credit Utilization\")\n",
    "                elif balance_ratio < 0.2:\n",
    "                    profile_components.append(\"Low Credit Utilization\")\n",
    "                else:\n",
    "                    profile_components.append(\"Moderate Credit Utilization\")\n",
    "            \n",
    "            if 'payments' in features and 'minimum_payments' in features:\n",
    "                payment_ratio = cluster_profiles.loc[cluster, 'payments'] / (cluster_profiles.loc[cluster, 'minimum_payments'] + 1e-10)\n",
    "                \n",
    "                if payment_ratio > 3:\n",
    "                    profile_components.append(\"Aggressive Overpayer\")\n",
    "                elif payment_ratio > 2:\n",
    "                    profile_components.append(\"Consistent Overpayer\")\n",
    "                elif payment_ratio < 1.2:\n",
    "                    profile_components.append(\"Minimum Payment User\")\n",
    "            \n",
    "            if 'balance' in features and 'credit_limit' in features and 'cash_advance' in features:\n",
    "                risk_score = (\n",
    "                    cluster_profiles.loc[cluster, 'balance'] / (cluster_profiles.loc[cluster, 'credit_limit'] + 1e-10) * 0.4 +\n",
    "                    cluster_profiles.loc[cluster, 'cash_advance'] / (cluster_profiles.loc[cluster, 'credit_limit'] + 1e-10) * 0.6\n",
    "                )\n",
    "                \n",
    "                if risk_score > 0.7:\n",
    "                    profile_components.append(\"Extremely High Financial Risk\")\n",
    "                elif risk_score > 0.5:\n",
    "                    profile_components.append(\"High Financial Risk\")\n",
    "                elif risk_score < 0.2:\n",
    "                    profile_components.append(\"Low Financial Risk\")\n",
    "                else:\n",
    "                    profile_components.append(\"Moderate Financial Risk\")\n",
    "            \n",
    "            if profile_components:\n",
    "                interpretations[cluster] = \"; \".join(profile_components)\n",
    "            else:\n",
    "                interpretations[cluster] = \"Undefined Customer Segment\"\n",
    "        \n",
    "        return interpretations\n",
    "\n",
    "    def upload_to_minio(local_path, bucket, object_key, max_retries=5):\n",
    "        \"\"\"Uploads a file to MinIO with improved error handling and content-length issues fixed\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Create a fresh client for each upload attempt\n",
    "                client = boto3.client(\n",
    "                    's3',\n",
    "                    endpoint_url=\"http://192.168.80.155:32000\",\n",
    "                    aws_access_key_id=\"admin\",\n",
    "                    aws_secret_access_key=\"dlyticaD123\",\n",
    "                    verify=False,\n",
    "                    config=Config(\n",
    "                        connect_timeout=30,\n",
    "                        read_timeout=60,\n",
    "                        retries={'max_attempts': 3}\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                # Get the file size\n",
    "                file_size = os.path.getsize(local_path)\n",
    "                \n",
    "                # For very small files or if multipart upload is causing issues, use a simpler method\n",
    "                if file_size < 8 * 1024 * 1024:  # Less than 8MB\n",
    "                    with open(local_path, 'rb') as file_data:\n",
    "                        client.put_object(\n",
    "                            Bucket=bucket,\n",
    "                            Key=object_key,\n",
    "                            Body=file_data,\n",
    "                            ContentLength=file_size  # Explicitly set content length\n",
    "                        )\n",
    "                else:\n",
    "                    # For larger files, we'll disable multipart upload and use a single put_object\n",
    "                    # This may be slower for very large files but avoids the IncompleteBody error\n",
    "                    with open(local_path, 'rb') as file_data:\n",
    "                        data = file_data.read()\n",
    "                        client.put_object(\n",
    "                            Bucket=bucket,\n",
    "                            Key=object_key,\n",
    "                            Body=data,\n",
    "                            ContentLength=len(data)  # Set exact content length\n",
    "                        )\n",
    "                    \n",
    "                print(f\"Successfully uploaded {local_path} to {bucket}/{object_key}\")\n",
    "                return f\"{bucket}/{object_key}\"\n",
    "                \n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = 2 ** attempt  # Exponential backoff\n",
    "                    print(f\"Upload attempt {attempt+1} failed: {str(e)}. Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Upload failed after {max_retries} attempts: {str(e)}\")\n",
    "                    raise\n",
    "                    \n",
    "    # Initialize Spark with optimized settings\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"AdvancedCustomerSegmentation\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "        .config(\"spark.executor.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Initialize MinIO client with improved configuration\n",
    "    boto_config = Config(\n",
    "        connect_timeout=30,\n",
    "        read_timeout=60,\n",
    "        retries={'max_attempts': 3}\n",
    "    )\n",
    "    \n",
    "    minio_client = boto3.client(\n",
    "        's3',\n",
    "        endpoint_url=\"http://192.168.80.155:32000\",\n",
    "        aws_access_key_id=\"admin\",\n",
    "        aws_secret_access_key=\"dlyticaD123\",\n",
    "        verify=False,\n",
    "        config=boto_config\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        today = datetime.now().strftime('%Y-%m-%d')\n",
    "        local_path = \"/tmp/raw_data.csv\"\n",
    "        \n",
    "        # Download with retry logic\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                minio_client.download_file(minio_bucket, file_path, local_path)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise\n",
    "                time.sleep(5)\n",
    "\n",
    "        # Read data with explicit schema\n",
    "        df = spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .csv(local_path)\n",
    "\n",
    "        if 'custid' in df.columns:\n",
    "            df = df.withColumnRenamed(\"custid\", \"cif_id\")\n",
    "\n",
    "        # Feature engineering\n",
    "        df = df.withColumn(\"balance_utilization_ratio\", \n",
    "            when(col(\"credit_limit\") != 0, col(\"balance\") / col(\"credit_limit\")).otherwise(0)\n",
    "        )\n",
    "        df = df.withColumn(\"cash_advance_intensity\", \n",
    "            when(col(\"credit_limit\") != 0, col(\"cash_advance\") / col(\"credit_limit\")).otherwise(0)\n",
    "        )\n",
    "        df = df.withColumn(\"payment_effort_ratio\", \n",
    "            when(col(\"minimum_payments\") != 0, col(\"payments\") / col(\"minimum_payments\")).otherwise(0)\n",
    "        )\n",
    "        df = df.withColumn(\"purchase_diversity\", \n",
    "            abs(col(\"oneoff_purchases\") - col(\"installments_purchases\")) / \n",
    "            (abs(col(\"oneoff_purchases\") + col(\"installments_purchases\")) + 1e-10)\n",
    "        )\n",
    "        df = df.na.fill(0)\n",
    "\n",
    "        numeric_cols = [\n",
    "            'purchases', 'oneoff_purchases', 'installments_purchases', \n",
    "            'cash_advance', 'cash_advance_frequency', \n",
    "            'balance', 'credit_limit', \n",
    "            'payments', 'minimum_payments',\n",
    "            'balance_utilization_ratio', \n",
    "            'cash_advance_intensity', \n",
    "            'payment_effort_ratio',\n",
    "            'purchase_diversity'\n",
    "        ]\n",
    "\n",
    "        # Feature transformation\n",
    "        assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "        assembled = assembler.transform(df.drop(\"cif_id\"))\n",
    "        scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "        scaled_data = scaler.fit(assembled).transform(assembled)\n",
    "\n",
    "        # Dimensionality reduction\n",
    "        pca = PCA(k=5, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
    "        pca_data = pca.fit(scaled_data).transform(scaled_data)\n",
    "\n",
    "        # Clustering optimization\n",
    "        evaluator = ClusteringEvaluator(featuresCol='pca_features')\n",
    "        k_range = range(6, 15)\n",
    "        k_metrics = []\n",
    "\n",
    "        for k in k_range:\n",
    "            kmeans = BisectingKMeans(k=k, seed=42, featuresCol=\"pca_features\")\n",
    "            model = kmeans.fit(pca_data)\n",
    "            predictions = model.transform(pca_data)\n",
    "            \n",
    "            cost = model.summary.trainingCost if hasattr(model.summary, 'trainingCost') else 0\n",
    "            silhouette = evaluator.evaluate(predictions)\n",
    "            \n",
    "            centers = model.clusterCenters()\n",
    "            separation = np.mean([np.min(np.linalg.norm(centers[i] - centers[j])) \n",
    "                                  for i in range(len(centers)) \n",
    "                                  for j in range(i+1, len(centers))])\n",
    "            \n",
    "            k_metrics.append({\n",
    "                'k': k,\n",
    "                'cost': cost,\n",
    "                'silhouette': silhouette,\n",
    "                'separation': separation\n",
    "            })\n",
    "\n",
    "        # Determine optimal k\n",
    "        k_metrics_df = pd.DataFrame(k_metrics)\n",
    "        k_metrics_df['normalized_cost'] = (k_metrics_df['cost'] - k_metrics_df['cost'].min()) / (k_metrics_df['cost'].max() - k_metrics_df['cost'].min())\n",
    "        k_metrics_df['normalized_silhouette'] = (k_metrics_df['silhouette'] - k_metrics_df['silhouette'].min()) / (k_metrics_df['silhouette'].max() - k_metrics_df['silhouette'].min())\n",
    "        k_metrics_df['normalized_separation'] = (k_metrics_df['separation'] - k_metrics_df['separation'].min()) / (k_metrics_df['separation'].max() - k_metrics_df['separation'].min())\n",
    "        \n",
    "        k_metrics_df['composite_score'] = (\n",
    "            0.3 * (1 - k_metrics_df['normalized_cost']) +\n",
    "            0.4 * k_metrics_df['normalized_silhouette'] +\n",
    "            0.3 * k_metrics_df['normalized_separation']\n",
    "        )\n",
    "        \n",
    "        optimal_k = k_metrics_df.loc[k_metrics_df['composite_score'].idxmax(), 'k']\n",
    "\n",
    "        # Plotting\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15))\n",
    "        ax1.plot(k_metrics_df['k'], k_metrics_df['cost'], marker='o', color='blue')\n",
    "        ax1.set_xlabel('Number of Clusters (k)')\n",
    "        ax1.set_ylabel('Cost', color='blue')\n",
    "        ax1.grid(True)\n",
    "\n",
    "        ax2.plot(k_metrics_df['k'], k_metrics_df['silhouette'], marker='x', color='green')\n",
    "        ax2.set_xlabel('Number of Clusters (k)')\n",
    "        ax2.set_ylabel('Silhouette Score', color='green')\n",
    "        ax2.grid(True)\n",
    "\n",
    "        ax3.plot(k_metrics_df['k'], k_metrics_df['separation'], marker='^', color='red')\n",
    "        ax3.set_xlabel('Number of Clusters (k)')\n",
    "        ax3.set_ylabel('Cluster Separation', color='red')\n",
    "        ax3.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        cluster_plot_path = \"/tmp/cluster_diagnostics.png\"\n",
    "        plt.savefig(cluster_plot_path)\n",
    "        plt.close()\n",
    "        \n",
    "        # Final clustering\n",
    "        kmeans = BisectingKMeans(k=optimal_k, seed=42, featuresCol=\"pca_features\")\n",
    "        model = kmeans.fit(pca_data)\n",
    "        clustered_data = model.transform(pca_data)\n",
    "\n",
    "        # Add cluster information\n",
    "        df_with_id = df.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "        clustered_data = clustered_data.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "        clustered_data = clustered_data.join(df_with_id.select(\"row_id\", \"cif_id\"), on=\"row_id\").drop(\"row_id\")\n",
    "\n",
    "        # Generate cluster profiles\n",
    "        summary = clustered_data.groupBy(\"prediction\").agg(\n",
    "            *[mean(col(c)).alias(c) for c in numeric_cols],\n",
    "            *[stddev(col(c)).alias(f\"{c}_std\") for c in numeric_cols],\n",
    "            count(\"cif_id\").alias(\"cluster_size\")\n",
    "        )\n",
    "        \n",
    "        # Convert to Pandas safely\n",
    "        cluster_data = summary.collect()\n",
    "        cluster_profiles = pd.DataFrame([row.asDict() for row in cluster_data]).set_index(\"prediction\")\n",
    "        \n",
    "        # Get interpretations\n",
    "        interpretations = interpret_clusters(cluster_profiles)\n",
    "        \n",
    "        # Save outputs\n",
    "        outputs = []\n",
    "        output_paths = {\n",
    "            'segmentation': \"/tmp/segmentation_results.csv\",\n",
    "            'interpretations': \"/tmp/cluster_interpretations.json\",\n",
    "            'model': \"/tmp/segmentation_model\",\n",
    "            'plot': cluster_plot_path\n",
    "        }\n",
    "        \n",
    "        # Save segmentation results - ensure file is closed properly\n",
    "        results = clustered_data.select(\"cif_id\", \"prediction\").collect()\n",
    "        results_df = pd.DataFrame([row.asDict() for row in results])\n",
    "        results_df.to_csv(output_paths['segmentation'], index=False)\n",
    "        \n",
    "        # Save interpretations - ensure file is closed properly\n",
    "        with open(output_paths['interpretations'], 'w') as f:\n",
    "            json.dump(interpretations, f)\n",
    "            f.flush()\n",
    "            os.fsync(f.fileno())\n",
    "        \n",
    "        # Save model\n",
    "        model.write().overwrite().save(output_paths['model'])\n",
    "        \n",
    "        # Upload to MinIO with improved upload function\n",
    "        minio_paths = {}\n",
    "        \n",
    "        # Upload single files first\n",
    "        for name, local_path in output_paths.items():\n",
    "            if name != 'model':  # Handle model directory separately\n",
    "                minio_path = f\"results/{name}_{today}{os.path.splitext(local_path)[1]}\"\n",
    "                try:\n",
    "                    upload_to_minio(local_path, minio_bucket, minio_path)\n",
    "                    minio_paths[name] = minio_path\n",
    "                except Exception as e:\n",
    "                    print(f\"Error uploading {name}: {str(e)}\")\n",
    "                    raise\n",
    "        \n",
    "        # Handle model directory upload separately\n",
    "        model_path = output_paths['model']\n",
    "        minio_model_path = f\"models/segmentation_model_{today}\"\n",
    "        model_files_uploaded = []\n",
    "        \n",
    "        try:\n",
    "            for root, _, files in os.walk(model_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    relative_path = os.path.relpath(file_path, model_path)\n",
    "                    s3_key = f\"{minio_model_path}/{relative_path}\"\n",
    "                    upload_to_minio(file_path, minio_bucket, s3_key)\n",
    "                    model_files_uploaded.append(s3_key)\n",
    "            \n",
    "            minio_paths['model'] = minio_model_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading model files: {str(e)}\")\n",
    "            # Continue with other files even if model upload fails\n",
    "            minio_paths['model'] = f\"models/segmentation_model_{today}_partial\"\n",
    "\n",
    "        output = namedtuple('Outputs', [\n",
    "            'segmentation_path',\n",
    "            'model_path',\n",
    "            'cluster_plot_path',\n",
    "            'cluster_interpretations'\n",
    "        ])\n",
    "        \n",
    "        return output(\n",
    "            minio_paths.get('segmentation', ''),\n",
    "            minio_paths.get('model', ''),\n",
    "            minio_paths.get('plot', ''),\n",
    "            minio_paths.get('interpretations', '')\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in feature_engineering_and_segmentation: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        # Make sure Spark is stopped properly\n",
    "        if spark:\n",
    "            spark.stop()\n",
    "\n",
    "\n",
    "# Step 3: Save predictions to Trino\n",
    "@component(\n",
    "    base_image='quay.io/datanature_dev/jupyternotebook:java_home14',\n",
    "    packages_to_install=['trino', 'pandas', 'boto3', 'urllib3']\n",
    ")\n",
    "def save_predictions_to_trino(\n",
    "    segmentation_path: str,\n",
    "    minio_bucket: str\n",
    ") -> str:\n",
    "    import trino\n",
    "    from trino.auth import BasicAuthentication\n",
    "    import pandas as pd\n",
    "    import boto3\n",
    "    import urllib3\n",
    "    import pandas as pd\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "    from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "    import os\n",
    "    import shutil\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql.functions import lit, col, when\n",
    "    \n",
    "    # variables\n",
    "    # variables\n",
    "    # access_key_id = 'admin'\n",
    "    # secret_access_key = 'dlyticaD123'\n",
    "    # minio_endpoint = 'dn-minio-tenant-hl.dn-minio-tenant.svc.cluster.local:9000'  # Replace with your MinIO server endpoint\n",
    "    # data_bucket = 'ai360fd-recommendation'\n",
    "    # apps_bucket = ''\n",
    "    # hive_metastore_uri = \"thrift://dn-hive-metastore.dn-hive-metastore.svc.cluster.local:9083\"\n",
    "    # iceberg_warehouse_location = f\"s3a://{data_bucket}/data/\"\n",
    "    # custom_catalog = \"iceberg_catalog\"\n",
    "    # spark_eventlog_dir = f\"s3a://{apps_bucket}/logs/spark/\"\n",
    "    # master_url=\"spark://dn-spark-master-svc.dn-spark.svc.cluster.local:7077\"\n",
    "    # spark_driver_host=\"notebook-five-spark-driver-headless.kubeflow-user-example-com.svc.cluster.local\"\n",
    "    # app_name=\"All Data Generation\"\n",
    "    # additional_packages=\"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.spark:spark-avro_2.12:3.5.0,com.crealytics:spark-excel_2.12:0.13.7,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.hadoop:hadoop-common:3.3.4\"\n",
    "\n",
    "    \n",
    "    access_key_id = 'admin'\n",
    "    secret_access_key = 'dlyticaD123'\n",
    "    minio_endpoint = 'dn-minio-tenant-hl.dn-minio-tenant.svc.cluster.local:9000'  # Replace with your MinIO server endpoint\n",
    "    data_bucket = 'ai360fd-recommendation'\n",
    "    # apps_bucket = 'dn-apps'\n",
    "    hive_metastore_uri = \"thrift://dn-hive-metastore.dn-hive-metastore.svc.cluster.local:9083\"\n",
    "    iceberg_warehouse_location = f\"s3a://{data_bucket}/data/\"\n",
    "    custom_catalog = \"iceberg_catalog\"\n",
    "    # spark_eventlog_dir = f\"s3a://{apps_bucket}/logs/spark/\"\n",
    "    # master_url=\"spark://dn-spark-master-svc.dn-spark.svc.cluster.local:7077\"\n",
    "    # spark_driver_host=\"notebook-five-spark-driver-headless.kubeflow-user-example-com.svc.cluster.local\"\n",
    "    app_name=\"All Data Generation\"\n",
    "    additional_packages=\"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.spark:spark-avro_2.12:3.5.0,com.crealytics:spark-excel_2.12:0.13.7,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.hadoop:hadoop-common:3.3.4\"\n",
    "\n",
    "    #spark configuration\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", minio_endpoint) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", access_key_id) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", secret_access_key) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .config(\"spark.hadoop.hive.metastore.uris\", hive_metastore_uri) \\\n",
    "        .config(\"spark.sql.warehouse.dir\", iceberg_warehouse_location) \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "        .config(f\"spark.sql.catalog.{custom_catalog}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "        .config(\"spark.jars.packages\", additional_packages) \\\n",
    "        .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "        .config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # spark.sql(f\"\"\"\n",
    "    # show tables in {custom_catalog}.gold\n",
    "    # \"\"\").show(100,False)\n",
    "    # print(\"train_model is called\")\n",
    "    # Initialize Spark session\n",
    "    # spark = SparkSession.builder.appName(\"Feature Engineering\").getOrCreate()\n",
    "    \n",
    "        \n",
    "    # Disable SSL warnings\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "    try:\n",
    "        # Download predictions from MinIO\n",
    "        minio_client = boto3.client(\n",
    "            's3',\n",
    "            endpoint_url=\"http://192.168.80.155:32000\",\n",
    "            aws_access_key_id=\"admin\",\n",
    "            aws_secret_access_key=\"dlyticaD123\",\n",
    "            verify=False\n",
    "        )\n",
    "        \n",
    "        local_path = \"/tmp/predictions.csv\"\n",
    "        minio_client.download_file(minio_bucket, segmentation_path, local_path)\n",
    "        predictions = pd.read_csv(local_path)\n",
    "        \n",
    "        # # Connect to Trino\n",
    "        # conn = trino.dbapi.connect(\n",
    "        #     host=\"192.168.80.155\",\n",
    "        #     port=30071,\n",
    "        #     user=\"ctzn.bank\",\n",
    "        #     auth=BasicAuthentication(\"ctzn.bank\", \"ctzn.bank_123\"),\n",
    "        #     catalog=\"iceberg\",\n",
    "        #     schema=\"silver_crmuser\",\n",
    "        #     http_scheme=\"https\",\n",
    "        #     verify=False\n",
    "        # )\n",
    "        # cursor = conn.cursor()\n",
    "\n",
    "        # # Create table if not exists\n",
    "        # cursor.execute(\"\"\"\n",
    "        # CREATE TABLE IF NOT EXISTS iceberg.silver_crmuser.customer_segments (\n",
    "        #     cif_id VARCHAR,\n",
    "        #     cluster_id INTEGER,\n",
    "        #     processed_date TIMESTAMP\n",
    "        # ) WITH (\n",
    "        #     partitioning = ARRAY['processed_date']\n",
    "        # )\n",
    "        # \"\"\")\n",
    "\n",
    "        # Insert predictions in batches\n",
    "        batch_size = 1000\n",
    "        for i in range(0, len(predictions), batch_size):\n",
    "            batch = predictions[i:i+batch_size]\n",
    "            values = [(str(row['custid']), int(row['prediction'])) for _, row in batch.iterrows()]\n",
    "            \n",
    "            insert_sql = \"\"\"\n",
    "            INSERT INTO iceberg.silver_crmuser.customer_segments \n",
    "            (cif_id, cluster_id, processed_date) \n",
    "            VALUES (?, ?, CURRENT_TIMESTAMP)\n",
    "            \"\"\"\n",
    "            \n",
    "            cursor.executemany(insert_sql, values)\n",
    "            conn.commit()\n",
    "\n",
    "        return f\"Successfully saved {len(predictions)} predictions to Trino\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in save_predictions_to_trino: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "# Pipeline definition\n",
    "@pipeline(name=\"Customer Segmentation Pipeline\")\n",
    "def customer_segmentation_pipeline(\n",
    "    minio_bucket: str = \"ai360ctzn-customer-segmentation\"\n",
    "):\n",
    "    # Step 1: Fetch data\n",
    "    fetch_task = fetch_data_trino(minio_bucket=minio_bucket)\n",
    "    \n",
    "    # Step 2: Process data and create segments\n",
    "    segmentation_task = feature_engineering_and_segmentation(\n",
    "        file_path=fetch_task.output,\n",
    "        minio_bucket=minio_bucket\n",
    "    )\n",
    "    \n",
    "    # Step 3: Save results to Trino\n",
    "    save_task = save_predictions_to_trino(\n",
    "        segmentation_path=segmentation_task.outputs['segmentation_path'],\n",
    "        minio_bucket=minio_bucket\n",
    "    )\n",
    "\n",
    "# Compile the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    from kfp import compiler\n",
    "    compiler.Compiler().compile(\n",
    "        customer_segmentation_pipeline,\n",
    "        \"customer_segmentation_pipeline.yaml\"\n",
    "    )\n",
    "    print(\"Pipeline compiled successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7cf640-d718-486c-96c2-74a789a34f4c",
   "metadata": {},
   "source": [
    "### FInal pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "40ca9970-d627-4aa5-b9a8-80b8eb906b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp.dsl import pipeline, component\n",
    "from datetime import datetime\n",
    "from typing import NamedTuple\n",
    "\n",
    "# Step 1: Fetch data from Trino and save it to MinIO\n",
    "@component(\n",
    "    base_image='bitnami/spark:3.5', \n",
    "    packages_to_install=['trino', 'pandas', 'pyarrow', 's3fs', 'boto3', 'urllib3']\n",
    ")\n",
    "def fetch_data_trino(minio_bucket: str) -> str:\n",
    "    import trino\n",
    "    from trino.auth import BasicAuthentication\n",
    "    import csv\n",
    "    import boto3\n",
    "    import os\n",
    "    import urllib3\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Disable SSL warnings\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "    # Trino Connection Details\n",
    "    TRINO_HOST = \"192.168.80.155\"\n",
    "    TRINO_PORT = \"30071\"\n",
    "    TRINO_USER = \"ctzn.bank\"\n",
    "    TRINO_PASSWORD = \"ctzn.bank_123\"\n",
    "    TRINO_CATALOG = \"iceberg\"\n",
    "    TRINO_SCHEMA = \"silver_crmuser\"\n",
    "    TRINO_HTTP_SCHEME = \"https\"\n",
    "\n",
    "    # Output file\n",
    "    OUTPUT_FILE = \"/tmp/ctzn_10_years_data.csv\"\n",
    "\n",
    "    # SQL Query\n",
    "    SQL_QUERY = \"\"\"\n",
    "    WITH recent_customers AS (\n",
    "        SELECT DISTINCT g.cif_id\n",
    "        FROM gold.dim_gam AS g\n",
    "        WHERE CAST(DATE_PARSE(SUBSTRING(g.acct_opn_date, 1, 19), '%Y-%m-%dT%H:%i:%s') AS DATE) >= CURRENT_DATE - INTERVAL '10' YEAR\n",
    "    ),\n",
    "    account_activity AS (\n",
    "        SELECT \n",
    "            a.cif_id,\n",
    "            SUM(COALESCE(a.total_credit_tran_vol, 0) - COALESCE(a.total_debit_tran_vol, 0)) AS balance,\n",
    "            COUNT(DISTINCT a.nepali_month)/6.0 AS balance_frequency,\n",
    "            SUM(COALESCE(a.total_debit_tran_vol, 0)) AS purchases,\n",
    "            MAX(COALESCE(a.total_debit_tran_vol, 0)) AS oneoff_purchases,\n",
    "            SUM(COALESCE(a.total_debit_tran_vol, 0)) - MAX(COALESCE(a.total_debit_tran_vol, 0)) AS installments_purchases,\n",
    "            SUM(CASE WHEN COALESCE(a.total_credit_tran_vol, 0) > 0 AND COALESCE(a.total_debit_tran_vol, 0) = 0 \n",
    "                    THEN COALESCE(a.total_credit_tran_vol, 0) ELSE 0 END) AS cash_advance,\n",
    "            COUNT(DISTINCT a.foracid)/6.0 AS purchases_frequency,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_debit_tran_vol, 0) > 100000 THEN a.foracid END)/6.0 AS oneoff_purchases_frequency,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_debit_tran_vol, 0) BETWEEN 1 AND 100000 THEN a.foracid END)/6.0 AS purchases_installments_frequency,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_credit_tran_vol, 0) > 0 THEN a.foracid END)/6.0 AS cash_advance_frequency,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_credit_tran_vol, 0) > 0 THEN a.foracid END) AS cash_advance_trx,\n",
    "            COUNT(DISTINCT a.foracid) AS purchases_trx,\n",
    "            SUM(COALESCE(a.total_credit_tran_vol, 0)) AS payments,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_credit_tran_vol, 0) >= COALESCE(a.total_debit_tran_vol, 0) \n",
    "                                THEN a.nepali_month END)/6.0 AS prc_full_payment\n",
    "        FROM gold.mv_fact_deposit_account_insights a\n",
    "        JOIN recent_customers rc ON a.cif_id = rc.cif_id\n",
    "        GROUP BY a.cif_id\n",
    "    ),\n",
    "    salary_stats AS (\n",
    "        SELECT \n",
    "            APPROX_PERCENTILE(COALESCE(salary_per_month, 0), 0.5) AS median_salary,\n",
    "            APPROX_PERCENTILE(COALESCE(salary_per_month, 0), 0.05) AS fifth_percentile_salary\n",
    "        FROM gold.dim_customers\n",
    "    ),\n",
    "    customer_profile AS (\n",
    "        SELECT \n",
    "            g.cif_id,\n",
    "            DATE_DIFF('year', \n",
    "                     CAST(DATE_PARSE(SUBSTRING(MIN(g.acct_opn_date), 1, 19), '%Y-%m-%dT%H:%i:%s') AS DATE), \n",
    "                     CURRENT_DATE) AS tenure,\n",
    "            (SELECT fifth_percentile_salary FROM salary_stats) AS minimum_payments\n",
    "        FROM gold.dim_gam g\n",
    "        LEFT JOIN gold.dim_customers c ON g.cif_id = c.cif_id\n",
    "        GROUP BY g.cif_id\n",
    "    )\n",
    "    SELECT \n",
    "        aa.cif_id AS custid,\n",
    "        aa.balance,\n",
    "        aa.balance_frequency,\n",
    "        aa.purchases,\n",
    "        aa.oneoff_purchases,\n",
    "        aa.installments_purchases,\n",
    "        aa.cash_advance,\n",
    "        aa.purchases_frequency,\n",
    "        aa.oneoff_purchases_frequency,\n",
    "        aa.purchases_installments_frequency,\n",
    "        aa.cash_advance_frequency,\n",
    "        aa.cash_advance_trx,\n",
    "        aa.purchases_trx,\n",
    "        (SELECT median_salary * 3 FROM salary_stats) AS credit_limit,\n",
    "        aa.payments,\n",
    "        cp.minimum_payments,\n",
    "        aa.prc_full_payment,\n",
    "        cp.tenure\n",
    "    FROM account_activity aa\n",
    "    JOIN customer_profile cp ON aa.cif_id = cp.cif_id\n",
    "    ORDER BY aa.cif_id\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Connect to Trino\n",
    "        conn = trino.dbapi.connect(\n",
    "            host=TRINO_HOST,\n",
    "            port=TRINO_PORT,\n",
    "            user=TRINO_USER,\n",
    "            auth=BasicAuthentication(TRINO_USER, TRINO_PASSWORD),\n",
    "            catalog=TRINO_CATALOG,\n",
    "            schema=TRINO_SCHEMA,\n",
    "            http_scheme=TRINO_HTTP_SCHEME,\n",
    "            request_timeout=600,\n",
    "            verify=False\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Execute query and fetch data\n",
    "        cursor.execute(SQL_QUERY)\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        \n",
    "        # Write to CSV in batches\n",
    "        with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(columns)\n",
    "            \n",
    "            while True:\n",
    "                rows = cursor.fetchmany(1000)\n",
    "                if not rows:\n",
    "                    break\n",
    "                writer.writerows(rows)\n",
    "        \n",
    "        # Upload to MinIO\n",
    "        minio_client = boto3.client(\n",
    "            's3',\n",
    "            endpoint_url=\"http://192.168.80.155:32000\",\n",
    "            aws_access_key_id=\"admin\",\n",
    "            aws_secret_access_key=\"dlyticaD123\",\n",
    "            verify=False\n",
    "        )\n",
    "        \n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        minio_path = f\"data/customer_segmentation_raw_{current_date}.csv\"\n",
    "        minio_client.upload_file(OUTPUT_FILE, minio_bucket, minio_path)\n",
    "        \n",
    "        return minio_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in fetch_data_trino: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "@component(\n",
    "    base_image='quay.io/datanature_dev/jupyternotebook:java_home14',\n",
    "    packages_to_install=[\n",
    "        'pyspark==3.5.0',\n",
    "        'pandas==2.0.3',\n",
    "        'numpy==1.24.4',\n",
    "        'boto3==1.28.57',\n",
    "        'scikit-learn==1.3.0',\n",
    "        'matplotlib==3.7.2',\n",
    "        'pyarrow==12.0.1',\n",
    "        'urllib3==2.0.4'\n",
    "    ]\n",
    ")\n",
    "def feature_engineering_and_segmentation(\n",
    "    file_path: str, \n",
    "    minio_bucket: str\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('segmentation_path', str),\n",
    "    ('model_path', str),\n",
    "    ('cluster_plot_path', str),\n",
    "    ('cluster_interpretations', str),\n",
    "    ('final_output_path', str)\n",
    "]):\n",
    "    import numpy as np\n",
    "    import boto3\n",
    "    import pandas as pd\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "    import json\n",
    "    import time\n",
    "    from collections import namedtuple\n",
    "\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col, when, mean, monotonically_increasing_id, stddev, count, abs, udf\n",
    "    from pyspark.sql.types import StringType\n",
    "    from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\n",
    "    from pyspark.ml.clustering import BisectingKMeans\n",
    "    from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "    def interpret_clusters(cluster_profiles):\n",
    "        interpretations = {}\n",
    "        for cluster in cluster_profiles.index:\n",
    "            profile_components = []\n",
    "            spend = cluster_profiles.loc[cluster, 'purchases']\n",
    "            spend_mean = cluster_profiles['purchases'].mean()\n",
    "            spend_std = cluster_profiles['purchases'].std()\n",
    "            if spend > spend_mean + 1.5 * spend_std:\n",
    "                profile_components.append(\"High Spender\")\n",
    "            elif spend < spend_mean - 1.5 * spend_std:\n",
    "                profile_components.append(\"Low Spender\")\n",
    "            else:\n",
    "                profile_components.append(\"Moderate Spender\")\n",
    "            interpretations[cluster] = \"; \".join(profile_components)\n",
    "        return interpretations\n",
    "\n",
    "    def upload_to_minio(local_path, bucket, object_key):\n",
    "        client = boto3.client(\n",
    "            's3',\n",
    "            endpoint_url=\"http://192.168.80.155:32000\",\n",
    "            aws_access_key_id=\"admin\",\n",
    "            aws_secret_access_key=\"dlyticaD123\",\n",
    "            verify=False\n",
    "        )\n",
    "        client.upload_file(local_path, bucket, object_key)\n",
    "        return f\"{bucket}/{object_key}\"\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"CustomerSegmentation\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    try:\n",
    "        today = datetime.now().strftime('%Y-%m-%d')\n",
    "        local_path = \"/tmp/raw_data.csv\"\n",
    "        boto_client = boto3.client(\n",
    "            's3',\n",
    "            endpoint_url=\"http://192.168.80.155:32000\",\n",
    "            aws_access_key_id=\"admin\",\n",
    "            aws_secret_access_key=\"dlyticaD123\",\n",
    "            verify=False\n",
    "        )\n",
    "        boto_client.download_file(minio_bucket, file_path, local_path)\n",
    "\n",
    "        df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(local_path)\n",
    "        if 'custid' in df.columns:\n",
    "            df = df.withColumnRenamed(\"custid\", \"cif_id\")\n",
    "\n",
    "        df = df.withColumn(\"balance_utilization_ratio\", when(col(\"credit_limit\") != 0, col(\"balance\") / col(\"credit_limit\")).otherwise(0))\n",
    "        df = df.na.fill(0)\n",
    "\n",
    "        numeric_cols = ['purchases', 'balance', 'credit_limit', 'balance_utilization_ratio']\n",
    "        assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "        scaled = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "        pca = PCA(k=3, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
    "\n",
    "        assembled = assembler.transform(df.drop(\"cif_id\"))\n",
    "        scaled_data = scaled.fit(assembled).transform(assembled)\n",
    "        pca_data = pca.fit(scaled_data).transform(scaled_data)\n",
    "\n",
    "        kmeans = BisectingKMeans(k=4, seed=42, featuresCol=\"pca_features\")\n",
    "        model = kmeans.fit(pca_data)\n",
    "        clustered = model.transform(pca_data)\n",
    "\n",
    "        df_id = df.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "        clustered = clustered.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "        clustered = clustered.join(df_id.select(\"row_id\", \"cif_id\"), on=\"row_id\").drop(\"row_id\")\n",
    "\n",
    "        summary = clustered.groupBy(\"prediction\").agg(*[mean(col(c)).alias(c) for c in numeric_cols])\n",
    "        cluster_profiles = pd.DataFrame([row.asDict() for row in summary.collect()]).set_index(\"prediction\")\n",
    "        interpretations = interpret_clusters(cluster_profiles)\n",
    "\n",
    "        interpret_udf = udf(lambda x: interpretations.get(x, \"Unknown\"), StringType())\n",
    "        clustered = clustered.withColumn(\"interpretation\", interpret_udf(col(\"prediction\")))\n",
    "\n",
    "        final_df = clustered.select(\"cif_id\", \"prediction\", \"interpretation\").toPandas()\n",
    "        final_csv_path = f\"/tmp/final_segmented_customers_{today}.csv\"\n",
    "        final_df.to_csv(final_csv_path, index=False)\n",
    "        final_minio_path = f\"results/final_segmented_customers_{today}.csv\"\n",
    "        upload_to_minio(final_csv_path, minio_bucket, final_minio_path)\n",
    "\n",
    "        return namedtuple('Outputs', [\n",
    "            'segmentation_path',\n",
    "            'model_path',\n",
    "            'cluster_plot_path',\n",
    "            'cluster_interpretations',\n",
    "            'final_output_path']) (\n",
    "            '', '', '', '', final_minio_path\n",
    "        )\n",
    "\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "\n",
    "# # Step 3: Save predictions to Trino\n",
    "# @component(\n",
    "#     base_image='quay.io/datanature_dev/jupyternotebook:java_home14',\n",
    "#     packages_to_install=['trino', 'pandas', 'boto3', 'urllib3']\n",
    "# )\n",
    "# def save_predictions_to_trino(\n",
    "#     segmentation_path: str,\n",
    "#     minio_bucket: str\n",
    "# ) -> str:\n",
    "#     import trino\n",
    "#     from trino.auth import BasicAuthentication\n",
    "#     import pandas as pd\n",
    "#     import boto3\n",
    "#     import urllib3\n",
    "#     import pandas as pd\n",
    "#     from pyspark.sql import SparkSession\n",
    "#     from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "#     from pyspark.ml import Pipeline\n",
    "#     from pyspark.ml.classification import LogisticRegression\n",
    "#     from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "#     import os\n",
    "#     import shutil\n",
    "#     from datetime import datetime\n",
    "#     from pyspark.sql.functions import lit, col, when\n",
    "    \n",
    "#     # variables\n",
    "#     # variables\n",
    "#     # access_key_id = 'admin'\n",
    "#     # secret_access_key = 'dlyticaD123'\n",
    "#     # minio_endpoint = 'dn-minio-tenant-hl.dn-minio-tenant.svc.cluster.local:9000'  # Replace with your MinIO server endpoint\n",
    "#     # data_bucket = 'ai360fd-recommendation'\n",
    "#     # apps_bucket = ''\n",
    "#     # hive_metastore_uri = \"thrift://dn-hive-metastore.dn-hive-metastore.svc.cluster.local:9083\"\n",
    "#     # iceberg_warehouse_location = f\"s3a://{data_bucket}/data/\"\n",
    "#     # custom_catalog = \"iceberg_catalog\"\n",
    "#     # spark_eventlog_dir = f\"s3a://{apps_bucket}/logs/spark/\"\n",
    "#     # master_url=\"spark://dn-spark-master-svc.dn-spark.svc.cluster.local:7077\"\n",
    "#     # spark_driver_host=\"notebook-five-spark-driver-headless.kubeflow-user-example-com.svc.cluster.local\"\n",
    "#     # app_name=\"All Data Generation\"\n",
    "#     # additional_packages=\"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.spark:spark-avro_2.12:3.5.0,com.crealytics:spark-excel_2.12:0.13.7,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.hadoop:hadoop-common:3.3.4\"\n",
    "\n",
    "    \n",
    "#     access_key_id = 'admin'\n",
    "#     secret_access_key = 'dlyticaD123'\n",
    "#     minio_endpoint = 'dn-minio-tenant-hl.dn-minio-tenant.svc.cluster.local:9000'  # Replace with your MinIO server endpoint\n",
    "#     data_bucket = 'ai360fd-recommendation'\n",
    "#     # apps_bucket = 'dn-apps'\n",
    "#     hive_metastore_uri = \"thrift://dn-hive-metastore.dn-hive-metastore.svc.cluster.local:9083\"\n",
    "#     iceberg_warehouse_location = f\"s3a://{data_bucket}/data/\"\n",
    "#     custom_catalog = \"iceberg_catalog\"\n",
    "#     # spark_eventlog_dir = f\"s3a://{apps_bucket}/logs/spark/\"\n",
    "#     # master_url=\"spark://dn-spark-master-svc.dn-spark.svc.cluster.local:7077\"\n",
    "#     # spark_driver_host=\"notebook-five-spark-driver-headless.kubeflow-user-example-com.svc.cluster.local\"\n",
    "#     app_name=\"All Data Generation\"\n",
    "#     additional_packages=\"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.spark:spark-avro_2.12:3.5.0,com.crealytics:spark-excel_2.12:0.13.7,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.hadoop:hadoop-common:3.3.4\"\n",
    "\n",
    "#     #spark configuration\n",
    "#     spark = SparkSession.builder \\\n",
    "#         .appName(app_name) \\\n",
    "#         .config(\"spark.driver.memory\", \"8g\") \\\n",
    "#         .config(\"spark.executor.memory\", \"8g\") \\\n",
    "#         .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "#         .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "#         .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "#         .config(\"spark.hadoop.fs.s3a.endpoint\", minio_endpoint) \\\n",
    "#         .config(\"spark.hadoop.fs.s3a.access.key\", access_key_id) \\\n",
    "#         .config(\"spark.hadoop.fs.s3a.secret.key\", secret_access_key) \\\n",
    "#         .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "#         .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "#         .config(\"spark.hadoop.hive.metastore.uris\", hive_metastore_uri) \\\n",
    "#         .config(\"spark.sql.warehouse.dir\", iceberg_warehouse_location) \\\n",
    "#         .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "#         .config(f\"spark.sql.catalog.{custom_catalog}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "#         .config(\"spark.jars.packages\", additional_packages) \\\n",
    "#         .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "#         .config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n",
    "#         .enableHiveSupport() \\\n",
    "#         .getOrCreate()\n",
    "    \n",
    "#     # spark.sql(f\"\"\"\n",
    "#     # show tables in {custom_catalog}.gold\n",
    "#     # \"\"\").show(100,False)\n",
    "#     # print(\"train_model is called\")\n",
    "#     # Initialize Spark session\n",
    "#     # spark = SparkSession.builder.appName(\"Feature Engineering\").getOrCreate()\n",
    "    \n",
    "        \n",
    "#     # Disable SSL warnings\n",
    "#     urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "#     try:\n",
    "#         # Download predictions from MinIO\n",
    "#         minio_client = boto3.client(\n",
    "#             's3',\n",
    "#             endpoint_url=\"http://192.168.80.155:32000\",\n",
    "#             aws_access_key_id=\"admin\",\n",
    "#             aws_secret_access_key=\"dlyticaD123\",\n",
    "#             verify=False\n",
    "#         )\n",
    "        \n",
    "#         local_path = \"/tmp/predictions.csv\"\n",
    "#         minio_client.download_file(minio_bucket, segmentation_path, local_path)\n",
    "#         predictions = pd.read_csv(local_path)\n",
    "        \n",
    "#         # # Connect to Trino\n",
    "#         # conn = trino.dbapi.connect(\n",
    "#         #     host=\"192.168.80.155\",\n",
    "#         #     port=30071,\n",
    "#         #     user=\"ctzn.bank\",\n",
    "#         #     auth=BasicAuthentication(\"ctzn.bank\", \"ctzn.bank_123\"),\n",
    "#         #     catalog=\"iceberg\",\n",
    "#         #     schema=\"silver_crmuser\",\n",
    "#         #     http_scheme=\"https\",\n",
    "#         #     verify=False\n",
    "#         # )\n",
    "#         # cursor = conn.cursor()\n",
    "\n",
    "#         # # Create table if not exists\n",
    "#         # cursor.execute(\"\"\"\n",
    "#         # CREATE TABLE IF NOT EXISTS iceberg.silver_crmuser.customer_segments (\n",
    "#         #     cif_id VARCHAR,\n",
    "#         #     cluster_id INTEGER,\n",
    "#         #     processed_date TIMESTAMP\n",
    "#         # ) WITH (\n",
    "#         #     partitioning = ARRAY['processed_date']\n",
    "#         # )\n",
    "#         # \"\"\")\n",
    "\n",
    "#         # Insert predictions in batches\n",
    "#         batch_size = 1000\n",
    "#         for i in range(0, len(predictions), batch_size):\n",
    "#             batch = predictions[i:i+batch_size]\n",
    "#             values = [(str(row['custid']), int(row['prediction'])) for _, row in batch.iterrows()]\n",
    "            \n",
    "#             insert_sql = \"\"\"\n",
    "#             INSERT INTO iceberg.silver_crmuser.customer_segments \n",
    "#             (cif_id, cluster_id, processed_date) \n",
    "#             VALUES (?, ?, CURRENT_TIMESTAMP)\n",
    "#             \"\"\"\n",
    "            \n",
    "#             cursor.executemany(insert_sql, values)\n",
    "#             conn.commit()\n",
    "\n",
    "#         return f\"Successfully saved {len(predictions)} predictions to Trino\"\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in save_predictions_to_trino: {str(e)}\")\n",
    "#         raise\n",
    "#     finally:\n",
    "#         if 'cursor' in locals():\n",
    "#             cursor.close()\n",
    "#         if 'conn' in locals():\n",
    "#             conn.close()\n",
    "\n",
    "# Pipeline definition\n",
    "@pipeline(name=\"Customer Segmentation Pipeline\")\n",
    "def customer_segmentation_pipeline(\n",
    "    minio_bucket: str = \"ai360ctzn-customer-segmentation\"\n",
    "):\n",
    "    # Step 1: Fetch data\n",
    "    fetch_task = fetch_data_trino(minio_bucket=minio_bucket)\n",
    "    \n",
    "    # Step 2: Process data and create segments\n",
    "    segmentation_task = feature_engineering_and_segmentation(\n",
    "        file_path=fetch_task.output,\n",
    "        minio_bucket=minio_bucket\n",
    "    )\n",
    "    \n",
    "    # # Step 3: Save results to Trino\n",
    "    # save_task = save_predictions_to_trino(\n",
    "    #     segmentation_path=segmentation_task.outputs['segmentation_path'],\n",
    "    #     minio_bucket=minio_bucket\n",
    "    # )\n",
    "\n",
    "# Compile the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    from kfp import compiler\n",
    "    compiler.Compiler().compile(\n",
    "        customer_segmentation_pipeline,\n",
    "        \"customer_segmentation_pipeline.yaml\"\n",
    "    )\n",
    "    print(\"Pipeline compiled successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f939751b-c3e3-4cef-9c8e-61b901386375",
   "metadata": {},
   "source": [
    "### Adding dex authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "47d18b47-e713-4fd1-b292-4c8b90f07e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.parse import urlsplit, urlencode\n",
    "\n",
    "import kfp\n",
    "import requests\n",
    "import urllib3\n",
    "\n",
    "\n",
    "class KFPClientManager:\n",
    "    \"\"\"\n",
    "    A class that creates `kfp.Client` instances with Dex authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_url: str,\n",
    "        dex_username: str,\n",
    "        dex_password: str,\n",
    "        dex_auth_type: str = \"local\",\n",
    "        skip_tls_verify: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the KfpClient\"\n",
    "\n",
    "        :param api_url: the Kubeflow Pipelines API URL\n",
    "        :param skip_tls_verify: if True, skip TLS verification\n",
    "        :param dex_username: the Dex username\n",
    "        :param dex_password: the Dex password\n",
    "        :param dex_auth_type: the auth type to use if Dex has multiple enabled, one of: ['ldap', 'local']\n",
    "        \"\"\"\n",
    "        self._api_url = api_url\n",
    "        self._skip_tls_verify = skip_tls_verify\n",
    "        self._dex_username = dex_username\n",
    "        self._dex_password = dex_password\n",
    "        self._dex_auth_type = dex_auth_type\n",
    "        self._client = None\n",
    "\n",
    "        # disable SSL verification, if requested\n",
    "        if self._skip_tls_verify:\n",
    "            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "        # ensure `dex_default_auth_type` is valid\n",
    "        if self._dex_auth_type not in [\"ldap\", \"local\"]:\n",
    "            raise ValueError(\n",
    "                f\"Invalid `dex_auth_type` '{self._dex_auth_type}', must be one of: ['ldap', 'local']\"\n",
    "            )\n",
    "\n",
    "    def _get_session_cookies(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the session cookies by authenticating against Dex\n",
    "        :return: a string of session cookies in the form \"key1=value1; key2=value2\"\n",
    "        \"\"\"\n",
    "\n",
    "        # use a persistent session (for cookies)\n",
    "        s = requests.Session()\n",
    "\n",
    "        # GET the api_url, which should redirect to Dex\n",
    "        resp = s.get(\n",
    "            self._api_url, allow_redirects=True, verify=not self._skip_tls_verify\n",
    "        )\n",
    "        if resp.status_code == 200:\n",
    "            pass\n",
    "        elif resp.status_code == 403:\n",
    "            # if we get 403, we might be at the oauth2-proxy sign-in page\n",
    "            # the default path to start the sign-in flow is `/oauth2/start?rd=<url>`\n",
    "            url_obj = urlsplit(resp.url)\n",
    "            url_obj = url_obj._replace(\n",
    "                path=\"/oauth2/start\", query=urlencode({\"rd\": url_obj.path})\n",
    "            )\n",
    "            resp = s.get(\n",
    "                url_obj.geturl(), allow_redirects=True, verify=not self._skip_tls_verify\n",
    "            )\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f\"HTTP status code '{resp.status_code}' for GET against: {self._api_url}\"\n",
    "            )\n",
    "\n",
    "        # if we were NOT redirected, then the endpoint is unsecured\n",
    "        if len(resp.history) == 0:\n",
    "            # no cookies are needed\n",
    "            return \"\"\n",
    "\n",
    "        # if we are at `../auth` path, we need to select an auth type\n",
    "        url_obj = urlsplit(resp.url)\n",
    "        if re.search(r\"/auth$\", url_obj.path):\n",
    "            url_obj = url_obj._replace(\n",
    "                path=re.sub(r\"/auth$\", f\"/auth/{self._dex_auth_type}\", url_obj.path)\n",
    "            )\n",
    "\n",
    "        # if we are at `../auth/xxxx/login` path, then we are at the login page\n",
    "        if re.search(r\"/auth/.*/login$\", url_obj.path):\n",
    "            dex_login_url = url_obj.geturl()\n",
    "        else:\n",
    "            # otherwise, we need to follow a redirect to the login page\n",
    "            resp = s.get(\n",
    "                url_obj.geturl(), allow_redirects=True, verify=not self._skip_tls_verify\n",
    "            )\n",
    "            if resp.status_code != 200:\n",
    "                raise RuntimeError(\n",
    "                    f\"HTTP status code '{resp.status_code}' for GET against: {url_obj.geturl()}\"\n",
    "                )\n",
    "            dex_login_url = resp.url\n",
    "\n",
    "        # attempt Dex login\n",
    "        resp = s.post(\n",
    "            dex_login_url,\n",
    "            data={\"login\": self._dex_username, \"password\": self._dex_password},\n",
    "            allow_redirects=True,\n",
    "            verify=not self._skip_tls_verify,\n",
    "        )\n",
    "        if resp.status_code != 200:\n",
    "            raise RuntimeError(\n",
    "                f\"HTTP status code '{resp.status_code}' for POST against: {dex_login_url}\"\n",
    "            )\n",
    "\n",
    "        # if we were NOT redirected, then the login credentials were probably invalid\n",
    "        if len(resp.history) == 0:\n",
    "            raise RuntimeError(\n",
    "                f\"Login credentials are probably invalid - \"\n",
    "                f\"No redirect after POST to: {dex_login_url}\"\n",
    "            )\n",
    "\n",
    "        # if we are at `../approval` path, we need to approve the login\n",
    "        url_obj = urlsplit(resp.url)\n",
    "        if re.search(r\"/approval$\", url_obj.path):\n",
    "            dex_approval_url = url_obj.geturl()\n",
    "\n",
    "            # approve the login\n",
    "            resp = s.post(\n",
    "                dex_approval_url,\n",
    "                data={\"approval\": \"approve\"},\n",
    "                allow_redirects=True,\n",
    "                verify=not self._skip_tls_verify,\n",
    "            )\n",
    "            if resp.status_code != 200:\n",
    "                raise RuntimeError(\n",
    "                    f\"HTTP status code '{resp.status_code}' for POST against: {url_obj.geturl()}\"\n",
    "                )\n",
    "\n",
    "        return \"; \".join([f\"{c.name}={c.value}\" for c in s.cookies])\n",
    "\n",
    "    def _create_kfp_client(self) -> kfp.Client:\n",
    "        try:\n",
    "            session_cookies = self._get_session_cookies()\n",
    "        except Exception as ex:\n",
    "            raise RuntimeError(f\"Failed to get Dex session cookies\") from ex\n",
    "\n",
    "        # monkey patch the kfp.Client to support disabling SSL verification\n",
    "        # kfp only added support in v2: https://github.com/kubeflow/pipelines/pull/7174\n",
    "        original_load_config = kfp.Client._load_config\n",
    "\n",
    "        def patched_load_config(client_self, *args, **kwargs):\n",
    "            config = original_load_config(client_self, *args, **kwargs)\n",
    "            config.verify_ssl = not self._skip_tls_verify\n",
    "            return config\n",
    "\n",
    "        patched_kfp_client = kfp.Client\n",
    "        patched_kfp_client._load_config = patched_load_config\n",
    "\n",
    "        return patched_kfp_client(\n",
    "            host=self._api_url,\n",
    "            cookies=session_cookies,\n",
    "        )\n",
    "\n",
    "    def create_kfp_client(self) -> kfp.Client:\n",
    "        \"\"\"Get a newly authenticated Kubeflow Pipelines client.\"\"\"\n",
    "        return self._create_kfp_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4534f8ae-1f20-49e3-b402-818395613f1a",
   "metadata": {},
   "source": [
    "### Creating pipeline client and running pipeline dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e3ae23b0-2d38-421e-8e96-b8c8e63f0db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/kfp/client/client.py:159: FutureWarning: This client only works with Kubeflow Pipeline v2.0.0-beta.2 and later versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tutorial] Data passing in python components\n",
      "[Tutorial] DSL - Control structures\n",
      "Citizen_pipeline16\n",
      "Customer_Segmentation_Pipeline\n",
      "recommendation_pipeline\n",
      "pipeline matched\n",
      "600686ba-f0cd-4421-9d07-a3ae96f9e74f\n",
      "Version ID: 872689fd-5ca7-44bb-bb11-e9b1eedb4aff, Name: recommendation_pipeline\n",
      "Deleted version: 872689fd-5ca7-44bb-bb11-e9b1eedb4aff\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://192.168.80.155:31904/pipeline/#/pipelines/details/59d30069-d701-4b53-aed1-21c77cc2f124\" target=\"_blank\" >Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://192.168.80.155:31904/pipeline/#/experiments/details/af85287c-a6f4-45e8-b8a5-3fc053065211\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://192.168.80.155:31904/pipeline/#/runs/details/e7a6c44d-dbc2-4d44-a05c-095acf305874\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kfp_client_manager = KFPClientManager(\n",
    "    api_url=\"http://192.168.80.155:31904/pipeline\",\n",
    "    skip_tls_verify=True,\n",
    "\n",
    "    dex_username=\"user@dlytica.com\",\n",
    "    dex_password=\"dlytica@D123#\",\n",
    "\n",
    "    # can be 'ldap' or 'local' depending on your Dex configuration\n",
    "    dex_auth_type=\"local\",\n",
    ")\n",
    "\n",
    "# get a newly authenticated KFP client\n",
    "# TIP: long-lived sessions might need to get a new client when their session expires\n",
    "kfp_client = kfp_client_manager.create_kfp_client()\n",
    "\n",
    "# # test the client by listing experiments\n",
    "# experiments = kfp_client.list_experiments(namespace=\"kubeflow-user-example-com\")\n",
    "# print(experiments)\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "pipeline_name = 'recommendation_pipeline'\n",
    "pipelines = kfp_client.list_pipelines(page_size=100)\n",
    "# experiments = kfp_client.list_experiments()\n",
    "# print(experiments)\n",
    "# print(pipelines)\n",
    "for pipeline in pipelines.pipelines:\n",
    "    print(pipeline.display_name)\n",
    "    if pipeline.display_name == pipeline_name:\n",
    "        print(\"pipeline matched\")\n",
    "        pipeline_id = pipeline.pipeline_id\n",
    "        print(pipeline_id)\n",
    "        # pipeline_id = '4758a6db-8057-463f-a185-a68f3e45d920'\n",
    "        pipeline_versions = kfp_client.list_pipeline_versions(pipeline_id=pipeline_id, page_size=100)\n",
    "        for version in pipeline_versions.pipeline_versions:\n",
    "            print(f\"Version ID: {version.pipeline_version_id}, Name: {version.display_name}\")\n",
    "        for version in pipeline_versions.pipeline_versions:\n",
    "            version_id = version.pipeline_version_id\n",
    "            try:\n",
    "                kfp_client.delete_pipeline_version(pipeline_id = pipeline_id, pipeline_version_id=version_id)  \n",
    "                print(f\"Deleted version: {version_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete version {version_id}: {e}\")\n",
    "        kfp_client.delete_pipeline(pipeline.pipeline_id)\n",
    "description = 'this is automl pipeline'\n",
    "kfp_client.upload_pipeline('customer_segmentation_pipeline.yaml',pipeline_name = pipeline_name, description = description)\n",
    "\n",
    "run = kfp_client.create_run_from_pipeline_func(\n",
    "    customer_segmentation_pipeline,\n",
    "    arguments={\"minio_bucket\": \"ai360ctzn-customer-segmentation\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af526e55-f620-4264-8440-8cebb3a515da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline compiled successfully.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddde60b6-e5b5-4ae3-af94-c09f0746bcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIR CLOSE GARNU PARCAH PLEASE SAVE UR WORK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6609e91d-7b4b-43a9-85c6-1e33f16db36d",
   "metadata": {},
   "source": [
    "SIR TODY OUR OFFICE TIME IS 1:30 \n",
    "IM GETTING LATE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf549b5b-f3aa-4689-b49f-cf90d9c43433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

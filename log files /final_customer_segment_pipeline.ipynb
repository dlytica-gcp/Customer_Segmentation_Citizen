{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6b8d1bd-e63e-4985-b8d8-260f2563d286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp.dsl import pipeline, component\n",
    "from datetime import datetime\n",
    "from typing import NamedTuple\n",
    "\n",
    "@component(    base_image='bitnami/spark:3.5',  # Spark-compatible base image\n",
    "    packages_to_install=['pandas','pyarrow','s3fs','boto3','psycopg2-binary','trino'])\n",
    "\n",
    "def fetch_data(minio_bucket: str) -> str:\n",
    "    import boto3\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Initialize MinIO client using boto3\n",
    "    minio_client = boto3.client(\n",
    "        's3',\n",
    "        endpoint_url=\"http://192.168.80.155:32000\",\n",
    "        aws_access_key_id=\"admin\",\n",
    "        aws_secret_access_key=\"dlyticaD123\"\n",
    "    )\n",
    "    # from fastapi import FastAPI\n",
    "    from trino.dbapi import connect\n",
    "    # from trino.auth import BasicAuthentication\n",
    "    # import json\n",
    "    import csv\n",
    "    \n",
    "    # Trino Connection Details\n",
    "    TRINO_HOST = \"192.168.80.155\"\n",
    "    TRINO_PORT = \"30686\"\n",
    "    TRINO_USER = \"admin\"\n",
    "    # TRINO_USER = \"your_username\"\n",
    "    # TRINO_PASSWORD = \"admin123\"\n",
    "    TRINO_CATALOG = \"iceberg\"\n",
    "    TRINO_SCHEMA = \"gold\"\n",
    "    \n",
    "    # Your SQL Query (put the full query here)\n",
    "    SQL_QUERY = \"\"\" \n",
    "         WITH debit_credit_summary AS (\n",
    "    SELECT\n",
    "        a.cif_id,\n",
    "        a.nepali_fiscal_year,\n",
    "        g.schm_type,\n",
    "        SUM(total_debit_tran_vol) AS dr_amount_count,\n",
    "        SUM(total_credit_tran_vol) AS cr_amount_count,\n",
    "        COUNT(DISTINCT CASE WHEN total_debit_tran_vol > 0 THEN a.nepali_month END) AS dr_month_count,\n",
    "        COUNT(DISTINCT CASE WHEN total_credit_tran_vol > 0 THEN a.nepali_month END) AS cr_month_count,\n",
    "        SUM(total_debit_tran_count) AS total_debit_transaction,\n",
    "        SUM(total_credit_tran_count) AS total_credit_transaction,\n",
    "        CASE\n",
    "            WHEN COUNT(DISTINCT CASE WHEN total_debit_tran_count > 0 THEN a.nepali_month END) > 0 THEN\n",
    "                SUM(total_debit_tran_count) / COUNT(DISTINCT CASE WHEN total_debit_tran_count > 0 THEN a.nepali_month END)\n",
    "        END AS average_debit_transaction_count,\n",
    "        CASE\n",
    "            WHEN COUNT(DISTINCT CASE WHEN total_credit_tran_count > 0 THEN a.nepali_month END) > 0 THEN\n",
    "                SUM(total_credit_tran_count) / COUNT(DISTINCT CASE WHEN total_credit_tran_count > 0 THEN a.nepali_month END)\n",
    "        END AS average_credit_transaction_count,\n",
    "        CASE\n",
    "            WHEN COUNT(DISTINCT CASE WHEN total_debit_tran_vol > 0 THEN a.nepali_month END) > 0 THEN\n",
    "                SUM(total_debit_tran_vol) / COUNT(DISTINCT CASE WHEN total_debit_tran_vol > 0 THEN a.nepali_month END)\n",
    "            ELSE 0\n",
    "        END AS dr_average,\n",
    "        CASE\n",
    "            WHEN COUNT(DISTINCT CASE WHEN total_credit_tran_vol > 0 THEN a.nepali_month END) > 0 THEN\n",
    "                SUM(total_credit_tran_vol) / COUNT(DISTINCT CASE WHEN total_credit_tran_vol > 0 THEN a.nepali_month END)\n",
    "            ELSE 0\n",
    "        END AS cr_average,\n",
    "        (CASE\n",
    "            WHEN COUNT(DISTINCT CASE WHEN total_credit_tran_vol > 0 THEN a.nepali_month END) > 0 THEN\n",
    "                SUM(total_credit_tran_vol) / COUNT(DISTINCT CASE WHEN total_credit_tran_vol > 0 THEN a.nepali_month END)\n",
    "            ELSE 0\n",
    "        END) - (CASE\n",
    "            WHEN COUNT(DISTINCT CASE WHEN total_debit_tran_vol > 0 THEN a.nepali_month END) > 0 THEN\n",
    "                SUM(total_debit_tran_vol) / COUNT(DISTINCT CASE WHEN total_debit_tran_vol > 0 THEN a.nepali_month END)\n",
    "            ELSE 0\n",
    "        END) AS average_yearly_saving,\n",
    "        SUM(total_credit_tran_vol) - SUM(total_debit_tran_vol) AS total_yearly_saving\n",
    " \n",
    "    FROM\n",
    "        gold.mv_fact_deposit_account_insights AS a\n",
    "    JOIN\n",
    "        gold.dim_gam AS g ON g.cif_id = a.cif_id\n",
    "    WHERE\n",
    "        g.schm_type = 'SBA' AND g.schm_type != 'TDA'\n",
    "    GROUP BY\n",
    "        a.cif_id, a.nepali_fiscal_year, g.schm_type\n",
    "    HAVING\n",
    "        a.nepali_fiscal_year = '2081/2082'\n",
    "    ),\n",
    "    \n",
    "    customer_info AS (\n",
    "    SELECT\n",
    "        cif_id,\n",
    "        CAST(SUBSTR(cust_dob, 1, 4) AS INTEGER) AS birth_year,\n",
    "        EXTRACT(YEAR FROM CURRENT_DATE) - CAST(SUBSTR(cust_dob, 1, 4) AS INTEGER) AS age,\n",
    "        CASE \n",
    "            WHEN CAST(SUBSTR(cust_dob, 1, 4) AS INTEGER) BETWEEN 1946 AND 1964 THEN 'Baby Boomers'\n",
    "            WHEN CAST(SUBSTR(cust_dob, 1, 4) AS INTEGER) BETWEEN 1965 AND 1980 THEN 'Gen X'\n",
    "            WHEN CAST(SUBSTR(cust_dob, 1, 4) AS INTEGER) BETWEEN 1981 AND 1996 THEN 'Gen Y'\n",
    "            WHEN CAST(SUBSTR(cust_dob, 1, 4) AS INTEGER) BETWEEN 1997 AND 2012 THEN 'Gen Z'\n",
    "            ELSE 'Minor' \n",
    "        END AS age_group,\n",
    "        employment_status,\n",
    "        marital_status,\n",
    "        occupation,\n",
    "        salary_per_month,\n",
    "        gender\n",
    "    FROM\n",
    "        gold.dim_customers\n",
    "    WHERE\n",
    "        cust_type = 'INDIVIDUAL'\n",
    "    ),\n",
    "    \n",
    "    salary_range AS (\n",
    "    SELECT\n",
    "        cif_id,\n",
    "        CASE\n",
    "            WHEN salary_per_month IS NULL THEN NULL\n",
    "            WHEN salary_per_month < 10000 THEN 'low salary'\n",
    "            WHEN salary_per_month >= 10000 AND salary_per_month < 30000 THEN 'moderate salary'\n",
    "            ELSE 'high salary'\n",
    "        END AS salary_range\n",
    "    FROM\n",
    "        gold.dim_customers\n",
    "    ),\n",
    "    \n",
    "    withdrawal_amount_trends AS (\n",
    "    SELECT\n",
    "        cif_id,\n",
    "        SUM(total_credit_tran_vol) AS total_credit_tran_vol,\n",
    "        CASE\n",
    "            WHEN SUM(total_credit_tran_vol) < 0 THEN 'Negative Withdrawal'\n",
    "            WHEN SUM(total_credit_tran_vol) BETWEEN 0 AND 100000 THEN 'Low Withdrawal'\n",
    "            WHEN SUM(total_credit_tran_vol) BETWEEN 100000 AND 1000000 THEN 'Moderate Withdrawal'\n",
    "            ELSE 'High Withdrawal'\n",
    "        END AS withdrawal_trends\n",
    "    FROM\n",
    "        gold.mv_fact_deposit_account_insights\n",
    "    GROUP BY\n",
    "        cif_id\n",
    "    ),\n",
    "    \n",
    "    account_details AS (\n",
    "    SELECT\n",
    "        cif_id,\n",
    "        COUNT(foracid) AS total_accounts,\n",
    "        COUNT(CASE WHEN schm_type = 'SBA' THEN foracid END) AS total_saving_accounts,\n",
    "        COUNT(CASE WHEN schm_type = 'TDA' THEN foracid END) AS total_fixed_accounts,\n",
    "        COUNT(CASE WHEN schm_type = 'LAA' THEN foracid END) AS total_loan_accounts,\n",
    "        COUNT(CASE WHEN schm_type = 'ODA' THEN foracid END) AS total_overdraft_accounts\n",
    "    FROM\n",
    "        gold.dim_gam\n",
    "    GROUP BY\n",
    "        cif_id\n",
    "    ),\n",
    "    \n",
    "    dormant_table AS (\n",
    "    SELECT \n",
    "        cif_id,\n",
    "        MAX(CAST(SUBSTR(dormant_date, 1, 10) AS DATE)) AS last_dormant_date,\n",
    "        SUM(DATE_DIFF('day', CAST(SUBSTR(dormant_date, 1, 10) AS DATE), CURRENT_DATE)) AS total_dormant_days,\n",
    "        COUNT(CASE WHEN dormant_date IS NOT NULL THEN foracid END) AS total_dormant_account,\n",
    "        COUNT(CASE WHEN dormant_date IS NULL THEN foracid END) AS total_active_account\n",
    "    FROM \n",
    "        gold.dim_deposit_accounts \n",
    "    GROUP BY \n",
    "        cif_id\n",
    "    ),\n",
    "    \n",
    "    first_ac_open_last_ac_open AS (\n",
    "    SELECT\n",
    "        cif_id,\n",
    "        MAX(date_diff('day', acct_opn_date_date_part, CURRENT_DATE)) AS first_account_opened_days,\n",
    "        MIN(date_diff('day', acct_opn_date_date_part, CURRENT_DATE)) AS last_account_opened_days,\n",
    "        MIN(acct_opn_date_date_part) AS first_account_date,\n",
    "        MAX(acct_opn_date_date_part) AS last_account_date\n",
    "    FROM\n",
    "        gold.dim_gam\n",
    "    GROUP BY\n",
    "        cif_id\n",
    "    ),\n",
    "    \n",
    "    recently_active_status AS (\n",
    "    SELECT\n",
    "        cif_id,\n",
    "        MAX(TRY_CAST(SUBSTR(last_customer_induced_transaction_date, 1, 10) AS DATE)) AS last_transaction_date,\n",
    "        MAX(TRY_CAST(SUBSTR(dormant_date, 1, 10) AS DATE)) AS last_dormant_date,\n",
    "        CASE\n",
    "            WHEN MAX(TRY_CAST(SUBSTR(dormant_date, 1, 10) AS DATE)) < MAX(TRY_CAST(SUBSTR(last_customer_induced_transaction_date, 1, 10) AS DATE)) THEN 'Y'\n",
    "            ELSE 'N'\n",
    "        END AS Recently_Active\n",
    "    FROM\n",
    "        gold.dim_deposit_accounts\n",
    "    GROUP BY\n",
    "        cif_id\n",
    "    )\n",
    "    \n",
    "    SELECT DISTINCT\n",
    "    dcs.cif_id,\n",
    "    a.total_accounts,\n",
    "    a.total_saving_accounts,\n",
    "    a.total_fixed_accounts,\n",
    "    a.total_loan_accounts,\n",
    "    a.total_overdraft_accounts,\n",
    "    dcs.schm_type AS scheme_type,\n",
    "    dcs.total_debit_transaction AS total_debit_transaction_count,\n",
    "    dcs.total_credit_transaction AS total_credit_transaction_count,\n",
    "    dcs.average_debit_transaction_count,\n",
    "    dcs.average_credit_transaction_count,\n",
    "    dcs.dr_amount_count AS total_debit_transaction_amount,\n",
    "    dcs.cr_amount_count AS total_credit_transaction_amount,\n",
    "    dcs.dr_average AS average_debit_amount,\n",
    "    dcs.cr_average AS average_credit_amount,\n",
    "    dcs.average_yearly_saving,\n",
    "    dcs.total_yearly_saving,\n",
    "    ci.employment_status,\n",
    "    ci.marital_status,\n",
    "    ci.occupation,\n",
    "    ci.gender,\n",
    "    ci.age_group,\n",
    "    -- sr.salary_range,\n",
    "    wat.withdrawal_trends,\n",
    "    ad.total_dormant_days,\n",
    "    ad.total_dormant_account,\n",
    "    ad.total_active_account,\n",
    "    fa.first_account_opened_days,\n",
    "    fa.last_account_opened_days,\n",
    "    fa.first_account_date,\n",
    "    fa.last_account_date,\n",
    "    ra.last_transaction_date,\n",
    "    ra.last_dormant_date,\n",
    "    ra.Recently_Active\n",
    "    FROM\n",
    "    debit_credit_summary dcs\n",
    "    JOIN customer_info ci ON dcs.cif_id = ci.cif_id\n",
    "    JOIN withdrawal_amount_trends wat ON dcs.cif_id = wat.cif_id\n",
    "    JOIN salary_range sr ON dcs.cif_id = sr.cif_id\n",
    "    JOIN account_details a ON dcs.cif_id = a.cif_id\n",
    "    LEFT JOIN dormant_table ad ON dcs.cif_id = ad.cif_id\n",
    "    LEFT JOIN first_ac_open_last_ac_open fa ON dcs.cif_id = fa.cif_id\n",
    "    LEFT JOIN recently_active_status ra ON dcs.cif_id = ra.cif_id\n",
    "    ORDER BY dcs.cif_id\n",
    "    \"\"\"\n",
    "    try:\n",
    "            print(\"Connecting to Trino...\")\n",
    "            conn = connect(\n",
    "                host=TRINO_HOST,\n",
    "                port=TRINO_PORT,\n",
    "                user=TRINO_USER,\n",
    "                # auth=BasicAuthentication(TRINO_USER, TRINO_PASSWORD), \n",
    "                catalog=TRINO_CATALOG,\n",
    "                schema=TRINO_SCHEMA\n",
    "                # http_scheme=\"https\"\n",
    "            )\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            print(\"Executing query...\")\n",
    "            cursor.execute(SQL_QUERY)\n",
    "    \n",
    "            # Fetch data\n",
    "            rows = cursor.fetchall()\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "    \n",
    "            conn.close()\n",
    "            print(\"Query executed successfully.\")\n",
    "    \n",
    "            # Convert to JSON\n",
    "            results = [dict(zip(columns, row)) for row in rows]\n",
    "    \n",
    "            # Save JSON file\n",
    "              # Save to CSV file\n",
    "            output_file = \"trino_data.csv\"\n",
    "            with open(output_file, \"w\", newline=\"\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(columns)  # Write header\n",
    "                writer.writerows(rows)    # Write data rows\n",
    "    \n",
    "            print(f\"Data saved to {output_file}\")\n",
    "            # return output_file  # Return the file path\n",
    "    \n",
    "    except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            return None\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    saving_file_name = f\"data/AI360FDREC_raw_data_{current_date}.csv\"\n",
    "    os.makedirs(saving_file_name, exist_ok=True)\n",
    "    minio_client.upload_file(output_file, minio_bucket, saving_file_name)\n",
    "\n",
    "    print(\"CSV file uploaded to MinIO successfully.\")\n",
    "    return saving_file_name\n",
    "    \n",
    "@component(\n",
    "    base_image='quay.io/datanature_dev/jupyternotebook:java_home14',\n",
    "    packages_to_install=[\n",
    "        'pyspark==3.5.0',\n",
    "        'pandas==2.0.3',\n",
    "        'numpy==1.24.4',\n",
    "        'boto3==1.28.57',\n",
    "        'scikit-learn==1.3.0',\n",
    "        'matplotlib==3.7.2',\n",
    "        'pyarrow==12.0.1',\n",
    "        'urllib3==2.0.4'\n",
    "    ]\n",
    ")\n",
    "def feature_engineering_and_segmentation(\n",
    "    file_path: str, \n",
    "    minio_bucket: str\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('segmentation_path', str),\n",
    "    ('model_path', str),\n",
    "    ('cluster_plot_path', str),\n",
    "    ('cluster_interpretations', str),\n",
    "    ('final_output_path', str)\n",
    "]):\n",
    "    # First ensure numpy is imported with the correct version\n",
    "    import numpy as np\n",
    "    np.__version__  # This helps catch version issues early\n",
    "    \n",
    "    # Then import other packages\n",
    "    import boto3\n",
    "    from botocore.config import Config\n",
    "    import pandas as pd\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')  # Set non-interactive backend\n",
    "    import matplotlib.pyplot as plt\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "    import json\n",
    "    import tempfile\n",
    "    import time\n",
    "    from collections import namedtuple\n",
    "    from io import StringIO\n",
    "    import shutil\n",
    "    \n",
    "    # Now import pyspark components\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col, when, mean, monotonically_increasing_id, stddev, count, abs\n",
    "    from pyspark.sql.types import DoubleType, StringType\n",
    "    from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\n",
    "    from pyspark.ml.clustering import BisectingKMeans\n",
    "    from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "    from pyspark.sql.functions import udf\n",
    "\n",
    "    def calculate_feature_distinctiveness(cluster_profiles):\n",
    "        \"\"\"Enhanced feature distinctiveness calculation\"\"\"\n",
    "        feature_variance = cluster_profiles.std()\n",
    "        distinctiveness = feature_variance / (feature_variance.max() + 1e-10)\n",
    "        return distinctiveness\n",
    "\n",
    "    def interpret_clusters(cluster_profiles):\n",
    "        interpretations = {}\n",
    "        distinctiveness = calculate_feature_distinctiveness(cluster_profiles)\n",
    "        \n",
    "        for cluster in cluster_profiles.index:\n",
    "            features = cluster_profiles.columns.tolist()\n",
    "            profile_components = []\n",
    "            \n",
    "            if 'purchases' in features:\n",
    "                spend_level = cluster_profiles.loc[cluster, 'purchases']\n",
    "                mean_spend = cluster_profiles['purchases'].mean()\n",
    "                std_spend = cluster_profiles['purchases'].std()\n",
    "                \n",
    "                if spend_level > mean_spend + 1.5 * std_spend:\n",
    "                    profile_components.append(\"High Spender\")\n",
    "                elif spend_level < mean_spend - 1.5 * std_spend:\n",
    "                    profile_components.append(\"Low Spender\")\n",
    "                else:\n",
    "                    profile_components.append(\"Moderate Spender\")\n",
    "            \n",
    "            if 'cash_advance' in features and 'cash_advance_frequency' in features:\n",
    "                cash_advance = cluster_profiles.loc[cluster, 'cash_advance']\n",
    "                cash_freq = cluster_profiles.loc[cluster, 'cash_advance_frequency']\n",
    "                mean_cash = cluster_profiles['cash_advance'].mean()\n",
    "                mean_freq = cluster_profiles['cash_advance_frequency'].mean()\n",
    "                \n",
    "                if cash_advance > mean_cash * 2 and cash_freq > mean_freq * 2:\n",
    "                    profile_components.append(\"Intensive Cash Advance User\")\n",
    "                elif cash_advance < mean_cash * 0.5 and cash_freq < mean_freq * 0.5:\n",
    "                    profile_components.append(\"Rare Cash Advance User\")\n",
    "            \n",
    "            if 'balance' in features and 'credit_limit' in features:\n",
    "                balance_ratio = cluster_profiles.loc[cluster, 'balance'] / (cluster_profiles.loc[cluster, 'credit_limit'] + 1e-10)\n",
    "                \n",
    "                if balance_ratio > 0.8:\n",
    "                    profile_components.append(\"Very High Credit Utilization\")\n",
    "                elif balance_ratio > 0.5:\n",
    "                    profile_components.append(\"High Credit Utilization\")\n",
    "                elif balance_ratio < 0.2:\n",
    "                    profile_components.append(\"Low Credit Utilization\")\n",
    "                else:\n",
    "                    profile_components.append(\"Moderate Credit Utilization\")\n",
    "            \n",
    "            if 'payments' in features and 'minimum_payments' in features:\n",
    "                payment_ratio = cluster_profiles.loc[cluster, 'payments'] / (cluster_profiles.loc[cluster, 'minimum_payments'] + 1e-10)\n",
    "                \n",
    "                if payment_ratio > 3:\n",
    "                    profile_components.append(\"Aggressive Overpayer\")\n",
    "                elif payment_ratio > 2:\n",
    "                    profile_components.append(\"Consistent Overpayer\")\n",
    "                elif payment_ratio < 1.2:\n",
    "                    profile_components.append(\"Minimum Payment User\")\n",
    "            \n",
    "            if 'balance' in features and 'credit_limit' in features and 'cash_advance' in features:\n",
    "                risk_score = (\n",
    "                    cluster_profiles.loc[cluster, 'balance'] / (cluster_profiles.loc[cluster, 'credit_limit'] + 1e-10) * 0.4 +\n",
    "                    cluster_profiles.loc[cluster, 'cash_advance'] / (cluster_profiles.loc[cluster, 'credit_limit'] + 1e-10) * 0.6\n",
    "                )\n",
    "                \n",
    "                if risk_score > 0.7:\n",
    "                    profile_components.append(\"Extremely High Financial Risk\")\n",
    "                elif risk_score > 0.5:\n",
    "                    profile_components.append(\"High Financial Risk\")\n",
    "                elif risk_score < 0.2:\n",
    "                    profile_components.append(\"Low Financial Risk\")\n",
    "                else:\n",
    "                    profile_components.append(\"Moderate Financial Risk\")\n",
    "            \n",
    "            if profile_components:\n",
    "                interpretations[cluster] = \"; \".join(profile_components)\n",
    "            else:\n",
    "                interpretations[cluster] = \"Undefined Customer Segment\"\n",
    "        \n",
    "        return interpretations\n",
    "\n",
    "    def upload_to_minio(local_path, bucket, object_key, max_retries=5):\n",
    "        \"\"\"Uploads a file to MinIO with improved error handling and content-length issues fixed\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Create a fresh client for each upload attempt\n",
    "                client = boto3.client(\n",
    "                    's3',\n",
    "                    endpoint_url=\"http://192.168.80.155:32000\",\n",
    "                    aws_access_key_id=\"admin\",\n",
    "                    aws_secret_access_key=\"dlyticaD123\",\n",
    "                    verify=False,\n",
    "                    config=Config(\n",
    "                        connect_timeout=30,\n",
    "                        read_timeout=60,\n",
    "                        retries={'max_attempts': 3}\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                # Get the file size\n",
    "                file_size = os.path.getsize(local_path)\n",
    "                \n",
    "                # For CSV files, we'll read and write with pandas to ensure proper formatting\n",
    "                if object_key.endswith('.csv'):\n",
    "                    df = pd.read_csv(local_path)\n",
    "                    csv_buffer = StringIO()\n",
    "                    df.to_csv(csv_buffer, index=False)\n",
    "                    client.put_object(\n",
    "                        Bucket=bucket,\n",
    "                        Key=object_key,\n",
    "                        Body=csv_buffer.getvalue(),\n",
    "                        ContentType='text/csv'\n",
    "                    )\n",
    "                else:\n",
    "                    # For non-CSV files, use standard upload\n",
    "                    with open(local_path, 'rb') as file_data:\n",
    "                        client.put_object(\n",
    "                            Bucket=bucket,\n",
    "                            Key=object_key,\n",
    "                            Body=file_data,\n",
    "                            ContentLength=file_size\n",
    "                        )\n",
    "                    \n",
    "                print(f\"Successfully uploaded {local_path} to {bucket}/{object_key}\")\n",
    "                return f\"{bucket}/{object_key}\"\n",
    "                \n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = 2 ** attempt  # Exponential backoff\n",
    "                    print(f\"Upload attempt {attempt+1} failed: {str(e)}. Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Upload failed after {max_retries} attempts: {str(e)}\")\n",
    "                    raise\n",
    "\n",
    "    def clean_directory_structure(minio_client, bucket):\n",
    "        \"\"\"Clean up and organize the directory structure in MinIO\"\"\"\n",
    "        try:\n",
    "            # Create necessary folders if they don't exist\n",
    "            for folder in ['results/csv/', 'results/json/', 'images/', 'models/', 'archive/']:\n",
    "                minio_client.put_object(Bucket=bucket, Key=folder, Body='')\n",
    "            \n",
    "            # Move old results to archive if they exist\n",
    "            current_date = datetime.now().strftime('%m_%d')\n",
    "            \n",
    "            # Archive previous CSV if exists\n",
    "            try:\n",
    "                csv_objects = minio_client.list_objects_v2(Bucket=bucket, Prefix='results/csv/')\n",
    "                if 'Contents' in csv_objects:\n",
    "                    for obj in csv_objects['Contents']:\n",
    "                        if obj['Key'].endswith('.csv'):\n",
    "                            # Extract filename\n",
    "                            filename = os.path.basename(obj['Key'])\n",
    "                            # Copy to archive\n",
    "                            minio_client.copy_object(\n",
    "                                Bucket=bucket,\n",
    "                                CopySource={'Bucket': bucket, 'Key': obj['Key']},\n",
    "                                Key=f'archive/{filename}'\n",
    "                            )\n",
    "                            # Delete original\n",
    "                            minio_client.delete_object(Bucket=bucket, Key=obj['Key'])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not archive previous CSV files: {str(e)}\")\n",
    "            \n",
    "            # Archive previous JSON if exists\n",
    "            try:\n",
    "                json_objects = minio_client.list_objects_v2(Bucket=bucket, Prefix='results/json/')\n",
    "                if 'Contents' in json_objects:\n",
    "                    for obj in json_objects['Contents']:\n",
    "                        if obj['Key'].endswith('.json'):\n",
    "                            # Extract filename\n",
    "                            filename = os.path.basename(obj['Key'])\n",
    "                            # Copy to archive\n",
    "                            minio_client.copy_object(\n",
    "                                Bucket=bucket,\n",
    "                                CopySource={'Bucket': bucket, 'Key': obj['Key']},\n",
    "                                Key=f'archive/{filename}'\n",
    "                            )\n",
    "                            # Delete original\n",
    "                            minio_client.delete_object(Bucket=bucket, Key=obj['Key'])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not archive previous JSON files: {str(e)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not clean directory structure: {str(e)}\")\n",
    "\n",
    "    # Initialize Spark with optimized settings\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"AdvancedCustomerSegmentation\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "        .config(\"spark.executor.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Initialize MinIO client with improved configuration\n",
    "    boto_config = Config(\n",
    "        connect_timeout=30,\n",
    "        read_timeout=60,\n",
    "        retries={'max_attempts': 3}\n",
    "    )\n",
    "    \n",
    "    minio_client = boto3.client(\n",
    "        's3',\n",
    "        endpoint_url=\"http://192.168.80.155:32000\",\n",
    "        aws_access_key_id=\"admin\",\n",
    "        aws_secret_access_key=\"dlyticaD123\",\n",
    "        verify=False,\n",
    "        config=boto_config\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Clean up and organize directory structure\n",
    "        clean_directory_structure(minio_client, minio_bucket)\n",
    "        \n",
    "        current_date = datetime.now().strftime('%m_%d')\n",
    "        today_full = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        local_path = \"/tmp/raw_data.csv\"\n",
    "        \n",
    "        # Download with retry logic\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                minio_client.download_file(minio_bucket, file_path, local_path)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise\n",
    "                time.sleep(5)\n",
    "\n",
    "        # Read data with explicit schema\n",
    "        df = spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .csv(local_path)\n",
    "\n",
    "        if 'custid' in df.columns:\n",
    "            df = df.withColumnRenamed(\"custid\", \"cif_id\")\n",
    "\n",
    "        # Feature engineering\n",
    "        df = df.withColumn(\"balance_utilization_ratio\", \n",
    "            when(col(\"credit_limit\") != 0, col(\"balance\") / col(\"credit_limit\")).otherwise(0)\n",
    "        )\n",
    "        df = df.withColumn(\"cash_advance_intensity\", \n",
    "            when(col(\"credit_limit\") != 0, col(\"cash_advance\") / col(\"credit_limit\")).otherwise(0)\n",
    "        )\n",
    "        df = df.withColumn(\"payment_effort_ratio\", \n",
    "            when(col(\"minimum_payments\") != 0, col(\"payments\") / col(\"minimum_payments\")).otherwise(0)\n",
    "        )\n",
    "        df = df.withColumn(\"purchase_diversity\", \n",
    "            abs(col(\"oneoff_purchases\") - col(\"installments_purchases\")) / \n",
    "            (abs(col(\"oneoff_purchases\") + col(\"installments_purchases\")) + 1e-10)\n",
    "        )\n",
    "        df = df.na.fill(0)\n",
    "\n",
    "        numeric_cols = [\n",
    "            'purchases', 'oneoff_purchases', 'installments_purchases', \n",
    "            'cash_advance', 'cash_advance_frequency', \n",
    "            'balance', 'credit_limit', \n",
    "            'payments', 'minimum_payments',\n",
    "            'balance_utilization_ratio', \n",
    "            'cash_advance_intensity', \n",
    "            'payment_effort_ratio',\n",
    "            'purchase_diversity'\n",
    "        ]\n",
    "\n",
    "        # Feature transformation\n",
    "        assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "        assembled = assembler.transform(df.drop(\"cif_id\"))\n",
    "        scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "        scaled_data = scaler.fit(assembled).transform(assembled)\n",
    "\n",
    "        # Dimensionality reduction\n",
    "        pca = PCA(k=5, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
    "        pca_data = pca.fit(scaled_data).transform(scaled_data)\n",
    "\n",
    "        # Clustering optimization\n",
    "        evaluator = ClusteringEvaluator(featuresCol='pca_features')\n",
    "        k_range = range(6, 15)\n",
    "        k_metrics = []\n",
    "\n",
    "        for k in k_range:\n",
    "            kmeans = BisectingKMeans(k=k, seed=42, featuresCol=\"pca_features\")\n",
    "            model = kmeans.fit(pca_data)\n",
    "            predictions = model.transform(pca_data)\n",
    "            \n",
    "            cost = model.summary.trainingCost if hasattr(model.summary, 'trainingCost') else 0\n",
    "            silhouette = evaluator.evaluate(predictions)\n",
    "            \n",
    "            centers = model.clusterCenters()\n",
    "            separation = np.mean([np.min(np.linalg.norm(centers[i] - centers[j])) \n",
    "                                  for i in range(len(centers)) \n",
    "                                  for j in range(i+1, len(centers))])\n",
    "            \n",
    "            k_metrics.append({\n",
    "                'k': k,\n",
    "                'cost': cost,\n",
    "                'silhouette': silhouette,\n",
    "                'separation': separation\n",
    "            })\n",
    "\n",
    "        # Determine optimal k\n",
    "        k_metrics_df = pd.DataFrame(k_metrics)\n",
    "        k_metrics_df['normalized_cost'] = (k_metrics_df['cost'] - k_metrics_df['cost'].min()) / (k_metrics_df['cost'].max() - k_metrics_df['cost'].min())\n",
    "        k_metrics_df['normalized_silhouette'] = (k_metrics_df['silhouette'] - k_metrics_df['silhouette'].min()) / (k_metrics_df['silhouette'].max() - k_metrics_df['silhouette'].min())\n",
    "        k_metrics_df['normalized_separation'] = (k_metrics_df['separation'] - k_metrics_df['separation'].min()) / (k_metrics_df['separation'].max() - k_metrics_df['separation'].min())\n",
    "        \n",
    "        k_metrics_df['composite_score'] = (\n",
    "            0.3 * (1 - k_metrics_df['normalized_cost']) +\n",
    "            0.4 * k_metrics_df['normalized_silhouette'] +\n",
    "            0.3 * k_metrics_df['normalized_separation']\n",
    "        )\n",
    "        \n",
    "        optimal_k = k_metrics_df.loc[k_metrics_df['composite_score'].idxmax(), 'k']\n",
    "\n",
    "        # Plotting\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15))\n",
    "        ax1.plot(k_metrics_df['k'], k_metrics_df['cost'], marker='o', color='blue')\n",
    "        ax1.set_xlabel('Number of Clusters (k)')\n",
    "        ax1.set_ylabel('Cost', color='blue')\n",
    "        ax1.grid(True)\n",
    "\n",
    "        ax2.plot(k_metrics_df['k'], k_metrics_df['silhouette'], marker='x', color='green')\n",
    "        ax2.set_xlabel('Number of Clusters (k)')\n",
    "        ax2.set_ylabel('Silhouette Score', color='green')\n",
    "        ax2.grid(True)\n",
    "\n",
    "        ax3.plot(k_metrics_df['k'], k_metrics_df['separation'], marker='^', color='red')\n",
    "        ax3.set_xlabel('Number of Clusters (k)')\n",
    "        ax3.set_ylabel('Cluster Separation', color='red')\n",
    "        ax3.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        cluster_plot_path = f\"/tmp/cluster_diagnostics_{today_full}.png\"\n",
    "        plt.savefig(cluster_plot_path)\n",
    "        plt.close()\n",
    "        \n",
    "        # Final clustering\n",
    "        kmeans = BisectingKMeans(k=optimal_k, seed=42, featuresCol=\"pca_features\")\n",
    "        model = kmeans.fit(pca_data)\n",
    "        clustered_data = model.transform(pca_data)\n",
    "\n",
    "        # Add cluster information\n",
    "        df_with_id = df.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "        clustered_data = clustered_data.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "        clustered_data = clustered_data.join(df_with_id.select(\"row_id\", \"cif_id\"), on=\"row_id\").drop(\"row_id\")\n",
    "\n",
    "        # Generate cluster profiles\n",
    "        summary = clustered_data.groupBy(\"prediction\").agg(\n",
    "            *[mean(col(c)).alias(c) for c in numeric_cols],\n",
    "            *[stddev(col(c)).alias(f\"{c}_std\") for c in numeric_cols],\n",
    "            count(\"cif_id\").alias(\"cluster_size\")\n",
    "        )\n",
    "        \n",
    "        # Convert to Pandas safely\n",
    "        cluster_data = summary.collect()\n",
    "        cluster_profiles = pd.DataFrame([row.asDict() for row in cluster_data]).set_index(\"prediction\")\n",
    "        \n",
    "        # Get interpretations\n",
    "        interpretations = interpret_clusters(cluster_profiles)\n",
    "        \n",
    "        # Add cluster interpretation to dataframe\n",
    "        interpret_udf = udf(lambda x: interpretations.get(x, \"Unknown\"), StringType())\n",
    "        clustered_data = clustered_data.withColumn(\"interpretation\", interpret_udf(col(\"prediction\")))\n",
    "\n",
    "        # Prepare final output with all relevant columns\n",
    "        output_columns = [\"cif_id\", \"prediction\", \"interpretation\"] + numeric_cols\n",
    "        final_output = clustered_data.select(\n",
    "            col(\"cif_id\").alias(\"cif_id\"),\n",
    "            col(\"prediction\").alias(\"cluster\"),\n",
    "            col(\"interpretation\").alias(\"interpretations\")\n",
    "        )\n",
    "\n",
    "        # Convert to Pandas DataFrame for CSV export\n",
    "        final_pdf = pd.DataFrame(\n",
    "            final_output.rdd.map(lambda row: row.asDict()).collect()\n",
    "        )\n",
    "\n",
    "        # Define output paths\n",
    "        output_paths = {\n",
    "            'final_output': f\"/tmp/final_segmented_customers_{current_date}.csv\",\n",
    "            'interpretations': f\"/tmp/cluster_interpretations_{current_date}.json\",\n",
    "            'plot': cluster_plot_path,\n",
    "            'model': f\"/tmp/segmentation_model_{today_full}\"\n",
    "        }\n",
    "        \n",
    "        # Save final output (only the 3 columns)\n",
    "        final_pdf.to_csv(output_paths['final_output'], index=False)\n",
    "        print(f\"Final output sample:\\n{final_pdf.head()}\")\n",
    "        \n",
    "        # Save interpretations\n",
    "        with open(output_paths['interpretations'], 'w') as f:\n",
    "            json.dump(interpretations, f, indent=2)\n",
    "        \n",
    "        # Save model\n",
    "        model.write().overwrite().save(output_paths['model'])\n",
    "        \n",
    "        # Upload to MinIO with specified folder structure\n",
    "        minio_paths = {}\n",
    "        \n",
    "        # Upload CSV files to results/csv/\n",
    "        csv_key = f\"results/csv/customer_segments_{current_date}.csv\"\n",
    "        try:\n",
    "            uploaded_path = upload_to_minio(output_paths['final_output'], minio_bucket, csv_key)\n",
    "            minio_paths['final_output'] = uploaded_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading CSV: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # Upload JSON to results/json/\n",
    "        json_key = f\"results/json/cluster_interpretations_{current_date}.json\"\n",
    "        try:\n",
    "            uploaded_path = upload_to_minio(output_paths['interpretations'], minio_bucket, json_key)\n",
    "            minio_paths['interpretations'] = uploaded_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading JSON: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # Upload plot to images/\n",
    "        plot_key = f\"images/cluster_diagnostics_{today_full}.png\"\n",
    "        try:\n",
    "            uploaded_path = upload_to_minio(output_paths['plot'], minio_bucket, plot_key)\n",
    "            minio_paths['plot'] = uploaded_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading plot: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # Upload model to models/\n",
    "        model_key_prefix = f\"models/segmentation_model_{today_full}\"\n",
    "        try:\n",
    "            for root, _, files in os.walk(output_paths['model']):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    relative_path = os.path.relpath(file_path, output_paths['model'])\n",
    "                    s3_key = f\"{model_key_prefix}/{relative_path}\"\n",
    "                    upload_to_minio(file_path, minio_bucket, s3_key)\n",
    "            minio_paths['model'] = f\"{minio_bucket}/{model_key_prefix}\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading model files: {str(e)}\")\n",
    "            minio_paths['model'] = f\"{minio_bucket}/{model_key_prefix}_partial\"\n",
    "\n",
    "        output = namedtuple('Outputs', [\n",
    "            'segmentation_path',\n",
    "            'model_path',\n",
    "            'cluster_plot_path',\n",
    "            'cluster_interpretations',\n",
    "            'final_output_path'\n",
    "        ])\n",
    "        \n",
    "        return output(\n",
    "            f\"{minio_bucket}/{csv_key}\",  # segmentation_path\n",
    "            f\"{minio_bucket}/{model_key_prefix}\",  # model_path\n",
    "            f\"{minio_bucket}/{plot_key}\",  # cluster_plot_path\n",
    "            f\"{minio_bucket}/{json_key}\",  # cluster_interpretations\n",
    "            f\"{minio_bucket}/{csv_key}\"   # final_output_path\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in feature_engineering_and_segmentation: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if spark:\n",
    "            spark.stop()\n",
    "            \n",
    "\n",
    "@component(\n",
    "    base_image='quay.io/datanature_dev/jupyternotebook:java_home14',\n",
    "    packages_to_install=[\n",
    "        'pyspark==3.5.0',\n",
    "        'pandas==2.0.3',\n",
    "        'numpy==1.24.4',\n",
    "        'boto3==1.28.57',\n",
    "        'pyarrow==12.0.1',\n",
    "        'urllib3==2.0.4'\n",
    "    ]\n",
    ")\n",
    "def save_predictions_to_trino(\n",
    "    segmentation_path: str,\n",
    "    minio_bucket: str\n",
    ") -> str:\n",
    "    import os\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col\n",
    "    from pyspark.sql.utils import AnalysisException\n",
    "    import boto3\n",
    "    from botocore.config import Config\n",
    "    from datetime import datetime\n",
    "\n",
    "    # Configuration\n",
    "    access_key_id = 'admin'\n",
    "    secret_access_key = 'dlyticaD123'\n",
    "    minio_endpoint = 'dn-minio-tenant-hl.dn-minio-tenant.svc.cluster.local:9000'\n",
    "    data_bucket = 'ai360fd-recommendation'\n",
    "    hive_metastore_uri = \"thrift://dn-hive-metastore.dn-hive-metastore.svc.cluster.local:9083\"\n",
    "    iceberg_warehouse_location = f\"s3a://{data_bucket}/data/\"\n",
    "    custom_catalog = \"iceberg_catalog\"\n",
    "    app_name = \"Customer Segmentation Loader\"\n",
    "    additional_packages = \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.spark:spark-avro_2.12:3.5.0,com.crealytics:spark-excel_2.12:0.13.7,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.hadoop:hadoop-common:3.3.4\"\n",
    "    \n",
    "    # Initialize MinIO client for file management\n",
    "    minio_client = boto3.client(\n",
    "        's3',\n",
    "        endpoint_url=\"http://192.168.80.155:32000\",\n",
    "        aws_access_key_id=\"admin\",\n",
    "        aws_secret_access_key=\"dlyticaD123\",\n",
    "        verify=False,\n",
    "        config=Config(\n",
    "            connect_timeout=30,\n",
    "            read_timeout=60,\n",
    "            retries={'max_attempts': 3}\n",
    "        )\n",
    "    )\n",
    "\n",
    "    def archive_results():\n",
    "        \"\"\"Archive the results after successful Trino upload\"\"\"\n",
    "        current_date = datetime.now().strftime('%m_%d')\n",
    "        try:\n",
    "            # Archive CSV file\n",
    "            csv_key = segmentation_path.replace(f\"{minio_bucket}/\", \"\")\n",
    "            archive_csv_key = f\"archive/customer_segments_{current_date}.csv\"\n",
    "            \n",
    "            # Copy CSV to archive\n",
    "            minio_client.copy_object(\n",
    "                Bucket=minio_bucket,\n",
    "                CopySource={'Bucket': minio_bucket, 'Key': csv_key},\n",
    "                Key=archive_csv_key\n",
    "            )\n",
    "            \n",
    "            # Delete original CSV\n",
    "            minio_client.delete_object(Bucket=minio_bucket, Key=csv_key)\n",
    "            \n",
    "            # Find and archive corresponding JSON file\n",
    "            json_prefix = csv_key.replace(\"csv/\", \"json/\").replace(\".csv\", \".json\")\n",
    "            json_objects = minio_client.list_objects_v2(Bucket=minio_bucket, Prefix=json_prefix)\n",
    "            \n",
    "            if 'Contents' in json_objects:\n",
    "                json_key = json_objects['Contents'][0]['Key']\n",
    "                archive_json_key = f\"archive/{os.path.basename(json_key)}\"\n",
    "                \n",
    "                # Copy JSON to archive\n",
    "                minio_client.copy_object(\n",
    "                    Bucket=minio_bucket,\n",
    "                    CopySource={'Bucket': minio_bucket, 'Key': json_key},\n",
    "                    Key=archive_json_key\n",
    "                )\n",
    "                \n",
    "                # Delete original JSON\n",
    "                minio_client.delete_object(Bucket=minio_bucket, Key=json_key)\n",
    "            \n",
    "            print(\"Results successfully archived\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not archive results: {str(e)}\")\n",
    "\n",
    "    # Initialize Spark Session with more compatible settings\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", minio_endpoint) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", access_key_id) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", secret_access_key) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .config(\"spark.hadoop.hive.metastore.uris\", hive_metastore_uri) \\\n",
    "        .config(\"spark.sql.warehouse.dir\", iceberg_warehouse_location) \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "        .config(f\"spark.sql.catalog.{custom_catalog}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "        .config(f\"spark.sql.catalog.{custom_catalog}.warehouse\", iceberg_warehouse_location) \\\n",
    "        .config(f\"spark.sql.catalog.{custom_catalog}.s3.endpoint\", minio_endpoint) \\\n",
    "        .config(f\"spark.sql.catalog.{custom_catalog}.s3.access-key\", access_key_id) \\\n",
    "        .config(f\"spark.sql.catalog.{custom_catalog}.s3.secret-key\", secret_access_key) \\\n",
    "        .config(f\"spark.sql.catalog.{custom_catalog}.s3.path-style-access\", \"true\") \\\n",
    "        .config(\"spark.jars.packages\", additional_packages) \\\n",
    "        .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "        .config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n",
    "        .config(\"spark.sql.legacy.createHiveTableByDefault\", \"false\") \\\n",
    "        .config(\"spark.sql.iceberg.handle-timestamp-without-timezone\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"100\") \\\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    try:\n",
    "        # Extract the object key from the segmentation_path\n",
    "        if segmentation_path.startswith(f\"{minio_bucket}/\"):\n",
    "            object_key = segmentation_path[len(minio_bucket)+1:]\n",
    "        else:\n",
    "            object_key = segmentation_path\n",
    "        \n",
    "        # Construct the S3 path for Spark to read\n",
    "        input_s3_path = f\"s3a://{minio_bucket}/{object_key}\"\n",
    "        print(f\"Reading data from: {input_s3_path}\")\n",
    "        \n",
    "        # Read the segmentation data\n",
    "        df = spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .csv(input_s3_path)\n",
    "        \n",
    "        # Print input schema for debugging\n",
    "        print(\"Input DataFrame Schema:\")\n",
    "        df.printSchema()\n",
    "        \n",
    "        # Rename columns to match our desired schema\n",
    "        column_mapping = {\n",
    "            \"cluster\": \"cluster_id\",\n",
    "            \"interpretations\": \"interpretation\"\n",
    "        }\n",
    "        \n",
    "        for old_name, new_name in column_mapping.items():\n",
    "            if old_name in df.columns:\n",
    "                df = df.withColumnRenamed(old_name, new_name)\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        required_columns = [\"cif_id\", \"cluster_id\", \"interpretation\"]\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "        \n",
    "        # Select only the required columns and cache to minimize re-computation\n",
    "        df = df.select(*required_columns).cache()\n",
    "        \n",
    "        # Define the table name\n",
    "        table_name = f\"{custom_catalog}.gold.customer_segments\"\n",
    "        \n",
    "        # Handle existing table\n",
    "        try:\n",
    "            # Check if table exists\n",
    "            print(f\"Checking if {table_name} exists\")\n",
    "            \n",
    "            tables = spark.catalog.listTables(f\"{custom_catalog}.gold\")\n",
    "            table_exists = any(t.name.lower() == \"customer_segments\" for t in tables)\n",
    "            \n",
    "            if table_exists:\n",
    "                print(f\"Table {table_name} exists, will use 'overwrite' mode\")\n",
    "                # Get sample of existing data to verify schema\n",
    "                try:\n",
    "                    existing_df = spark.table(table_name).limit(1)\n",
    "                    print(\"Existing table schema:\")\n",
    "                    existing_df.printSchema()\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not read existing table: {str(e)}\")\n",
    "            else:\n",
    "                print(f\"Table {table_name} does not exist\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error checking table existence: {str(e)}\")\n",
    "            print(\"Proceeding with assumption that table does not exist\")\n",
    "            table_exists = False\n",
    "        \n",
    "        # Write data - handle both cases (exists or not)\n",
    "        print(\"Writing data to table...\")\n",
    "        try:\n",
    "            if table_exists:\n",
    "                # Overwrite existing table\n",
    "                print(f\"Overwriting existing table {table_name}\")\n",
    "                df.writeTo(table_name).overwrite()\n",
    "            else:\n",
    "                # Create new table\n",
    "                print(f\"Creating new table {table_name}\")\n",
    "                df.writeTo(table_name).using(\"iceberg\").create()\n",
    "            \n",
    "            # Verify the data was written\n",
    "            verification_df = spark.table(table_name)\n",
    "            count = verification_df.count()\n",
    "            print(f\"Successfully wrote {count} records to {table_name}\")\n",
    "            \n",
    "            # Archive the results after successful write\n",
    "            archive_results()\n",
    "            \n",
    "            return f\"Successfully loaded {count} records to {table_name}\"\n",
    "            \n",
    "        except Exception as write_error:\n",
    "            if \"XMinioStorageFull\" in str(write_error):\n",
    "                print(\"Storage full error detected\")\n",
    "                # Handle storage full error by using compact format and reducing data size\n",
    "                print(\"Attempting to write with more compact format and reduced dataset\")\n",
    "                \n",
    "                # Sample smaller dataset as emergency fallback\n",
    "                small_df = df.sample(fraction=0.1).cache()\n",
    "                \n",
    "                # Try write with most efficient settings\n",
    "                small_df.coalesce(1).writeTo(table_name).using(\"iceberg\").option(\"write-format\", \"parquet\").option(\"compression\", \"snappy\").createOrReplace()\n",
    "                \n",
    "                warn_msg = \"WARNING: Storage was full - only wrote 10% sample of data\"\n",
    "                print(warn_msg)\n",
    "                return warn_msg\n",
    "            else:\n",
    "                # Re-raise other errors\n",
    "                raise\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error in save_predictions_to_trino: {str(e)}\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "    finally:\n",
    "        if 'spark' in locals():\n",
    "            spark.stop()\n",
    "# Pipeline definition\n",
    "@pipeline(name=\"Customer Segmentation Pipeline\")\n",
    "def customer_segmentation_pipeline(\n",
    "    minio_bucket: str = \"ai360ctzn-customer-segmentation\"\n",
    "):\n",
    "    # Step 1: Fetch data\n",
    "    fetch_task = fetch_data_trino(minio_bucket=minio_bucket)\n",
    "    fetch_task.set_caching_options(False)\n",
    "    \n",
    "    # Step 2: Process data and create segments\n",
    "    segmentation_task = feature_engineering_and_segmentation(\n",
    "        file_path=fetch_task.output,\n",
    "        minio_bucket=minio_bucket\n",
    "    )\n",
    "    segmentation_task.set_caching_options(False)\n",
    "    \n",
    "    # Step 3: Save results to Trino\n",
    "    save_to_trino = save_predictions_to_trino(\n",
    "        segmentation_path=segmentation_task.outputs['final_output_path'],\n",
    "        minio_bucket=minio_bucket\n",
    "    )\n",
    "    save_to_trino.set_caching_options(False)\n",
    "\n",
    "# Compile the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    from kfp import compiler\n",
    "    compiler.Compiler().compile(\n",
    "        customer_segmentation_pipeline,\n",
    "        \"customer_segmentation_pipeline.yaml\"\n",
    "    )\n",
    "    print(\"Pipeline compiled successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d276dab-9335-4941-8771-dd1d727403d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.parse import urlsplit, urlencode\n",
    "\n",
    "import kfp\n",
    "import requests\n",
    "import urllib3\n",
    "\n",
    "\n",
    "class KFPClientManager:\n",
    "    \"\"\"\n",
    "    A class that creates `kfp.Client` instances with Dex authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_url: str,\n",
    "        dex_username: str,\n",
    "        dex_password: str,\n",
    "        dex_auth_type: str = \"local\",\n",
    "        skip_tls_verify: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the KfpClient\"\n",
    "\n",
    "        :param api_url: the Kubeflow Pipelines API URL\n",
    "        :param skip_tls_verify: if True, skip TLS verification\n",
    "        :param dex_username: the Dex username\n",
    "        :param dex_password: the Dex password\n",
    "        :param dex_auth_type: the auth type to use if Dex has multiple enabled, one of: ['ldap', 'local']\n",
    "        \"\"\"\n",
    "        self._api_url = api_url\n",
    "        self._skip_tls_verify = skip_tls_verify\n",
    "        self._dex_username = dex_username\n",
    "        self._dex_password = dex_password\n",
    "        self._dex_auth_type = dex_auth_type\n",
    "        self._client = None\n",
    "\n",
    "        # disable SSL verification, if requested\n",
    "        if self._skip_tls_verify:\n",
    "            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "        # ensure `dex_default_auth_type` is valid\n",
    "        if self._dex_auth_type not in [\"ldap\", \"local\"]:\n",
    "            raise ValueError(\n",
    "                f\"Invalid `dex_auth_type` '{self._dex_auth_type}', must be one of: ['ldap', 'local']\"\n",
    "            )\n",
    "\n",
    "    def _get_session_cookies(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the session cookies by authenticating against Dex\n",
    "        :return: a string of session cookies in the form \"key1=value1; key2=value2\"\n",
    "        \"\"\"\n",
    "\n",
    "        # use a persistent session (for cookies)\n",
    "        s = requests.Session()\n",
    "\n",
    "        # GET the api_url, which should redirect to Dex\n",
    "        resp = s.get(\n",
    "            self._api_url, allow_redirects=True, verify=not self._skip_tls_verify\n",
    "        )\n",
    "        if resp.status_code == 200:\n",
    "            pass\n",
    "        elif resp.status_code == 403:\n",
    "            # if we get 403, we might be at the oauth2-proxy sign-in page\n",
    "            # the default path to start the sign-in flow is `/oauth2/start?rd=<url>`\n",
    "            url_obj = urlsplit(resp.url)\n",
    "            url_obj = url_obj._replace(\n",
    "                path=\"/oauth2/start\", query=urlencode({\"rd\": url_obj.path})\n",
    "            )\n",
    "            resp = s.get(\n",
    "                url_obj.geturl(), allow_redirects=True, verify=not self._skip_tls_verify\n",
    "            )\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f\"HTTP status code '{resp.status_code}' for GET against: {self._api_url}\"\n",
    "            )\n",
    "\n",
    "        # if we were NOT redirected, then the endpoint is unsecured\n",
    "        if len(resp.history) == 0:\n",
    "            # no cookies are needed\n",
    "            return \"\"\n",
    "\n",
    "        # if we are at `../auth` path, we need to select an auth type\n",
    "        url_obj = urlsplit(resp.url)\n",
    "        if re.search(r\"/auth$\", url_obj.path):\n",
    "            url_obj = url_obj._replace(\n",
    "                path=re.sub(r\"/auth$\", f\"/auth/{self._dex_auth_type}\", url_obj.path)\n",
    "            )\n",
    "\n",
    "        # if we are at `../auth/xxxx/login` path, then we are at the login page\n",
    "        if re.search(r\"/auth/.*/login$\", url_obj.path):\n",
    "            dex_login_url = url_obj.geturl()\n",
    "        else:\n",
    "            # otherwise, we need to follow a redirect to the login page\n",
    "            resp = s.get(\n",
    "                url_obj.geturl(), allow_redirects=True, verify=not self._skip_tls_verify\n",
    "            )\n",
    "            if resp.status_code != 200:\n",
    "                raise RuntimeError(\n",
    "                    f\"HTTP status code '{resp.status_code}' for GET against: {url_obj.geturl()}\"\n",
    "                )\n",
    "            dex_login_url = resp.url\n",
    "\n",
    "        # attempt Dex login\n",
    "        resp = s.post(\n",
    "            dex_login_url,\n",
    "            data={\"login\": self._dex_username, \"password\": self._dex_password},\n",
    "            allow_redirects=True,\n",
    "            verify=not self._skip_tls_verify,\n",
    "        )\n",
    "        if resp.status_code != 200:\n",
    "            raise RuntimeError(\n",
    "                f\"HTTP status code '{resp.status_code}' for POST against: {dex_login_url}\"\n",
    "            )\n",
    "\n",
    "        # if we were NOT redirected, then the login credentials were probably invalid\n",
    "        if len(resp.history) == 0:\n",
    "            raise RuntimeError(\n",
    "                f\"Login credentials are probably invalid - \"\n",
    "                f\"No redirect after POST to: {dex_login_url}\"\n",
    "            )\n",
    "\n",
    "        # if we are at `../approval` path, we need to approve the login\n",
    "        url_obj = urlsplit(resp.url)\n",
    "        if re.search(r\"/approval$\", url_obj.path):\n",
    "            dex_approval_url = url_obj.geturl()\n",
    "\n",
    "            # approve the login\n",
    "            resp = s.post(\n",
    "                dex_approval_url,\n",
    "                data={\"approval\": \"approve\"},\n",
    "                allow_redirects=True,\n",
    "                verify=not self._skip_tls_verify,\n",
    "            )\n",
    "            if resp.status_code != 200:\n",
    "                raise RuntimeError(\n",
    "                    f\"HTTP status code '{resp.status_code}' for POST against: {url_obj.geturl()}\"\n",
    "                )\n",
    "\n",
    "        return \"; \".join([f\"{c.name}={c.value}\" for c in s.cookies])\n",
    "\n",
    "    def _create_kfp_client(self) -> kfp.Client:\n",
    "        try:\n",
    "            session_cookies = self._get_session_cookies()\n",
    "        except Exception as ex:\n",
    "            raise RuntimeError(f\"Failed to get Dex session cookies\") from ex\n",
    "\n",
    "        # monkey patch the kfp.Client to support disabling SSL verification\n",
    "        # kfp only added support in v2: https://github.com/kubeflow/pipelines/pull/7174\n",
    "        original_load_config = kfp.Client._load_config\n",
    "\n",
    "        def patched_load_config(client_self, *args, **kwargs):\n",
    "            config = original_load_config(client_self, *args, **kwargs)\n",
    "            config.verify_ssl = not self._skip_tls_verify\n",
    "            return config\n",
    "\n",
    "        patched_kfp_client = kfp.Client\n",
    "        patched_kfp_client._load_config = patched_load_config\n",
    "\n",
    "        return patched_kfp_client(\n",
    "            host=self._api_url,\n",
    "            cookies=session_cookies,\n",
    "        )\n",
    "\n",
    "    def create_kfp_client(self) -> kfp.Client:\n",
    "        \"\"\"Get a newly authenticated Kubeflow Pipelines client.\"\"\"\n",
    "        return self._create_kfp_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5bf3903-d353-4101-8174-b8401e0b5f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tutorial] Data passing in python components\n",
      "[Tutorial] DSL - Control structures\n",
      "Citizen_pipeline16\n",
      "Customer_Segmentation_Pipeline\n",
      "recommendation_pipeline\n",
      "pipeline matched\n",
      "671aeb3c-cf72-442c-8dd0-80f2ce2d28cc\n",
      "Version ID: 1000f23f-0588-49ca-a3c0-12844cee5603, Name: recommendation_pipeline\n",
      "Deleted version: 1000f23f-0588-49ca-a3c0-12844cee5603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/kfp/client/client.py:159: FutureWarning: This client only works with Kubeflow Pipeline v2.0.0-beta.2 and later versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://192.168.80.155:31904/pipeline/#/pipelines/details/a311aa7f-c677-4dfc-8450-1b04cfc169a4\" target=\"_blank\" >Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://192.168.80.155:31904/pipeline/#/experiments/details/af85287c-a6f4-45e8-b8a5-3fc053065211\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://192.168.80.155:31904/pipeline/#/runs/details/7c3c15cc-1cdd-49d9-8133-5b5600b704d8\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kfp_client_manager = KFPClientManager(\n",
    "    api_url=\"http://192.168.80.155:31904/pipeline\",\n",
    "    skip_tls_verify=True,\n",
    "\n",
    "    dex_username=\"user@dlytica.com\",\n",
    "    dex_password=\"dlytica@D123#\",\n",
    "\n",
    "    # can be 'ldap' or 'local' depending on your Dex configuration\n",
    "    dex_auth_type=\"local\",\n",
    ")\n",
    "\n",
    "# get a newly authenticated KFP client\n",
    "# TIP: long-lived sessions might need to get a new client when their session expires\n",
    "kfp_client = kfp_client_manager.create_kfp_client()\n",
    "\n",
    "# # test the client by listing experiments\n",
    "# experiments = kfp_client.list_experiments(namespace=\"kubeflow-user-example-com\")\n",
    "# print(experiments)\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "pipeline_name = 'recommendation_pipeline'\n",
    "pipelines = kfp_client.list_pipelines(page_size=100)\n",
    "# experiments = kfp_client.list_experiments()\n",
    "# print(experiments)\n",
    "# print(pipelines)\n",
    "for pipeline in pipelines.pipelines:\n",
    "    print(pipeline.display_name)\n",
    "    if pipeline.display_name == pipeline_name:\n",
    "        print(\"pipeline matched\")\n",
    "        pipeline_id = pipeline.pipeline_id\n",
    "        print(pipeline_id)\n",
    "        # pipeline_id = '4758a6db-8057-463f-a185-a68f3e45d920'\n",
    "        pipeline_versions = kfp_client.list_pipeline_versions(pipeline_id=pipeline_id, page_size=100)\n",
    "        for version in pipeline_versions.pipeline_versions:\n",
    "            print(f\"Version ID: {version.pipeline_version_id}, Name: {version.display_name}\")\n",
    "        for version in pipeline_versions.pipeline_versions:\n",
    "            version_id = version.pipeline_version_id\n",
    "            try:\n",
    "                kfp_client.delete_pipeline_version(pipeline_id = pipeline_id, pipeline_version_id=version_id)  \n",
    "                print(f\"Deleted version: {version_id}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete version {version_id}: {e}\")\n",
    "        kfp_client.delete_pipeline(pipeline.pipeline_id)\n",
    "description = 'this is automl pipeline'\n",
    "kfp_client.upload_pipeline('customer_segmentation_pipeline.yaml',pipeline_name = pipeline_name, description = description)\n",
    "\n",
    "run = kfp_client.create_run_from_pipeline_func(\n",
    "    customer_segmentation_pipeline,\n",
    "    arguments={\"minio_bucket\": \"ai360ctzn-customer-segmentation\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23e9d16-49db-4ba6-9dbb-061c5fe938a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# PIPELINE DEFINITION
# Name: customer-segmentation-pipeline
# Inputs:
#    minio_bucket: str [Default: 'ai360ctzn-customer-segmentation']
components:
  comp-feature-engineering-and-segmentation:
    executorLabel: exec-feature-engineering-and-segmentation
    inputDefinitions:
      parameters:
        file_path:
          parameterType: STRING
        minio_bucket:
          parameterType: STRING
    outputDefinitions:
      parameters:
        cluster_interpretations:
          parameterType: STRING
        cluster_plot_path:
          parameterType: STRING
        final_output_path:
          parameterType: STRING
        model_path:
          parameterType: STRING
        segmentation_path:
          parameterType: STRING
  comp-fetch-data-trino:
    executorLabel: exec-fetch-data-trino
    inputDefinitions:
      parameters:
        minio_bucket:
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-save-predictions-to-trino:
    executorLabel: exec-save-predictions-to-trino
    inputDefinitions:
      parameters:
        minio_bucket:
          parameterType: STRING
        segmentation_path:
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-feature-engineering-and-segmentation:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - feature_engineering_and_segmentation
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pyspark==3.5.0'\
          \ 'pandas==2.0.3' 'numpy==1.24.4' 'boto3==1.28.57' 'scikit-learn==1.3.0'\
          \ 'matplotlib==3.7.2' 'pyarrow==12.0.1' 'urllib3==2.0.4' && \"$0\" \"$@\"\
          \n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef feature_engineering_and_segmentation(\n    file_path: str, \n\
          \    minio_bucket: str\n) -> NamedTuple('Outputs', [\n    ('segmentation_path',\
          \ str),\n    ('model_path', str),\n    ('cluster_plot_path', str),\n   \
          \ ('cluster_interpretations', str),\n    ('final_output_path', str)\n]):\n\
          \    # First ensure numpy is imported with the correct version\n    import\
          \ numpy as np\n    np.__version__  # This helps catch version issues early\n\
          \n    # Then import other packages\n    import boto3\n    from botocore.config\
          \ import Config\n    import pandas as pd\n    import matplotlib\n    matplotlib.use('Agg')\
          \  # Set non-interactive backend\n    import matplotlib.pyplot as plt\n\
          \    from datetime import datetime\n    import os\n    import json\n   \
          \ import tempfile\n    import time\n    from collections import namedtuple\n\
          \    from io import StringIO\n\n    # Now import pyspark components\n  \
          \  from pyspark.sql import SparkSession\n    from pyspark.sql.functions\
          \ import col, when, mean, monotonically_increasing_id, stddev, count, abs\n\
          \    from pyspark.sql.types import DoubleType, StringType\n    from pyspark.ml.feature\
          \ import VectorAssembler, StandardScaler, PCA\n    from pyspark.ml.clustering\
          \ import BisectingKMeans\n    from pyspark.ml.evaluation import ClusteringEvaluator\n\
          \    from pyspark.sql.functions import udf\n\n    def calculate_feature_distinctiveness(cluster_profiles):\n\
          \        \"\"\"Enhanced feature distinctiveness calculation\"\"\"\n    \
          \    feature_variance = cluster_profiles.std()\n        distinctiveness\
          \ = feature_variance / (feature_variance.max() + 1e-10)\n        return\
          \ distinctiveness\n\n    def interpret_clusters(cluster_profiles):\n   \
          \     interpretations = {}\n        distinctiveness = calculate_feature_distinctiveness(cluster_profiles)\n\
          \n        for cluster in cluster_profiles.index:\n            features =\
          \ cluster_profiles.columns.tolist()\n            profile_components = []\n\
          \n            if 'purchases' in features:\n                spend_level =\
          \ cluster_profiles.loc[cluster, 'purchases']\n                mean_spend\
          \ = cluster_profiles['purchases'].mean()\n                std_spend = cluster_profiles['purchases'].std()\n\
          \n                if spend_level > mean_spend + 1.5 * std_spend:\n     \
          \               profile_components.append(\"High Spender\")\n          \
          \      elif spend_level < mean_spend - 1.5 * std_spend:\n              \
          \      profile_components.append(\"Low Spender\")\n                else:\n\
          \                    profile_components.append(\"Moderate Spender\")\n\n\
          \            if 'cash_advance' in features and 'cash_advance_frequency'\
          \ in features:\n                cash_advance = cluster_profiles.loc[cluster,\
          \ 'cash_advance']\n                cash_freq = cluster_profiles.loc[cluster,\
          \ 'cash_advance_frequency']\n                mean_cash = cluster_profiles['cash_advance'].mean()\n\
          \                mean_freq = cluster_profiles['cash_advance_frequency'].mean()\n\
          \n                if cash_advance > mean_cash * 2 and cash_freq > mean_freq\
          \ * 2:\n                    profile_components.append(\"Intensive Cash Advance\
          \ User\")\n                elif cash_advance < mean_cash * 0.5 and cash_freq\
          \ < mean_freq * 0.5:\n                    profile_components.append(\"Rare\
          \ Cash Advance User\")\n\n            if 'balance' in features and 'credit_limit'\
          \ in features:\n                balance_ratio = cluster_profiles.loc[cluster,\
          \ 'balance'] / (cluster_profiles.loc[cluster, 'credit_limit'] + 1e-10)\n\
          \n                if balance_ratio > 0.8:\n                    profile_components.append(\"\
          Very High Credit Utilization\")\n                elif balance_ratio > 0.5:\n\
          \                    profile_components.append(\"High Credit Utilization\"\
          )\n                elif balance_ratio < 0.2:\n                    profile_components.append(\"\
          Low Credit Utilization\")\n                else:\n                    profile_components.append(\"\
          Moderate Credit Utilization\")\n\n            if 'payments' in features\
          \ and 'minimum_payments' in features:\n                payment_ratio = cluster_profiles.loc[cluster,\
          \ 'payments'] / (cluster_profiles.loc[cluster, 'minimum_payments'] + 1e-10)\n\
          \n                if payment_ratio > 3:\n                    profile_components.append(\"\
          Aggressive Overpayer\")\n                elif payment_ratio > 2:\n     \
          \               profile_components.append(\"Consistent Overpayer\")\n  \
          \              elif payment_ratio < 1.2:\n                    profile_components.append(\"\
          Minimum Payment User\")\n\n            if 'balance' in features and 'credit_limit'\
          \ in features and 'cash_advance' in features:\n                risk_score\
          \ = (\n                    cluster_profiles.loc[cluster, 'balance'] / (cluster_profiles.loc[cluster,\
          \ 'credit_limit'] + 1e-10) * 0.4 +\n                    cluster_profiles.loc[cluster,\
          \ 'cash_advance'] / (cluster_profiles.loc[cluster, 'credit_limit'] + 1e-10)\
          \ * 0.6\n                )\n\n                if risk_score > 0.7:\n   \
          \                 profile_components.append(\"Extremely High Financial Risk\"\
          )\n                elif risk_score > 0.5:\n                    profile_components.append(\"\
          High Financial Risk\")\n                elif risk_score < 0.2:\n       \
          \             profile_components.append(\"Low Financial Risk\")\n      \
          \          else:\n                    profile_components.append(\"Moderate\
          \ Financial Risk\")\n\n            if profile_components:\n            \
          \    interpretations[cluster] = \"; \".join(profile_components)\n      \
          \      else:\n                interpretations[cluster] = \"Undefined Customer\
          \ Segment\"\n\n        return interpretations\n\n    def upload_to_minio(local_path,\
          \ bucket, object_key, max_retries=5):\n        \"\"\"Uploads a file to MinIO\
          \ with improved error handling and content-length issues fixed\"\"\"\n \
          \       for attempt in range(max_retries):\n            try:\n         \
          \       # Create a fresh client for each upload attempt\n              \
          \  client = boto3.client(\n                    's3',\n                 \
          \   endpoint_url=\"http://192.168.80.155:32000\",\n                    aws_access_key_id=\"\
          admin\",\n                    aws_secret_access_key=\"dlyticaD123\",\n \
          \                   verify=False,\n                    config=Config(\n\
          \                        connect_timeout=30,\n                        read_timeout=60,\n\
          \                        retries={'max_attempts': 3}\n                 \
          \   )\n                )\n\n                # Get the file size\n      \
          \          file_size = os.path.getsize(local_path)\n\n                #\
          \ For CSV files, we'll read and write with pandas to ensure proper formatting\n\
          \                if object_key.endswith('.csv'):\n                    df\
          \ = pd.read_csv(local_path)\n                    csv_buffer = StringIO()\n\
          \                    df.to_csv(csv_buffer, index=False)\n              \
          \      client.put_object(\n                        Bucket=bucket,\n    \
          \                    Key=object_key,\n                        Body=csv_buffer.getvalue(),\n\
          \                        ContentType='text/csv'\n                    )\n\
          \                else:\n                    # For non-CSV files, use standard\
          \ upload\n                    with open(local_path, 'rb') as file_data:\n\
          \                        client.put_object(\n                          \
          \  Bucket=bucket,\n                            Key=object_key,\n       \
          \                     Body=file_data,\n                            ContentLength=file_size\n\
          \                        )\n\n                print(f\"Successfully uploaded\
          \ {local_path} to {bucket}/{object_key}\")\n                return f\"{bucket}/{object_key}\"\
          \n\n            except Exception as e:\n                if attempt < max_retries\
          \ - 1:\n                    wait_time = 2 ** attempt  # Exponential backoff\n\
          \                    print(f\"Upload attempt {attempt+1} failed: {str(e)}.\
          \ Retrying in {wait_time} seconds...\")\n                    time.sleep(wait_time)\n\
          \                else:\n                    print(f\"Upload failed after\
          \ {max_retries} attempts: {str(e)}\")\n                    raise\n\n   \
          \ # Initialize Spark with optimized settings\n    spark = SparkSession.builder\
          \ \\\n        .appName(\"AdvancedCustomerSegmentation\") \\\n        .config(\"\
          spark.driver.memory\", \"8g\") \\\n        .config(\"spark.executor.memory\"\
          , \"8g\") \\\n        .config(\"spark.sql.shuffle.partitions\", \"200\"\
          ) \\\n        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"\
          false\") \\\n        .config(\"spark.executor.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\"\
          ) \\\n        .config(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\"\
          ) \\\n        .getOrCreate()\n\n    # Initialize MinIO client with improved\
          \ configuration\n    boto_config = Config(\n        connect_timeout=30,\n\
          \        read_timeout=60,\n        retries={'max_attempts': 3}\n    )\n\n\
          \    minio_client = boto3.client(\n        's3',\n        endpoint_url=\"\
          http://192.168.80.155:32000\",\n        aws_access_key_id=\"admin\",\n \
          \       aws_secret_access_key=\"dlyticaD123\",\n        verify=False,\n\
          \        config=boto_config\n    )\n\n    try:\n        today = datetime.now().strftime('%Y%m%d_%H%M%S')\n\
          \        local_path = \"/tmp/raw_data.csv\"\n\n        # Download with retry\
          \ logic\n        max_retries = 3\n        for attempt in range(max_retries):\n\
          \            try:\n                minio_client.download_file(minio_bucket,\
          \ file_path, local_path)\n                break\n            except Exception\
          \ as e:\n                if attempt == max_retries - 1:\n              \
          \      raise\n                time.sleep(5)\n\n        # Read data with\
          \ explicit schema\n        df = spark.read \\\n            .option(\"header\"\
          , \"true\") \\\n            .option(\"inferSchema\", \"true\") \\\n    \
          \        .csv(local_path)\n\n        if 'custid' in df.columns:\n      \
          \      df = df.withColumnRenamed(\"custid\", \"cif_id\")\n\n        # Feature\
          \ engineering\n        df = df.withColumn(\"balance_utilization_ratio\"\
          , \n            when(col(\"credit_limit\") != 0, col(\"balance\") / col(\"\
          credit_limit\")).otherwise(0)\n        )\n        df = df.withColumn(\"\
          cash_advance_intensity\", \n            when(col(\"credit_limit\") != 0,\
          \ col(\"cash_advance\") / col(\"credit_limit\")).otherwise(0)\n        )\n\
          \        df = df.withColumn(\"payment_effort_ratio\", \n            when(col(\"\
          minimum_payments\") != 0, col(\"payments\") / col(\"minimum_payments\")).otherwise(0)\n\
          \        )\n        df = df.withColumn(\"purchase_diversity\", \n      \
          \      abs(col(\"oneoff_purchases\") - col(\"installments_purchases\"))\
          \ / \n            (abs(col(\"oneoff_purchases\") + col(\"installments_purchases\"\
          )) + 1e-10)\n        )\n        df = df.na.fill(0)\n\n        numeric_cols\
          \ = [\n            'purchases', 'oneoff_purchases', 'installments_purchases',\
          \ \n            'cash_advance', 'cash_advance_frequency', \n           \
          \ 'balance', 'credit_limit', \n            'payments', 'minimum_payments',\n\
          \            'balance_utilization_ratio', \n            'cash_advance_intensity',\
          \ \n            'payment_effort_ratio',\n            'purchase_diversity'\n\
          \        ]\n\n        # Feature transformation\n        assembler = VectorAssembler(inputCols=numeric_cols,\
          \ outputCol=\"features\")\n        assembled = assembler.transform(df.drop(\"\
          cif_id\"))\n        scaler = StandardScaler(inputCol=\"features\", outputCol=\"\
          scaled_features\", withMean=True, withStd=True)\n        scaled_data = scaler.fit(assembled).transform(assembled)\n\
          \n        # Dimensionality reduction\n        pca = PCA(k=5, inputCol=\"\
          scaled_features\", outputCol=\"pca_features\")\n        pca_data = pca.fit(scaled_data).transform(scaled_data)\n\
          \n        # Clustering optimization\n        evaluator = ClusteringEvaluator(featuresCol='pca_features')\n\
          \        k_range = range(6, 15)\n        k_metrics = []\n\n        for k\
          \ in k_range:\n            kmeans = BisectingKMeans(k=k, seed=42, featuresCol=\"\
          pca_features\")\n            model = kmeans.fit(pca_data)\n            predictions\
          \ = model.transform(pca_data)\n\n            cost = model.summary.trainingCost\
          \ if hasattr(model.summary, 'trainingCost') else 0\n            silhouette\
          \ = evaluator.evaluate(predictions)\n\n            centers = model.clusterCenters()\n\
          \            separation = np.mean([np.min(np.linalg.norm(centers[i] - centers[j]))\
          \ \n                                  for i in range(len(centers)) \n  \
          \                                for j in range(i+1, len(centers))])\n\n\
          \            k_metrics.append({\n                'k': k,\n             \
          \   'cost': cost,\n                'silhouette': silhouette,\n         \
          \       'separation': separation\n            })\n\n        # Determine\
          \ optimal k\n        k_metrics_df = pd.DataFrame(k_metrics)\n        k_metrics_df['normalized_cost']\
          \ = (k_metrics_df['cost'] - k_metrics_df['cost'].min()) / (k_metrics_df['cost'].max()\
          \ - k_metrics_df['cost'].min())\n        k_metrics_df['normalized_silhouette']\
          \ = (k_metrics_df['silhouette'] - k_metrics_df['silhouette'].min()) / (k_metrics_df['silhouette'].max()\
          \ - k_metrics_df['silhouette'].min())\n        k_metrics_df['normalized_separation']\
          \ = (k_metrics_df['separation'] - k_metrics_df['separation'].min()) / (k_metrics_df['separation'].max()\
          \ - k_metrics_df['separation'].min())\n\n        k_metrics_df['composite_score']\
          \ = (\n            0.3 * (1 - k_metrics_df['normalized_cost']) +\n     \
          \       0.4 * k_metrics_df['normalized_silhouette'] +\n            0.3 *\
          \ k_metrics_df['normalized_separation']\n        )\n\n        optimal_k\
          \ = k_metrics_df.loc[k_metrics_df['composite_score'].idxmax(), 'k']\n\n\
          \        # Plotting\n        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12,\
          \ 15))\n        ax1.plot(k_metrics_df['k'], k_metrics_df['cost'], marker='o',\
          \ color='blue')\n        ax1.set_xlabel('Number of Clusters (k)')\n    \
          \    ax1.set_ylabel('Cost', color='blue')\n        ax1.grid(True)\n\n  \
          \      ax2.plot(k_metrics_df['k'], k_metrics_df['silhouette'], marker='x',\
          \ color='green')\n        ax2.set_xlabel('Number of Clusters (k)')\n   \
          \     ax2.set_ylabel('Silhouette Score', color='green')\n        ax2.grid(True)\n\
          \n        ax3.plot(k_metrics_df['k'], k_metrics_df['separation'], marker='^',\
          \ color='red')\n        ax3.set_xlabel('Number of Clusters (k)')\n     \
          \   ax3.set_ylabel('Cluster Separation', color='red')\n        ax3.grid(True)\n\
          \n        plt.tight_layout()\n        cluster_plot_path = f\"/tmp/cluster_diagnostics_{today}.png\"\
          \n        plt.savefig(cluster_plot_path)\n        plt.close()\n\n      \
          \  # Final clustering\n        kmeans = BisectingKMeans(k=optimal_k, seed=42,\
          \ featuresCol=\"pca_features\")\n        model = kmeans.fit(pca_data)\n\
          \        clustered_data = model.transform(pca_data)\n\n        # Add cluster\
          \ information\n        df_with_id = df.withColumn(\"row_id\", monotonically_increasing_id())\n\
          \        clustered_data = clustered_data.withColumn(\"row_id\", monotonically_increasing_id())\n\
          \        clustered_data = clustered_data.join(df_with_id.select(\"row_id\"\
          , \"cif_id\"), on=\"row_id\").drop(\"row_id\")\n\n        # Generate cluster\
          \ profiles\n        summary = clustered_data.groupBy(\"prediction\").agg(\n\
          \            *[mean(col(c)).alias(c) for c in numeric_cols],\n         \
          \   *[stddev(col(c)).alias(f\"{c}_std\") for c in numeric_cols],\n     \
          \       count(\"cif_id\").alias(\"cluster_size\")\n        )\n\n       \
          \ # Convert to Pandas safely\n        cluster_data = summary.collect()\n\
          \        cluster_profiles = pd.DataFrame([row.asDict() for row in cluster_data]).set_index(\"\
          prediction\")\n\n        # Get interpretations\n        interpretations\
          \ = interpret_clusters(cluster_profiles)\n\n        # Add cluster interpretation\
          \ to dataframe\n        interpret_udf = udf(lambda x: interpretations.get(x,\
          \ \"Unknown\"), StringType())\n        clustered_data = clustered_data.withColumn(\"\
          interpretation\", interpret_udf(col(\"prediction\")))\n\n        # Prepare\
          \ final output with all relevant columns\n        output_columns = [\"cif_id\"\
          , \"prediction\", \"interpretation\"] + numeric_cols\n        final_output\
          \ = clustered_data.select(\n            col(\"cif_id\").alias(\"cif_id\"\
          ),\n            col(\"prediction\").alias(\"cluster\"),\n            col(\"\
          interpretation\").alias(\"interpretations\")\n        )\n\n        # Convert\
          \ to Pandas DataFrame for CSV export\n        final_pdf = pd.DataFrame(\n\
          \            final_output.rdd.map(lambda row: row.asDict()).collect()\n\
          \        )\n\n        # Get current date in YYYY-MM-DD format\n        current_date\
          \ = datetime.now().strftime('%Y-%m-%d')\n\n        # Define output paths\n\
          \        output_paths = {\n            'final_output': f\"/tmp/final_segmented_customers_{current_date}.csv\"\
          ,\n            'interpretations': f\"/tmp/cluster_interpretations_{current_date}.json\"\
          ,\n            'plot': f\"/tmp/cluster_diagnostics_{current_date}.png\"\
          ,\n            'model': f\"/tmp/segmentation_model_{current_date}\"\n  \
          \      }\n\n        # Save final output (only the 3 columns)\n        final_pdf.to_csv(output_paths['final_output'],\
          \ index=False)\n        print(f\"Final output sample:\\n{final_pdf.head()}\"\
          )\n\n        # Save interpretations\n        with open(output_paths['interpretations'],\
          \ 'w') as f:\n            json.dump(interpretations, f, indent=2)\n\n  \
          \      # Save plot\n        plt.savefig(output_paths['plot'])\n        plt.close()\n\
          \n        # Save model\n        model.write().overwrite().save(output_paths['model'])\n\
          \n        # Upload to MinIO with specified folder structure\n        minio_paths\
          \ = {}\n\n        # Upload CSV files to results/\n        csv_files = {\n\
          \            'final_output': f\"results/final_segmented_customers_{current_date}.csv\"\
          ,\n            'interpretations': f\"results/cluster_interpretations_{current_date}.json\"\
          \n        }\n\n        for name, minio_key in csv_files.items():\n     \
          \       try:\n                uploaded_path = upload_to_minio(output_paths[name],\
          \ minio_bucket, minio_key)\n                minio_paths[name] = uploaded_path\n\
          \            except Exception as e:\n                print(f\"Error uploading\
          \ {name}: {str(e)}\")\n                raise\n\n        # Upload plot to\
          \ images/\n        plot_key = f\"images/cluster_diagnostics_{current_date}.png\"\
          \n        try:\n            uploaded_path = upload_to_minio(output_paths['plot'],\
          \ minio_bucket, plot_key)\n            minio_paths['plot'] = uploaded_path\n\
          \        except Exception as e:\n            print(f\"Error uploading plot:\
          \ {str(e)}\")\n            raise\n\n        # Upload model to models/\n\
          \        model_key_prefix = f\"models/segmentation_model_{current_date}\"\
          \n        try:\n            for root, _, files in os.walk(output_paths['model']):\n\
          \                for file in files:\n                    file_path = os.path.join(root,\
          \ file)\n                    relative_path = os.path.relpath(file_path,\
          \ output_paths['model'])\n                    s3_key = f\"{model_key_prefix}/{relative_path}\"\
          \n                    upload_to_minio(file_path, minio_bucket, s3_key)\n\
          \            minio_paths['model'] = f\"{minio_bucket}/{model_key_prefix}\"\
          \n        except Exception as e:\n            print(f\"Error uploading model\
          \ files: {str(e)}\")\n            minio_paths['model'] = f\"{minio_bucket}/{model_key_prefix}_partial\"\
          \n\n        output = namedtuple('Outputs', [\n            'segmentation_path',\n\
          \            'model_path',\n            'cluster_plot_path',\n         \
          \   'cluster_interpretations',\n            'final_output_path'\n      \
          \  ])\n\n        return output(\n            f\"{minio_bucket}/{csv_files['final_output']}\"\
          ,  # segmentation_path\n            f\"{minio_bucket}/{model_key_prefix}\"\
          ,           # model_path\n            f\"{minio_bucket}/{plot_key}\",  \
          \                 # cluster_plot_path\n            f\"{minio_bucket}/{csv_files['interpretations']}\"\
          ,  # cluster_interpretations\n            f\"{minio_bucket}/{csv_files['final_output']}\"\
          \   # final_output_path\n        )\n\n    except Exception as e:\n     \
          \   print(f\"Error in feature_engineering_and_segmentation: {str(e)}\")\n\
          \        raise\n    finally:\n        if spark:\n            spark.stop()\n\
          \n"
        image: quay.io/datanature_dev/jupyternotebook:java_home14
    exec-fetch-data-trino:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - fetch_data_trino
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'trino' 'pandas'\
          \ 'pyarrow' 's3fs' 'boto3' 'urllib3' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef fetch_data_trino(minio_bucket: str) -> str:\n    import trino\n\
          \    from trino.auth import BasicAuthentication\n    import csv\n    import\
          \ boto3\n    import os\n    import urllib3\n    from datetime import datetime\n\
          \n    # Disable SSL warnings\n    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\
          \n    # Trino Connection Details\n    TRINO_HOST = \"192.168.80.155\"\n\
          \    TRINO_PORT = \"30071\"\n    TRINO_USER = \"ctzn.bank\"\n    TRINO_PASSWORD\
          \ = \"ctzn.bank_123\"\n    TRINO_CATALOG = \"iceberg\"\n    TRINO_SCHEMA\
          \ = \"gold\"\n    TRINO_HTTP_SCHEME = \"https\"\n\n    # Output file\n \
          \   OUTPUT_FILE = \"/tmp/ctzn_10_years_data.csv\"\n\n    # SQL Query\n \
          \   SQL_QUERY = \"\"\"\n    WITH recent_customers AS (\n        SELECT DISTINCT\
          \ g.cif_id\n        FROM gold.dim_gam AS g\n        WHERE CAST(DATE_PARSE(SUBSTRING(g.acct_opn_date,\
          \ 1, 19), '%Y-%m-%dT%H:%i:%s') AS DATE) >= CURRENT_DATE - INTERVAL '10'\
          \ YEAR\n    ),\n    account_activity AS (\n        SELECT \n           \
          \ a.cif_id,\n            SUM(COALESCE(a.total_credit_tran_vol, 0) - COALESCE(a.total_debit_tran_vol,\
          \ 0)) AS balance,\n            COUNT(DISTINCT a.nepali_month)/6.0 AS balance_frequency,\n\
          \            SUM(COALESCE(a.total_debit_tran_vol, 0)) AS purchases,\n  \
          \          MAX(COALESCE(a.total_debit_tran_vol, 0)) AS oneoff_purchases,\n\
          \            SUM(COALESCE(a.total_debit_tran_vol, 0)) - MAX(COALESCE(a.total_debit_tran_vol,\
          \ 0)) AS installments_purchases,\n            SUM(CASE WHEN COALESCE(a.total_credit_tran_vol,\
          \ 0) > 0 AND COALESCE(a.total_debit_tran_vol, 0) = 0 \n                \
          \    THEN COALESCE(a.total_credit_tran_vol, 0) ELSE 0 END) AS cash_advance,\n\
          \            COUNT(DISTINCT a.foracid)/6.0 AS purchases_frequency,\n   \
          \         COUNT(DISTINCT CASE WHEN COALESCE(a.total_debit_tran_vol, 0) >\
          \ 100000 THEN a.foracid END)/6.0 AS oneoff_purchases_frequency,\n      \
          \      COUNT(DISTINCT CASE WHEN COALESCE(a.total_debit_tran_vol, 0) BETWEEN\
          \ 1 AND 100000 THEN a.foracid END)/6.0 AS purchases_installments_frequency,\n\
          \            COUNT(DISTINCT CASE WHEN COALESCE(a.total_credit_tran_vol,\
          \ 0) > 0 THEN a.foracid END)/6.0 AS cash_advance_frequency,\n          \
          \  COUNT(DISTINCT CASE WHEN COALESCE(a.total_credit_tran_vol, 0) > 0 THEN\
          \ a.foracid END) AS cash_advance_trx,\n            COUNT(DISTINCT a.foracid)\
          \ AS purchases_trx,\n            SUM(COALESCE(a.total_credit_tran_vol, 0))\
          \ AS payments,\n            COUNT(DISTINCT CASE WHEN COALESCE(a.total_credit_tran_vol,\
          \ 0) >= COALESCE(a.total_debit_tran_vol, 0) \n                         \
          \       THEN a.nepali_month END)/6.0 AS prc_full_payment\n        FROM gold.mv_fact_deposit_account_insights\
          \ a\n        JOIN recent_customers rc ON a.cif_id = rc.cif_id\n        GROUP\
          \ BY a.cif_id\n    ),\n    salary_stats AS (\n        SELECT \n        \
          \    APPROX_PERCENTILE(COALESCE(salary_per_month, 0), 0.5) AS median_salary,\n\
          \            APPROX_PERCENTILE(COALESCE(salary_per_month, 0), 0.05) AS fifth_percentile_salary\n\
          \        FROM gold.dim_customers\n    ),\n    customer_profile AS (\n  \
          \      SELECT \n            g.cif_id,\n            DATE_DIFF('year', \n\
          \                     CAST(DATE_PARSE(SUBSTRING(MIN(g.acct_opn_date), 1,\
          \ 19), '%Y-%m-%dT%H:%i:%s') AS DATE), \n                     CURRENT_DATE)\
          \ AS tenure,\n            (SELECT fifth_percentile_salary FROM salary_stats)\
          \ AS minimum_payments\n        FROM gold.dim_gam g\n        LEFT JOIN gold.dim_customers\
          \ c ON g.cif_id = c.cif_id\n        GROUP BY g.cif_id\n    )\n    SELECT\
          \ \n        aa.cif_id AS custid,\n        aa.balance,\n        aa.balance_frequency,\n\
          \        aa.purchases,\n        aa.oneoff_purchases,\n        aa.installments_purchases,\n\
          \        aa.cash_advance,\n        aa.purchases_frequency,\n        aa.oneoff_purchases_frequency,\n\
          \        aa.purchases_installments_frequency,\n        aa.cash_advance_frequency,\n\
          \        aa.cash_advance_trx,\n        aa.purchases_trx,\n        (SELECT\
          \ median_salary * 3 FROM salary_stats) AS credit_limit,\n        aa.payments,\n\
          \        cp.minimum_payments,\n        aa.prc_full_payment,\n        cp.tenure\n\
          \    FROM account_activity aa\n    JOIN customer_profile cp ON aa.cif_id\
          \ = cp.cif_id\n    ORDER BY aa.cif_id\n    \"\"\"\n\n    try:\n        #\
          \ Connect to Trino\n        conn = trino.dbapi.connect(\n            host=TRINO_HOST,\n\
          \            port=TRINO_PORT,\n            user=TRINO_USER,\n          \
          \  auth=BasicAuthentication(TRINO_USER, TRINO_PASSWORD),\n            catalog=TRINO_CATALOG,\n\
          \            schema=TRINO_SCHEMA,\n            http_scheme=TRINO_HTTP_SCHEME,\n\
          \            request_timeout=600,\n            verify=False\n        )\n\
          \        cursor = conn.cursor()\n\n        # Execute query and fetch data\n\
          \        cursor.execute(SQL_QUERY)\n        columns = [desc[0] for desc\
          \ in cursor.description]\n\n        # Write to CSV in batches\n        with\
          \ open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as f:\n         \
          \   writer = csv.writer(f)\n            writer.writerow(columns)\n\n   \
          \         while True:\n                rows = cursor.fetchmany(1000)\n \
          \               if not rows:\n                    break\n              \
          \  writer.writerows(rows)\n\n        # Upload to MinIO\n        minio_client\
          \ = boto3.client(\n            's3',\n            endpoint_url=\"http://192.168.80.155:32000\"\
          ,\n            aws_access_key_id=\"admin\",\n            aws_secret_access_key=\"\
          dlyticaD123\",\n            verify=False\n        )\n\n        current_date\
          \ = datetime.now().strftime(\"%Y-%m-%d\")\n        minio_path = f\"data/customer_segmentation_raw_{current_date}.csv\"\
          \n        minio_client.upload_file(OUTPUT_FILE, minio_bucket, minio_path)\n\
          \n        return minio_path\n\n    except Exception as e:\n        print(f\"\
          Error in fetch_data_trino: {str(e)}\")\n        raise\n    finally:\n  \
          \      if 'cursor' in locals():\n            cursor.close()\n        if\
          \ 'conn' in locals():\n            conn.close()\n\n"
        image: bitnami/spark:3.5
    exec-save-predictions-to-trino:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - save_predictions_to_trino
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pyspark==3.5.0'\
          \ 'pandas==2.0.3' 'numpy==1.24.4' 'boto3==1.28.57' 'pyarrow==12.0.1' 'urllib3==2.0.4'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef save_predictions_to_trino(\n    segmentation_path: str,\n   \
          \ minio_bucket: str\n) -> str:\n    import os\n    from pyspark.sql import\
          \ SparkSession\n    from pyspark.sql.functions import col, lit\n    from\
          \ pyspark.sql.utils import AnalysisException\n    import time\n\n    # Configuration\n\
          \    access_key_id = 'admin'\n    secret_access_key = 'dlyticaD123'\n  \
          \  minio_endpoint = 'dn-minio-tenant-hl.dn-minio-tenant.svc.cluster.local:9000'\n\
          \    data_bucket = 'ai360fd-recommendation'\n    hive_metastore_uri = \"\
          thrift://dn-hive-metastore.dn-hive-metastore.svc.cluster.local:9083\"\n\
          \    iceberg_warehouse_location = f\"s3a://{data_bucket}/data/\"\n    custom_catalog\
          \ = \"iceberg_catalog\"\n    app_name = \"Customer Segmentation Loader\"\
          \n    additional_packages = \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.spark:spark-avro_2.12:3.5.0,com.crealytics:spark-excel_2.12:0.13.7,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.hadoop:hadoop-common:3.3.4\"\
          \n\n    # Initialize Spark Session with optimized Iceberg settings\n   \
          \ spark = SparkSession.builder \\\n        .appName(app_name) \\\n     \
          \   .config(\"spark.driver.memory\", \"8g\") \\\n        .config(\"spark.executor.memory\"\
          , \"8g\") \\\n        .config(\"spark.executor.memoryOverhead\", \"2g\"\
          ) \\\n        .config(\"spark.driver.maxResultSize\", \"2g\") \\\n     \
          \   .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\"\
          ) \\\n        .config(\"spark.hadoop.fs.s3a.endpoint\", minio_endpoint)\
          \ \\\n        .config(\"spark.hadoop.fs.s3a.access.key\", access_key_id)\
          \ \\\n        .config(\"spark.hadoop.fs.s3a.secret.key\", secret_access_key)\
          \ \\\n        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\"\
          ) \\\n        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"\
          false\") \\\n        .config(\"spark.hadoop.hive.metastore.uris\", hive_metastore_uri)\
          \ \\\n        .config(\"spark.sql.warehouse.dir\", iceberg_warehouse_location)\
          \ \\\n        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\"\
          ) \\\n        .config(f\"spark.sql.catalog.{custom_catalog}\", \"org.apache.iceberg.spark.SparkCatalog\"\
          ) \\\n        .config(f\"spark.sql.catalog.{custom_catalog}.warehouse\"\
          , iceberg_warehouse_location) \\\n        .config(f\"spark.sql.catalog.{custom_catalog}.s3.endpoint\"\
          , minio_endpoint) \\\n        .config(f\"spark.sql.catalog.{custom_catalog}.s3.access-key\"\
          , access_key_id) \\\n        .config(f\"spark.sql.catalog.{custom_catalog}.s3.secret-key\"\
          , secret_access_key) \\\n        .config(f\"spark.sql.catalog.{custom_catalog}.s3.path-style-access\"\
          , \"true\") \\\n        .config(\"spark.jars.packages\", additional_packages)\
          \ \\\n        .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n \
          \       .config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n      \
          \  .config(\"spark.sql.legacy.createHiveTableByDefault\", \"false\") \\\n\
          \        .config(\"spark.sql.iceberg.handle-timestamp-without-timezone\"\
          , \"true\") \\\n        .config(\"spark.sql.adaptive.enabled\", \"true\"\
          ) \\\n        .config(\"spark.sql.adaptive.coalescePartitions.enabled\"\
          , \"true\") \\\n        .config(\"spark.sql.shuffle.partitions\", \"100\"\
          ) \\\n        .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"100\"\
          ) \\\n        .config(\"spark.sql.parquet.compression.codec\", \"snappy\"\
          ) \\\n        .config(\"spark.sql.iceberg.optimize.enabled\", \"true\")\
          \ \\\n        .config(\"spark.sql.iceberg.optimize.rewrite.delete.enabled\"\
          , \"true\") \\\n        .config(\"spark.sql.iceberg.optimize.rewrite.data.enabled\"\
          , \"true\") \\\n        .config(\"spark.sql.iceberg.vectorization.enabled\"\
          , \"true\") \\\n        .enableHiveSupport() \\\n        .getOrCreate()\n\
          \n    try:\n        # Extract the object key from the segmentation_path\n\
          \        if segmentation_path.startswith(f\"{minio_bucket}/\"):\n      \
          \      object_key = segmentation_path[len(minio_bucket)+1:]\n        else:\n\
          \            object_key = segmentation_path\n\n        # Construct the S3\
          \ path for Spark to read\n        input_s3_path = f\"s3a://{minio_bucket}/{object_key}\"\
          \n        print(f\"Reading data from: {input_s3_path}\")\n\n        # Read\
          \ the segmentation data with explicit schema\n        df = spark.read \\\
          \n            .option(\"header\", \"true\") \\\n            .option(\"inferSchema\"\
          , \"true\") \\\n            .csv(input_s3_path)\n\n        # Print input\
          \ schema for debugging\n        print(\"Input DataFrame Schema:\")\n   \
          \     df.printSchema()\n\n        # Rename columns to match our desired\
          \ schema\n        column_mapping = {\n            \"cluster\": \"cluster_id\"\
          ,\n            \"interpretations\": \"interpretation\"\n        }\n\n  \
          \      for old_name, new_name in column_mapping.items():\n            if\
          \ old_name in df.columns:\n                df = df.withColumnRenamed(old_name,\
          \ new_name)\n\n        # Ensure required columns exist\n        required_columns\
          \ = [\"cif_id\", \"cluster_id\", \"interpretation\"]\n        missing_columns\
          \ = [col for col in required_columns if col not in df.columns]\n       \
          \ if missing_columns:\n            raise ValueError(f\"Missing required\
          \ columns: {missing_columns}\")\n\n        # Select only the required columns\n\
          \        df = df.select(*required_columns)\n\n        # Add processing timestamp\n\
          \        df = df.withColumn(\"processing_timestamp\", lit(time.strftime('%Y-%m-%d\
          \ %H:%M:%S')))\n\n        # Count records before writing\n        row_count\
          \ = df.count()\n        print(f\"Preparing to write {row_count:,} records\
          \ to Trino\")\n\n        # Define the table name\n        table_name = f\"\
          {custom_catalog}.gold.customer_segments\"\n\n        # Check if table exists\n\
          \        table_exists = spark.catalog.tableExists(table_name)\n\n      \
          \  # Configure write properties\n        write_properties = {\n        \
          \    \"write.format.default\": \"parquet\",\n            \"write.parquet.compression-codec\"\
          : \"zstd\",\n            \"write.parquet.row-group-size-bytes\": \"8388608\"\
          ,  # 8MB\n            \"write.metadata.delete-after-commit.enabled\": \"\
          true\",\n            \"write.metadata.previous-versions-max\": \"3\",\n\
          \            \"write.parquet.dict-size-bytes\": \"1048576\",  # 1MB\n  \
          \          \"write.spark.fanout.enabled\": \"true\"\n        }\n\n     \
          \   # Write data with optimized settings\n        if table_exists:\n   \
          \         print(f\"Table {table_name} exists - appending with overwrite\"\
          )\n\n            # For large updates, it's better to create a temp table\
          \ and then replace\n            temp_table_name = f\"{custom_catalog}.gold.customer_segments_temp_{int(time.time())}\"\
          \n\n            print(f\"Creating temporary table {temp_table_name}\")\n\
          \            df.writeTo(temp_table_name) \\\n                .using(\"iceberg\"\
          ) \\\n                .partitionedBy(\"cluster_id\") \\\n              \
          \  .tableProperty(\"format-version\", \"2\") \\\n                .tableProperty(\"\
          write.parquet.compression-codec\", \"zstd\") \\\n                .create()\n\
          \n            print(\"Replacing main table with temporary table\")\n   \
          \         spark.sql(f\"ALTER TABLE {table_name} REPLACE AS SELECT * FROM\
          \ {temp_table_name}\")\n\n            print(\"Dropping temporary table\"\
          )\n            spark.sql(f\"DROP TABLE {temp_table_name}\")\n        else:\n\
          \            print(f\"Creating new table {table_name}\")\n            df.writeTo(table_name)\
          \ \\\n                .using(\"iceberg\") \\\n                .partitionedBy(\"\
          cluster_id\") \\\n                .tableProperty(\"format-version\", \"\
          2\") \\\n                .tableProperty(\"write.parquet.compression-codec\"\
          , \"zstd\") \\\n                .create()\n\n        # Verify the data was\
          \ written\n        verification_df = spark.table(table_name)\n        final_count\
          \ = verification_df.count()\n        print(f\"Successfully wrote {final_count:,}\
          \ records to {table_name}\")\n\n        # Optimize the table after write\n\
          \        print(\"Optimizing table layout...\")\n        spark.sql(f\"CALL\
          \ {custom_catalog}.system.rewrite_data_files(table => '{table_name}')\"\
          )\n\n        return f\"Successfully loaded {final_count:,} records to {table_name}\"\
          \n\n    except Exception as e:\n        print(f\"Error in save_predictions_to_trino:\
          \ {str(e)}\")\n        return f\"Error: {str(e)}\"\n    finally:\n     \
          \   if 'spark' in locals():\n            spark.stop()\n\n"
        image: quay.io/datanature_dev/jupyternotebook:java_home14
pipelineInfo:
  name: customer-segmentation-pipeline
root:
  dag:
    tasks:
      feature-engineering-and-segmentation:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-feature-engineering-and-segmentation
        dependentTasks:
        - fetch-data-trino
        inputs:
          parameters:
            file_path:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: fetch-data-trino
            minio_bucket:
              componentInputParameter: minio_bucket
        taskInfo:
          name: feature-engineering-and-segmentation
      fetch-data-trino:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-fetch-data-trino
        inputs:
          parameters:
            minio_bucket:
              componentInputParameter: minio_bucket
        taskInfo:
          name: fetch-data-trino
      save-predictions-to-trino:
        cachingOptions: {}
        componentRef:
          name: comp-save-predictions-to-trino
        dependentTasks:
        - feature-engineering-and-segmentation
        inputs:
          parameters:
            minio_bucket:
              componentInputParameter: minio_bucket
            segmentation_path:
              taskOutputParameter:
                outputParameterKey: final_output_path
                producerTask: feature-engineering-and-segmentation
        taskInfo:
          name: save-predictions-to-trino
  inputDefinitions:
    parameters:
      minio_bucket:
        defaultValue: ai360ctzn-customer-segmentation
        isOptional: true
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.12.1

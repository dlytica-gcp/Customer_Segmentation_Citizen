{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d779c973-7615-4fd7-9e78-94b3d8c658fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp.dsl import pipeline, component\n",
    "from datetime import datetime\n",
    "from typing import NamedTuple\n",
    "\n",
    "# Step 1: Fetch data from Trino and save it to MinIO\n",
    "@component(\n",
    "    base_image='bitnami/spark:3.5', \n",
    "    packages_to_install=['trino', 'pandas', 'pyarrow', 's3fs', 'boto3', 'urllib3']\n",
    ")\n",
    "def fetch_data_trino(minio_bucket: str) -> str:\n",
    "    import trino\n",
    "    from trino.auth import BasicAuthentication\n",
    "    import csv\n",
    "    import boto3\n",
    "    import os\n",
    "    import urllib3\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Disable SSL warnings\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "    # Trino Connection Details\n",
    "    TRINO_HOST = \"192.168.80.155\"\n",
    "    TRINO_PORT = \"30071\"\n",
    "    TRINO_USER = \"ctzn.bank\"\n",
    "    TRINO_PASSWORD = \"ctzn.bank_123\"\n",
    "    TRINO_CATALOG = \"iceberg\"\n",
    "    TRINO_SCHEMA = \"gold\"\n",
    "    TRINO_HTTP_SCHEME = \"https\"\n",
    "\n",
    "    # Output file\n",
    "    OUTPUT_FILE = \"/tmp/ctzn_10_years_data.csv\"\n",
    "\n",
    "    # SQL Query\n",
    "    SQL_QUERY = \"\"\"\n",
    "    WITH recent_customers AS (\n",
    "        SELECT DISTINCT g.cif_id\n",
    "        FROM gold.dim_gam AS g\n",
    "        WHERE CAST(DATE_PARSE(SUBSTRING(g.acct_opn_date, 1, 19), '%Y-%m-%dT%H:%i:%s') AS DATE) >= CURRENT_DATE - INTERVAL '10' YEAR\n",
    "    ),\n",
    "    account_activity AS (\n",
    "        SELECT \n",
    "            a.cif_id,\n",
    "            SUM(COALESCE(a.total_credit_tran_vol, 0) - COALESCE(a.total_debit_tran_vol, 0)) AS balance,\n",
    "            COUNT(DISTINCT a.nepali_month)/6.0 AS balance_frequency,\n",
    "            SUM(COALESCE(a.total_debit_tran_vol, 0)) AS purchases,\n",
    "            MAX(COALESCE(a.total_debit_tran_vol, 0)) AS oneoff_purchases,\n",
    "            SUM(COALESCE(a.total_debit_tran_vol, 0)) - MAX(COALESCE(a.total_debit_tran_vol, 0)) AS installments_purchases,\n",
    "            SUM(CASE WHEN COALESCE(a.total_credit_tran_vol, 0) > 0 AND COALESCE(a.total_debit_tran_vol, 0) = 0 \n",
    "                    THEN COALESCE(a.total_credit_tran_vol, 0) ELSE 0 END) AS cash_advance,\n",
    "            COUNT(DISTINCT a.foracid)/6.0 AS purchases_frequency,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_debit_tran_vol, 0) > 100000 THEN a.foracid END)/6.0 AS oneoff_purchases_frequency,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_debit_tran_vol, 0) BETWEEN 1 AND 100000 THEN a.foracid END)/6.0 AS purchases_installments_frequency,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_credit_tran_vol, 0) > 0 THEN a.foracid END)/6.0 AS cash_advance_frequency,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_credit_tran_vol, 0) > 0 THEN a.foracid END) AS cash_advance_trx,\n",
    "            COUNT(DISTINCT a.foracid) AS purchases_trx,\n",
    "            SUM(COALESCE(a.total_credit_tran_vol, 0)) AS payments,\n",
    "            COUNT(DISTINCT CASE WHEN COALESCE(a.total_credit_tran_vol, 0) >= COALESCE(a.total_debit_tran_vol, 0) \n",
    "                                THEN a.nepali_month END)/6.0 AS prc_full_payment\n",
    "        FROM gold.mv_fact_deposit_account_insights a\n",
    "        JOIN recent_customers rc ON a.cif_id = rc.cif_id\n",
    "        GROUP BY a.cif_id\n",
    "    ),\n",
    "    salary_stats AS (\n",
    "        SELECT \n",
    "            APPROX_PERCENTILE(COALESCE(salary_per_month, 0), 0.5) AS median_salary,\n",
    "            APPROX_PERCENTILE(COALESCE(salary_per_month, 0), 0.05) AS fifth_percentile_salary\n",
    "        FROM gold.dim_customers\n",
    "    ),\n",
    "    customer_profile AS (\n",
    "        SELECT \n",
    "            g.cif_id,\n",
    "            DATE_DIFF('year', \n",
    "                     CAST(DATE_PARSE(SUBSTRING(MIN(g.acct_opn_date), 1, 19), '%Y-%m-%dT%H:%i:%s') AS DATE), \n",
    "                     CURRENT_DATE) AS tenure,\n",
    "            (SELECT fifth_percentile_salary FROM salary_stats) AS minimum_payments\n",
    "        FROM gold.dim_gam g\n",
    "        LEFT JOIN gold.dim_customers c ON g.cif_id = c.cif_id\n",
    "        GROUP BY g.cif_id\n",
    "    )\n",
    "    SELECT \n",
    "        aa.cif_id AS custid,\n",
    "        aa.balance,\n",
    "        aa.balance_frequency,\n",
    "        aa.purchases,\n",
    "        aa.oneoff_purchases,\n",
    "        aa.installments_purchases,\n",
    "        aa.cash_advance,\n",
    "        aa.purchases_frequency,\n",
    "        aa.oneoff_purchases_frequency,\n",
    "        aa.purchases_installments_frequency,\n",
    "        aa.cash_advance_frequency,\n",
    "        aa.cash_advance_trx,\n",
    "        aa.purchases_trx,\n",
    "        (SELECT median_salary * 3 FROM salary_stats) AS credit_limit,\n",
    "        aa.payments,\n",
    "        cp.minimum_payments,\n",
    "        aa.prc_full_payment,\n",
    "        cp.tenure\n",
    "    FROM account_activity aa\n",
    "    JOIN customer_profile cp ON aa.cif_id = cp.cif_id\n",
    "    ORDER BY aa.cif_id\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Connect to Trino\n",
    "        conn = trino.dbapi.connect(\n",
    "            host=TRINO_HOST,\n",
    "            port=TRINO_PORT,\n",
    "            user=TRINO_USER,\n",
    "            auth=BasicAuthentication(TRINO_USER, TRINO_PASSWORD),\n",
    "            catalog=TRINO_CATALOG,\n",
    "            schema=TRINO_SCHEMA,\n",
    "            http_scheme=TRINO_HTTP_SCHEME,\n",
    "            request_timeout=600,\n",
    "            verify=False\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Execute query and fetch data\n",
    "        cursor.execute(SQL_QUERY)\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        \n",
    "        # Write to CSV in batches\n",
    "        with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(columns)\n",
    "            \n",
    "            while True:\n",
    "                rows = cursor.fetchmany(1000)\n",
    "                if not rows:\n",
    "                    break\n",
    "                writer.writerows(rows)\n",
    "        \n",
    "        # Upload to MinIO\n",
    "        minio_client = boto3.client(\n",
    "            's3',\n",
    "            endpoint_url=\"http://192.168.80.155:32000\",\n",
    "            aws_access_key_id=\"admin\",\n",
    "            aws_secret_access_key=\"dlyticaD123\",\n",
    "            verify=False\n",
    "        )\n",
    "        \n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        minio_path = f\"data/customer_segmentation_raw_{current_date}.csv\"\n",
    "        minio_client.upload_file(OUTPUT_FILE, minio_bucket, minio_path)\n",
    "        \n",
    "        return minio_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in fetch_data_trino: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "@component(\n",
    "    base_image='quay.io/datanature_dev/jupyternotebook:java_home14',\n",
    "    packages_to_install=[\n",
    "        'pyspark==3.5.0',\n",
    "        'pandas==2.0.3',\n",
    "        'numpy==1.24.4',\n",
    "        'boto3==1.28.57',\n",
    "        'scikit-learn==1.3.0',\n",
    "        'matplotlib==3.7.2',\n",
    "        'pyarrow==12.0.1',\n",
    "        'urllib3==2.0.4'\n",
    "    ]\n",
    ")\n",
    "def feature_engineering_and_segmentation(\n",
    "    file_path: str, \n",
    "    minio_bucket: str\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('segmentation_path', str),\n",
    "    ('model_path', str),\n",
    "    ('cluster_plot_path', str),\n",
    "    ('cluster_interpretations', str),\n",
    "    ('final_output_path', str)\n",
    "]):\n",
    "    # First ensure numpy is imported with the correct version\n",
    "    import numpy as np\n",
    "    np.__version__  # This helps catch version issues early\n",
    "    \n",
    "    # Then import other packages\n",
    "    import boto3\n",
    "    from botocore.config import Config\n",
    "    import pandas as pd\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')  # Set non-interactive backend\n",
    "    import matplotlib.pyplot as plt\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "    import json\n",
    "    import tempfile\n",
    "    import time\n",
    "    from collections import namedtuple\n",
    "    from io import StringIO\n",
    "    \n",
    "    # Now import pyspark components\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col, when, mean, monotonically_increasing_id, stddev, count, abs\n",
    "    from pyspark.sql.types import DoubleType, StringType\n",
    "    from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\n",
    "    from pyspark.ml.clustering import BisectingKMeans\n",
    "    from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "    from pyspark.sql.functions import udf\n",
    "\n",
    "    def calculate_feature_distinctiveness(cluster_profiles):\n",
    "        \"\"\"Enhanced feature distinctiveness calculation\"\"\"\n",
    "        feature_variance = cluster_profiles.std()\n",
    "        distinctiveness = feature_variance / (feature_variance.max() + 1e-10)\n",
    "        return distinctiveness\n",
    "\n",
    "    def interpret_clusters(cluster_profiles):\n",
    "        interpretations = {}\n",
    "        distinctiveness = calculate_feature_distinctiveness(cluster_profiles)\n",
    "        \n",
    "        for cluster in cluster_profiles.index:\n",
    "            features = cluster_profiles.columns.tolist()\n",
    "            profile_components = []\n",
    "            \n",
    "            if 'purchases' in features:\n",
    "                spend_level = cluster_profiles.loc[cluster, 'purchases']\n",
    "                mean_spend = cluster_profiles['purchases'].mean()\n",
    "                std_spend = cluster_profiles['purchases'].std()\n",
    "                \n",
    "                if spend_level > mean_spend + 1.5 * std_spend:\n",
    "                    profile_components.append(\"High Spender\")\n",
    "                elif spend_level < mean_spend - 1.5 * std_spend:\n",
    "                    profile_components.append(\"Low Spender\")\n",
    "                else:\n",
    "                    profile_components.append(\"Moderate Spender\")\n",
    "            \n",
    "            if 'cash_advance' in features and 'cash_advance_frequency' in features:\n",
    "                cash_advance = cluster_profiles.loc[cluster, 'cash_advance']\n",
    "                cash_freq = cluster_profiles.loc[cluster, 'cash_advance_frequency']\n",
    "                mean_cash = cluster_profiles['cash_advance'].mean()\n",
    "                mean_freq = cluster_profiles['cash_advance_frequency'].mean()\n",
    "                \n",
    "                if cash_advance > mean_cash * 2 and cash_freq > mean_freq * 2:\n",
    "                    profile_components.append(\"Intensive Cash Advance User\")\n",
    "                elif cash_advance < mean_cash * 0.5 and cash_freq < mean_freq * 0.5:\n",
    "                    profile_components.append(\"Rare Cash Advance User\")\n",
    "            \n",
    "            if 'balance' in features and 'credit_limit' in features:\n",
    "                balance_ratio = cluster_profiles.loc[cluster, 'balance'] / (cluster_profiles.loc[cluster, 'credit_limit'] + 1e-10)\n",
    "                \n",
    "                if balance_ratio > 0.8:\n",
    "                    profile_components.append(\"Very High Credit Utilization\")\n",
    "                elif balance_ratio > 0.5:\n",
    "                    profile_components.append(\"High Credit Utilization\")\n",
    "                elif balance_ratio < 0.2:\n",
    "                    profile_components.append(\"Low Credit Utilization\")\n",
    "                else:\n",
    "                    profile_components.append(\"Moderate Credit Utilization\")\n",
    "            \n",
    "            if 'payments' in features and 'minimum_payments' in features:\n",
    "                payment_ratio = cluster_profiles.loc[cluster, 'payments'] / (cluster_profiles.loc[cluster, 'minimum_payments'] + 1e-10)\n",
    "                \n",
    "                if payment_ratio > 3:\n",
    "                    profile_components.append(\"Aggressive Overpayer\")\n",
    "                elif payment_ratio > 2:\n",
    "                    profile_components.append(\"Consistent Overpayer\")\n",
    "                elif payment_ratio < 1.2:\n",
    "                    profile_components.append(\"Minimum Payment User\")\n",
    "            \n",
    "            if 'balance' in features and 'credit_limit' in features and 'cash_advance' in features:\n",
    "                risk_score = (\n",
    "                    cluster_profiles.loc[cluster, 'balance'] / (cluster_profiles.loc[cluster, 'credit_limit'] + 1e-10) * 0.4 +\n",
    "                    cluster_profiles.loc[cluster, 'cash_advance'] / (cluster_profiles.loc[cluster, 'credit_limit'] + 1e-10) * 0.6\n",
    "                )\n",
    "                \n",
    "                if risk_score > 0.7:\n",
    "                    profile_components.append(\"Extremely High Financial Risk\")\n",
    "                elif risk_score > 0.5:\n",
    "                    profile_components.append(\"High Financial Risk\")\n",
    "                elif risk_score < 0.2:\n",
    "                    profile_components.append(\"Low Financial Risk\")\n",
    "                else:\n",
    "                    profile_components.append(\"Moderate Financial Risk\")\n",
    "            \n",
    "            if profile_components:\n",
    "                interpretations[cluster] = \"; \".join(profile_components)\n",
    "            else:\n",
    "                interpretations[cluster] = \"Undefined Customer Segment\"\n",
    "        \n",
    "        return interpretations\n",
    "\n",
    "    def upload_to_minio(local_path, bucket, object_key, max_retries=5):\n",
    "        \"\"\"Uploads a file to MinIO with improved error handling and content-length issues fixed\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Create a fresh client for each upload attempt\n",
    "                client = boto3.client(\n",
    "                    's3',\n",
    "                    endpoint_url=\"http://192.168.80.155:32000\",\n",
    "                    aws_access_key_id=\"admin\",\n",
    "                    aws_secret_access_key=\"dlyticaD123\",\n",
    "                    verify=False,\n",
    "                    config=Config(\n",
    "                        connect_timeout=30,\n",
    "                        read_timeout=60,\n",
    "                        retries={'max_attempts': 3}\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                # Get the file size\n",
    "                file_size = os.path.getsize(local_path)\n",
    "                \n",
    "                # For CSV files, we'll read and write with pandas to ensure proper formatting\n",
    "                if object_key.endswith('.csv'):\n",
    "                    df = pd.read_csv(local_path)\n",
    "                    csv_buffer = StringIO()\n",
    "                    df.to_csv(csv_buffer, index=False)\n",
    "                    client.put_object(\n",
    "                        Bucket=bucket,\n",
    "                        Key=object_key,\n",
    "                        Body=csv_buffer.getvalue(),\n",
    "                        ContentType='text/csv'\n",
    "                    )\n",
    "                else:\n",
    "                    # For non-CSV files, use standard upload\n",
    "                    with open(local_path, 'rb') as file_data:\n",
    "                        client.put_object(\n",
    "                            Bucket=bucket,\n",
    "                            Key=object_key,\n",
    "                            Body=file_data,\n",
    "                            ContentLength=file_size\n",
    "                        )\n",
    "                    \n",
    "                print(f\"Successfully uploaded {local_path} to {bucket}/{object_key}\")\n",
    "                return f\"{bucket}/{object_key}\"\n",
    "                \n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = 2 ** attempt  # Exponential backoff\n",
    "                    print(f\"Upload attempt {attempt+1} failed: {str(e)}. Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Upload failed after {max_retries} attempts: {str(e)}\")\n",
    "                    raise\n",
    "\n",
    "    # Initialize Spark with optimized settings\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"AdvancedCustomerSegmentation\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "        .config(\"spark.executor.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Initialize MinIO client with improved configuration\n",
    "    boto_config = Config(\n",
    "        connect_timeout=30,\n",
    "        read_timeout=60,\n",
    "        retries={'max_attempts': 3}\n",
    "    )\n",
    "    \n",
    "    minio_client = boto3.client(\n",
    "        's3',\n",
    "        endpoint_url=\"http://192.168.80.155:32000\",\n",
    "        aws_access_key_id=\"admin\",\n",
    "        aws_secret_access_key=\"dlyticaD123\",\n",
    "        verify=False,\n",
    "        config=boto_config\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        today = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        local_path = \"/tmp/raw_data.csv\"\n",
    "        \n",
    "        # Download with retry logic\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                minio_client.download_file(minio_bucket, file_path, local_path)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise\n",
    "                time.sleep(5)\n",
    "\n",
    "        # Read data with explicit schema\n",
    "        df = spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .csv(local_path)\n",
    "\n",
    "        if 'custid' in df.columns:\n",
    "            df = df.withColumnRenamed(\"custid\", \"cif_id\")\n",
    "\n",
    "        # Feature engineering\n",
    "        df = df.withColumn(\"balance_utilization_ratio\", \n",
    "            when(col(\"credit_limit\") != 0, col(\"balance\") / col(\"credit_limit\")).otherwise(0)\n",
    "        )\n",
    "        df = df.withColumn(\"cash_advance_intensity\", \n",
    "            when(col(\"credit_limit\") != 0, col(\"cash_advance\") / col(\"credit_limit\")).otherwise(0)\n",
    "        )\n",
    "        df = df.withColumn(\"payment_effort_ratio\", \n",
    "            when(col(\"minimum_payments\") != 0, col(\"payments\") / col(\"minimum_payments\")).otherwise(0)\n",
    "        )\n",
    "        df = df.withColumn(\"purchase_diversity\", \n",
    "            abs(col(\"oneoff_purchases\") - col(\"installments_purchases\")) / \n",
    "            (abs(col(\"oneoff_purchases\") + col(\"installments_purchases\")) + 1e-10)\n",
    "        )\n",
    "        df = df.na.fill(0)\n",
    "\n",
    "        numeric_cols = [\n",
    "            'purchases', 'oneoff_purchases', 'installments_purchases', \n",
    "            'cash_advance', 'cash_advance_frequency', \n",
    "            'balance', 'credit_limit', \n",
    "            'payments', 'minimum_payments',\n",
    "            'balance_utilization_ratio', \n",
    "            'cash_advance_intensity', \n",
    "            'payment_effort_ratio',\n",
    "            'purchase_diversity'\n",
    "        ]\n",
    "\n",
    "        # Feature transformation\n",
    "        assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "        assembled = assembler.transform(df.drop(\"cif_id\"))\n",
    "        scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "        scaled_data = scaler.fit(assembled).transform(assembled)\n",
    "\n",
    "        # Dimensionality reduction\n",
    "        pca = PCA(k=5, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
    "        pca_data = pca.fit(scaled_data).transform(scaled_data)\n",
    "\n",
    "        # Clustering optimization\n",
    "        evaluator = ClusteringEvaluator(featuresCol='pca_features')\n",
    "        k_range = range(6, 15)\n",
    "        k_metrics = []\n",
    "\n",
    "        for k in k_range:\n",
    "            kmeans = BisectingKMeans(k=k, seed=42, featuresCol=\"pca_features\")\n",
    "            model = kmeans.fit(pca_data)\n",
    "            predictions = model.transform(pca_data)\n",
    "            \n",
    "            cost = model.summary.trainingCost if hasattr(model.summary, 'trainingCost') else 0\n",
    "            silhouette = evaluator.evaluate(predictions)\n",
    "            \n",
    "            centers = model.clusterCenters()\n",
    "            separation = np.mean([np.min(np.linalg.norm(centers[i] - centers[j])) \n",
    "                                  for i in range(len(centers)) \n",
    "                                  for j in range(i+1, len(centers))])\n",
    "            \n",
    "            k_metrics.append({\n",
    "                'k': k,\n",
    "                'cost': cost,\n",
    "                'silhouette': silhouette,\n",
    "                'separation': separation\n",
    "            })\n",
    "\n",
    "        # Determine optimal k\n",
    "        k_metrics_df = pd.DataFrame(k_metrics)\n",
    "        k_metrics_df['normalized_cost'] = (k_metrics_df['cost'] - k_metrics_df['cost'].min()) / (k_metrics_df['cost'].max() - k_metrics_df['cost'].min())\n",
    "        k_metrics_df['normalized_silhouette'] = (k_metrics_df['silhouette'] - k_metrics_df['silhouette'].min()) / (k_metrics_df['silhouette'].max() - k_metrics_df['silhouette'].min())\n",
    "        k_metrics_df['normalized_separation'] = (k_metrics_df['separation'] - k_metrics_df['separation'].min()) / (k_metrics_df['separation'].max() - k_metrics_df['separation'].min())\n",
    "        \n",
    "        k_metrics_df['composite_score'] = (\n",
    "            0.3 * (1 - k_metrics_df['normalized_cost']) +\n",
    "            0.4 * k_metrics_df['normalized_silhouette'] +\n",
    "            0.3 * k_metrics_df['normalized_separation']\n",
    "        )\n",
    "        \n",
    "        optimal_k = k_metrics_df.loc[k_metrics_df['composite_score'].idxmax(), 'k']\n",
    "\n",
    "        # Plotting\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15))\n",
    "        ax1.plot(k_metrics_df['k'], k_metrics_df['cost'], marker='o', color='blue')\n",
    "        ax1.set_xlabel('Number of Clusters (k)')\n",
    "        ax1.set_ylabel('Cost', color='blue')\n",
    "        ax1.grid(True)\n",
    "\n",
    "        ax2.plot(k_metrics_df['k'], k_metrics_df['silhouette'], marker='x', color='green')\n",
    "        ax2.set_xlabel('Number of Clusters (k)')\n",
    "        ax2.set_ylabel('Silhouette Score', color='green')\n",
    "        ax2.grid(True)\n",
    "\n",
    "        ax3.plot(k_metrics_df['k'], k_metrics_df['separation'], marker='^', color='red')\n",
    "        ax3.set_xlabel('Number of Clusters (k)')\n",
    "        ax3.set_ylabel('Cluster Separation', color='red')\n",
    "        ax3.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        cluster_plot_path = f\"/tmp/cluster_diagnostics_{today}.png\"\n",
    "        plt.savefig(cluster_plot_path)\n",
    "        plt.close()\n",
    "        \n",
    "        # Final clustering\n",
    "        kmeans = BisectingKMeans(k=optimal_k, seed=42, featuresCol=\"pca_features\")\n",
    "        model = kmeans.fit(pca_data)\n",
    "        clustered_data = model.transform(pca_data)\n",
    "\n",
    "        # Add cluster information\n",
    "        df_with_id = df.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "        clustered_data = clustered_data.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "        clustered_data = clustered_data.join(df_with_id.select(\"row_id\", \"cif_id\"), on=\"row_id\").drop(\"row_id\")\n",
    "\n",
    "        # Generate cluster profiles\n",
    "        summary = clustered_data.groupBy(\"prediction\").agg(\n",
    "            *[mean(col(c)).alias(c) for c in numeric_cols],\n",
    "            *[stddev(col(c)).alias(f\"{c}_std\") for c in numeric_cols],\n",
    "            count(\"cif_id\").alias(\"cluster_size\")\n",
    "        )\n",
    "        \n",
    "        # Convert to Pandas safely\n",
    "        cluster_data = summary.collect()\n",
    "        cluster_profiles = pd.DataFrame([row.asDict() for row in cluster_data]).set_index(\"prediction\")\n",
    "        \n",
    "        # Get interpretations\n",
    "        interpretations = interpret_clusters(cluster_profiles)\n",
    "        \n",
    "        # Add cluster interpretation to dataframe\n",
    "        interpret_udf = udf(lambda x: interpretations.get(x, \"Unknown\"), StringType())\n",
    "        clustered_data = clustered_data.withColumn(\"interpretation\", interpret_udf(col(\"prediction\")))\n",
    "\n",
    "        # Prepare final output with all relevant columns\n",
    "        output_columns = [\"cif_id\", \"prediction\", \"interpretation\"] + numeric_cols\n",
    "        final_output = clustered_data.select(\n",
    "            col(\"cif_id\").alias(\"cif_id\"),\n",
    "            col(\"prediction\").alias(\"cluster\"),\n",
    "            col(\"interpretation\").alias(\"interpretations\")\n",
    "        )\n",
    "\n",
    "        # Convert to Pandas DataFrame for CSV export\n",
    "        final_pdf = pd.DataFrame(\n",
    "            final_output.rdd.map(lambda row: row.asDict()).collect()\n",
    "        )\n",
    "\n",
    "        # Get current date in YYYY-MM-DD format\n",
    "        current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Define output paths\n",
    "        output_paths = {\n",
    "            'final_output': f\"/tmp/final_segmented_customers_{current_date}.csv\",\n",
    "            'interpretations': f\"/tmp/cluster_interpretations_{current_date}.json\",\n",
    "            'plot': f\"/tmp/cluster_diagnostics_{current_date}.png\",\n",
    "            'model': f\"/tmp/segmentation_model_{current_date}\"\n",
    "        }\n",
    "        \n",
    "        # Save final output (only the 3 columns)\n",
    "        final_pdf.to_csv(output_paths['final_output'], index=False)\n",
    "        print(f\"Final output sample:\\n{final_pdf.head()}\")\n",
    "        \n",
    "        # Save interpretations\n",
    "        with open(output_paths['interpretations'], 'w') as f:\n",
    "            json.dump(interpretations, f, indent=2)\n",
    "        \n",
    "        # Save plot\n",
    "        plt.savefig(output_paths['plot'])\n",
    "        plt.close()\n",
    "        \n",
    "        # Save model\n",
    "        model.write().overwrite().save(output_paths['model'])\n",
    "        \n",
    "        # Upload to MinIO with specified folder structure\n",
    "        minio_paths = {}\n",
    "        \n",
    "        # Upload CSV files to results/\n",
    "        csv_files = {\n",
    "            'final_output': f\"results/final_segmented_customers_{current_date}.csv\",\n",
    "            'interpretations': f\"results/cluster_interpretations_{current_date}.json\"\n",
    "        }\n",
    "        \n",
    "        for name, minio_key in csv_files.items():\n",
    "            try:\n",
    "                uploaded_path = upload_to_minio(output_paths[name], minio_bucket, minio_key)\n",
    "                minio_paths[name] = uploaded_path\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading {name}: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "        # Upload plot to images/\n",
    "        plot_key = f\"images/cluster_diagnostics_{current_date}.png\"\n",
    "        try:\n",
    "            uploaded_path = upload_to_minio(output_paths['plot'], minio_bucket, plot_key)\n",
    "            minio_paths['plot'] = uploaded_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading plot: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # Upload model to models/\n",
    "        model_key_prefix = f\"models/segmentation_model_{current_date}\"\n",
    "        try:\n",
    "            for root, _, files in os.walk(output_paths['model']):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    relative_path = os.path.relpath(file_path, output_paths['model'])\n",
    "                    s3_key = f\"{model_key_prefix}/{relative_path}\"\n",
    "                    upload_to_minio(file_path, minio_bucket, s3_key)\n",
    "            minio_paths['model'] = f\"{minio_bucket}/{model_key_prefix}\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading model files: {str(e)}\")\n",
    "            minio_paths['model'] = f\"{minio_bucket}/{model_key_prefix}_partial\"\n",
    "\n",
    "        output = namedtuple('Outputs', [\n",
    "            'segmentation_path',\n",
    "            'model_path',\n",
    "            'cluster_plot_path',\n",
    "            'cluster_interpretations',\n",
    "            'final_output_path'\n",
    "        ])\n",
    "        \n",
    "        return output(\n",
    "            f\"{minio_bucket}/{csv_files['final_output']}\",  # segmentation_path\n",
    "            f\"{minio_bucket}/{model_key_prefix}\",           # model_path\n",
    "            f\"{minio_bucket}/{plot_key}\",                   # cluster_plot_path\n",
    "            f\"{minio_bucket}/{csv_files['interpretations']}\",  # cluster_interpretations\n",
    "            f\"{minio_bucket}/{csv_files['final_output']}\"   # final_output_path\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in feature_engineering_and_segmentation: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if spark:\n",
    "            spark.stop()\n",
    "\n",
    "# @component(\n",
    "#     base_image='quay.io/datanature_dev/jupyternotebook:java_home14',\n",
    "#     packages_to_install=[\n",
    "#         'pyspark==3.5.0',\n",
    "#         'pandas==2.0.3',\n",
    "#         'numpy==1.24.4',\n",
    "#         'boto3==1.28.57',\n",
    "#         'pyarrow==12.0.1',\n",
    "#         'urllib3==2.0.4'\n",
    "#     ]\n",
    "# )\n",
    "# def save_predictions_to_trino(\n",
    "#     segmentation_path: str,\n",
    "#     minio_bucket: str\n",
    "# ) -> str:\n",
    "#     import os\n",
    "#     from pyspark.sql import SparkSession\n",
    "#     from pyspark.sql.functions import col\n",
    "#     from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "#     # Configuration\n",
    "#     access_key_id = 'admin'\n",
    "#     secret_access_key = 'dlyticaD123'\n",
    "#     minio_endpoint = 'dn-minio-tenant-hl.dn-minio-tenant.svc.cluster.local:9000'\n",
    "#     data_bucket = 'ai360fd-recommendation'\n",
    "#     hive_metastore_uri = \"thrift://dn-hive-metastore.dn-hive-metastore.svc.cluster.local:9083\"\n",
    "#     iceberg_warehouse_location = f\"s3a://{data_bucket}/data/\"\n",
    "#     custom_catalog = \"iceberg_catalog\"\n",
    "#     app_name = \"Customer Segmentation Loader\"\n",
    "#     additional_packages = \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.spark:spark-avro_2.12:3.5.0,com.crealytics:spark-excel_2.12:0.13.7,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.hadoop:hadoop-common:3.3.4\"\n",
    "    \n",
    "#     # Initialize Spark Session with more compatible settings\n",
    "#     spark = SparkSession.builder \\\n",
    "#         .appName(app_name) \\\n",
    "#         .config(\"spark.driver.memory\", \"8g\") \\\n",
    "#         .config(\"spark.executor.memory\", \"8g\") \\\n",
    "#         .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "#         .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "#         .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "#         .config(\"spark.hadoop.fs.s3a.endpoint\", minio_endpoint) \\\n",
    "#         .config(\"spark.hadoop.fs.s3a.access.key\", access_key_id) \\\n",
    "#         .config(\"spark.hadoop.fs.s3a.secret.key\", secret_access_key) \\\n",
    "#         .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "#         .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "#         .config(\"spark.hadoop.hive.metastore.uris\", hive_metastore_uri) \\\n",
    "#         .config(\"spark.sql.warehouse.dir\", iceberg_warehouse_location) \\\n",
    "#         .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "#         .config(f\"spark.sql.catalog.{custom_catalog}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "#         .config(f\"spark.sql.catalog.{custom_catalog}.warehouse\", iceberg_warehouse_location) \\\n",
    "#         .config(f\"spark.sql.catalog.{custom_catalog}.s3.endpoint\", minio_endpoint) \\\n",
    "#         .config(f\"spark.sql.catalog.{custom_catalog}.s3.access-key\", access_key_id) \\\n",
    "#         .config(f\"spark.sql.catalog.{custom_catalog}.s3.secret-key\", secret_access_key) \\\n",
    "#         .config(f\"spark.sql.catalog.{custom_catalog}.s3.path-style-access\", \"true\") \\\n",
    "#         .config(\"spark.jars.packages\", additional_packages) \\\n",
    "#         .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "#         .config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n",
    "#         .config(\"spark.sql.legacy.createHiveTableByDefault\", \"false\") \\\n",
    "#         .config(\"spark.sql.iceberg.handle-timestamp-without-timezone\", \"true\") \\\n",
    "#         .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "#         .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "#         .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "#         .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"100\") \\\n",
    "#         .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "#         .enableHiveSupport() \\\n",
    "#         .getOrCreate()\n",
    "\n",
    "#     try:\n",
    "#         # Extract the object key from the segmentation_path\n",
    "#         if segmentation_path.startswith(f\"{minio_bucket}/\"):\n",
    "#             object_key = segmentation_path[len(minio_bucket)+1:]\n",
    "#         else:\n",
    "#             object_key = segmentation_path\n",
    "        \n",
    "#         # Construct the S3 path for Spark to read\n",
    "#         input_s3_path = f\"s3a://{minio_bucket}/{object_key}\"\n",
    "#         print(f\"Reading data from: {input_s3_path}\")\n",
    "        \n",
    "#         # Read the segmentation data\n",
    "#         df = spark.read \\\n",
    "#             .option(\"header\", \"true\") \\\n",
    "#             .option(\"inferSchema\", \"true\") \\\n",
    "#             .csv(input_s3_path)\n",
    "        \n",
    "#         # Print input schema for debugging\n",
    "#         print(\"Input DataFrame Schema:\")\n",
    "#         df.printSchema()\n",
    "        \n",
    "#         # Rename columns to match our desired schema\n",
    "#         column_mapping = {\n",
    "#             \"cluster\": \"cluster_id\",\n",
    "#             \"interpretations\": \"interpretation\"  # Corrected spelling\n",
    "#         }\n",
    "        \n",
    "#         for old_name, new_name in column_mapping.items():\n",
    "#             if old_name in df.columns:\n",
    "#                 df = df.withColumnRenamed(old_name, new_name)\n",
    "        \n",
    "#         # Ensure required columns exist\n",
    "#         required_columns = [\"cif_id\", \"cluster_id\", \"interpretation\"]\n",
    "#         missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "#         if missing_columns:\n",
    "#             raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "        \n",
    "#         # Select only the required columns and cache to minimize re-computation\n",
    "#         df = df.select(*required_columns).cache()\n",
    "        \n",
    "#         # Sample a small subset for testing if dataset is large\n",
    "#         # This helps with storage full issues - remove in production\n",
    "#         row_count = df.count()\n",
    "#         if row_count > 1000:\n",
    "#             print(f\"Dataset has {row_count} rows, sampling for testing due to storage constraints\")\n",
    "#             df = df.sample(fraction=1000.0/row_count).cache()\n",
    "        \n",
    "#         # Define the table name\n",
    "#         table_name = f\"{custom_catalog}.gold.customer_segments\"\n",
    "#         temp_table_name = f\"{custom_catalog}.gold.customer_segments_temp\"\n",
    "        \n",
    "#         # Handle existing table - this is crucial as we're getting an \"already exists\" error\n",
    "#         try:\n",
    "#             # Check if table exists by attempting to access it directly\n",
    "#             print(f\"Checking if {table_name} exists\")\n",
    "            \n",
    "#             # Use more direct DataFrame API approach to check table existence\n",
    "#             tables = spark.catalog.listTables(f\"{custom_catalog}.gold\")\n",
    "#             table_exists = any(t.name.lower() == \"customer_segments\" for t in tables)\n",
    "            \n",
    "#             if table_exists:\n",
    "#                 print(f\"Table {table_name} exists, will use 'overwrite' mode instead of 'create'\")\n",
    "#                 # Get sample of existing data to verify schema\n",
    "#                 try:\n",
    "#                     existing_df = spark.table(table_name).limit(1)\n",
    "#                     print(\"Existing table schema:\")\n",
    "#                     existing_df.printSchema()\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Could not read existing table: {str(e)}\")\n",
    "#             else:\n",
    "#                 print(f\"Table {table_name} does not exist\")\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error checking table existence: {str(e)}\")\n",
    "#             print(\"Proceeding with assumption that table does not exist\")\n",
    "#             table_exists = False\n",
    "        \n",
    "#         # Write data - handle both cases (exists or not)\n",
    "#         print(\"Writing data to table...\")\n",
    "#         try:\n",
    "#             if table_exists:\n",
    "#                 # Try overwrite mode\n",
    "#                 print(f\"Using overwrite mode for existing table {table_name}\")\n",
    "#                 # df.writeTo(table_name).overwrite()\n",
    "\n",
    "#                 df.writeTo(table_name).using(\"iceberg\").partitionedBy(\"cluster_id\").createOrReplace()\n",
    "\n",
    "#             else:\n",
    "#                 # Try create mode with IF NOT EXISTS to avoid conflicts\n",
    "#                 print(f\"Creating new table {table_name}\")\n",
    "#                 try:\n",
    "#                     df.writeTo(table_name).using(\"iceberg\").createOrReplace()\n",
    "#                 except Exception as e:\n",
    "#                     # Last resort - try different approach if createOrReplace fails\n",
    "#                     print(f\"Error with createOrReplace: {str(e)}\")\n",
    "#                     print(\"Trying alternative approach with temporary table\")\n",
    "                    \n",
    "#                     # Create with temporary name first\n",
    "#                     df.writeTo(temp_table_name).using(\"iceberg\").create()\n",
    "                    \n",
    "#                     # Then rename or copy data if rename not supported\n",
    "#                     tables = spark.catalog.listTables(f\"{custom_catalog}.gold\")\n",
    "#                     if any(t.name.lower() == \"customer_segments\" for t in tables):\n",
    "#                         # If target exists, drop it first\n",
    "#                         spark.catalog.dropTable(table_name)\n",
    "                    \n",
    "#                     # Now rename temp to target\n",
    "#                     spark._jsparkSession.catalog().renameTable(temp_table_name, table_name)\n",
    "            \n",
    "#             # Verify the data was written\n",
    "#             verification_df = spark.table(table_name)\n",
    "#             count = verification_df.count()\n",
    "#             print(f\"Successfully wrote {count} records to {table_name}\")\n",
    "#             return f\"Successfully loaded {count} records to {table_name}\"\n",
    "            \n",
    "#         except Exception as write_error:\n",
    "#             if \"XMinioStorageFull\" in str(write_error):\n",
    "#                 print(\"Storage full error detected\")\n",
    "#                 # Handle storage full error by using compact format and reducing data size\n",
    "#                 print(\"Attempting to write with more compact format and reduced dataset\")\n",
    "                \n",
    "#                 # Sample smaller dataset as emergency fallback\n",
    "#                 small_df = df.sample(fraction=0.1).cache()\n",
    "                \n",
    "#                 # Try write with most efficient settings\n",
    "#                 small_df.coalesce(1).writeTo(table_name).using(\"iceberg\").option(\"write-format\", \"parquet\").option(\"compression\", \"snappy\").createOrReplace()\n",
    "                \n",
    "#                 warn_msg = \"WARNING: Storage was full - only wrote 10% sample of data\"\n",
    "#                 print(warn_msg)\n",
    "#                 return warn_msg\n",
    "#             else:\n",
    "#                 # Re-raise other errors\n",
    "#                 raise\n",
    "                \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in save_predictions_to_trino: {str(e)}\")\n",
    "#         return f\"Error: {str(e)}\"\n",
    "#     finally:\n",
    "#         if 'spark' in locals():\n",
    "#             spark.stop()\n",
    "\n",
    "@component(\n",
    "    base_image='quay.io/datanature_dev/jupyternotebook:java_home14',\n",
    "    packages_to_install=[\n",
    "        'pyspark==3.5.0',\n",
    "        'pandas==2.0.3',\n",
    "        'numpy==1.24.4',\n",
    "        'boto3==1.28.57',\n",
    "        'pyarrow==12.0.1',\n",
    "        'urllib3==2.0.4'\n",
    "    ]\n",
    ")\n",
    "def save_predictions_to_trino(\n",
    "    segmentation_path: str,\n",
    "    minio_bucket: str\n",
    ") -> str:\n",
    "    import os\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col, lit\n",
    "    from pyspark.sql.utils import AnalysisException\n",
    "    import time\n",
    "\n",
    "    # Configuration\n",
    "    access_key_id = 'admin'\n",
    "    secret_access_key = 'dlyticaD123'\n",
    "    minio_endpoint = 'dn-minio-tenant-hl.dn-minio-tenant.svc.cluster.local:9000'\n",
    "    data_bucket = 'ai360fd-recommendation'\n",
    "    hive_metastore_uri = \"thrift://dn-hive-metastore.dn-hive-metastore.svc.cluster.local:9083\"\n",
    "    iceberg_warehouse_location = f\"s3a://{data_bucket}/data/\"\n",
    "    custom_catalog = \"iceberg_catalog\"\n",
    "    app_name = \"Customer Segmentation Loader\"\n",
    "    additional_packages = \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.spark:spark-avro_2.12:3.5.0,com.crealytics:spark-excel_2.12:0.13.7,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.hadoop:hadoop-common:3.3.4\"\n",
    "    \n",
    "    # Initialize Spark Session with optimized Iceberg settings\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", minio_endpoint) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", access_key_id) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", secret_access_key) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .config(\"spark.hadoop.hive.metastore.uris\", hive_metastore_uri) \\\n",
    "        .config(\"spark.sql.warehouse.dir\", iceberg_warehouse_location) \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "        .config(f\"spark.sql.catalog.{custom_catalog}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "        .config(f\"spark.sql.catalog.{custom_catalog}.warehouse\", iceberg_warehouse_location) \\\n",
    "        .config(f\"spark.sql.catalog.{custom_catalog}.s3.endpoint\", minio_endpoint) \\\n",
    "        .config(f\"spark.sql.catalog.{custom_catalog}.s3.access-key\", access_key_id) \\\n",
    "        .config(f\"spark.sql.catalog.{custom_catalog}.s3.secret-key\", secret_access_key) \\\n",
    "        .config(f\"spark.sql.catalog.{custom_catalog}.s3.path-style-access\", \"true\") \\\n",
    "        .config(\"spark.jars.packages\", additional_packages) \\\n",
    "        .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "        .config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n",
    "        .config(\"spark.sql.legacy.createHiveTableByDefault\", \"false\") \\\n",
    "        .config(\"spark.sql.iceberg.handle-timestamp-without-timezone\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"100\") \\\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "        .config(\"spark.sql.iceberg.optimize.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.iceberg.optimize.rewrite.delete.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.iceberg.optimize.rewrite.data.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.iceberg.vectorization.enabled\", \"true\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    try:\n",
    "        # Extract the object key from the segmentation_path\n",
    "        if segmentation_path.startswith(f\"{minio_bucket}/\"):\n",
    "            object_key = segmentation_path[len(minio_bucket)+1:]\n",
    "        else:\n",
    "            object_key = segmentation_path\n",
    "        \n",
    "        # Construct the S3 path for Spark to read\n",
    "        input_s3_path = f\"s3a://{minio_bucket}/{object_key}\"\n",
    "        print(f\"Reading data from: {input_s3_path}\")\n",
    "        \n",
    "        # Read the segmentation data with explicit schema\n",
    "        df = spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .csv(input_s3_path)\n",
    "        \n",
    "        # Print input schema for debugging\n",
    "        print(\"Input DataFrame Schema:\")\n",
    "        df.printSchema()\n",
    "        \n",
    "        # Rename columns to match our desired schema\n",
    "        column_mapping = {\n",
    "            \"cluster\": \"cluster_id\",\n",
    "            \"interpretations\": \"interpretation\"\n",
    "        }\n",
    "        \n",
    "        for old_name, new_name in column_mapping.items():\n",
    "            if old_name in df.columns:\n",
    "                df = df.withColumnRenamed(old_name, new_name)\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        required_columns = [\"cif_id\", \"cluster_id\", \"interpretation\"]\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "        \n",
    "        # Select only the required columns\n",
    "        df = df.select(*required_columns)\n",
    "        \n",
    "        # Add processing timestamp\n",
    "        df = df.withColumn(\"processing_timestamp\", lit(time.strftime('%Y-%m-%d %H:%M:%S')))\n",
    "        \n",
    "        # Count records before writing\n",
    "        row_count = df.count()\n",
    "        print(f\"Preparing to write {row_count:,} records to Trino\")\n",
    "        \n",
    "        # Define the table name\n",
    "        table_name = f\"{custom_catalog}.gold.customer_segments\"\n",
    "        \n",
    "        # Check if table exists\n",
    "        table_exists = spark.catalog.tableExists(table_name)\n",
    "        \n",
    "        # Configure write properties\n",
    "        write_properties = {\n",
    "            \"write.format.default\": \"parquet\",\n",
    "            \"write.parquet.compression-codec\": \"zstd\",\n",
    "            \"write.parquet.row-group-size-bytes\": \"8388608\",  # 8MB\n",
    "            \"write.metadata.delete-after-commit.enabled\": \"true\",\n",
    "            \"write.metadata.previous-versions-max\": \"3\",\n",
    "            \"write.parquet.dict-size-bytes\": \"1048576\",  # 1MB\n",
    "            \"write.spark.fanout.enabled\": \"true\"\n",
    "        }\n",
    "        \n",
    "        # Write data with optimized settings\n",
    "        if table_exists:\n",
    "            print(f\"Table {table_name} exists - appending with overwrite\")\n",
    "            \n",
    "            # For large updates, it's better to create a temp table and then replace\n",
    "            temp_table_name = f\"{custom_catalog}.gold.customer_segments_temp_{int(time.time())}\"\n",
    "            \n",
    "            print(f\"Creating temporary table {temp_table_name}\")\n",
    "            df.writeTo(temp_table_name) \\\n",
    "                .using(\"iceberg\") \\\n",
    "                .partitionedBy(\"cluster_id\") \\\n",
    "                .tableProperty(\"format-version\", \"2\") \\\n",
    "                .tableProperty(\"write.parquet.compression-codec\", \"zstd\") \\\n",
    "                .create()\n",
    "            \n",
    "            print(\"Replacing main table with temporary table\")\n",
    "            spark.sql(f\"ALTER TABLE {table_name} REPLACE AS SELECT * FROM {temp_table_name}\")\n",
    "            \n",
    "            print(\"Dropping temporary table\")\n",
    "            spark.sql(f\"DROP TABLE {temp_table_name}\")\n",
    "        else:\n",
    "            print(f\"Creating new table {table_name}\")\n",
    "            df.writeTo(table_name) \\\n",
    "                .using(\"iceberg\") \\\n",
    "                .partitionedBy(\"cluster_id\") \\\n",
    "                .tableProperty(\"format-version\", \"2\") \\\n",
    "                .tableProperty(\"write.parquet.compression-codec\", \"zstd\") \\\n",
    "                .create()\n",
    "        \n",
    "        # Verify the data was written\n",
    "        verification_df = spark.table(table_name)\n",
    "        final_count = verification_df.count()\n",
    "        print(f\"Successfully wrote {final_count:,} records to {table_name}\")\n",
    "        \n",
    "        # Optimize the table after write\n",
    "        print(\"Optimizing table layout...\")\n",
    "        spark.sql(f\"CALL {custom_catalog}.system.rewrite_data_files(table => '{table_name}')\")\n",
    "        \n",
    "        return f\"Successfully loaded {final_count:,} records to {table_name}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in save_predictions_to_trino: {str(e)}\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "    finally:\n",
    "        if 'spark' in locals():\n",
    "            spark.stop()\n",
    "\n",
    "\n",
    "# Pipeline definition\n",
    "@pipeline(name=\"Customer Segmentation Pipeline\")\n",
    "def customer_segmentation_pipeline(\n",
    "    minio_bucket: str = \"ai360ctzn-customer-segmentation\"\n",
    "):\n",
    "    # Step 1: Fetch data\n",
    "    fetch_task = fetch_data_trino(minio_bucket=minio_bucket)\n",
    "    fetch_task.set_caching_options(False)\n",
    "    \n",
    "    # Step 2: Process data and create segments\n",
    "    segmentation_task = feature_engineering_and_segmentation(\n",
    "        file_path=fetch_task.output,\n",
    "        minio_bucket=minio_bucket\n",
    "    )\n",
    "    segmentation_task.set_caching_options(False)\n",
    "    \n",
    "    # Step 3: Save results to Trino\n",
    "    save_to_trino = save_predictions_to_trino(\n",
    "        segmentation_path=segmentation_task.outputs['final_output_path'],\n",
    "        minio_bucket=minio_bucket\n",
    "    )\n",
    "    save_to_trino.set_caching_options(False)\n",
    "\n",
    "    \n",
    "\n",
    "    save_to_trino.set_caching_options(False)\n",
    "# Compile the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    from kfp import compiler\n",
    "    compiler.Compiler().compile(\n",
    "        customer_segmentation_pipeline,\n",
    "        \"customer_segmentation_pipeline.yaml\"\n",
    "    )\n",
    "    print(\"Pipeline compiled successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ec60c746-ed2e-4044-b800-10047d2214ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.parse import urlsplit, urlencode\n",
    "\n",
    "import kfp\n",
    "import requests\n",
    "import urllib3\n",
    "\n",
    "\n",
    "class KFPClientManager:\n",
    "    \"\"\"\n",
    "    A class that creates `kfp.Client` instances with Dex authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_url: str,\n",
    "        dex_username: str,\n",
    "        dex_password: str,\n",
    "        dex_auth_type: str = \"local\",\n",
    "        skip_tls_verify: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the KfpClient\"\n",
    "\n",
    "        :param api_url: the Kubeflow Pipelines API URL\n",
    "        :param skip_tls_verify: if True, skip TLS verification\n",
    "        :param dex_username: the Dex username\n",
    "        :param dex_password: the Dex password\n",
    "        :param dex_auth_type: the auth type to use if Dex has multiple enabled, one of: ['ldap', 'local']\n",
    "        \"\"\"\n",
    "        self._api_url = api_url\n",
    "        self._skip_tls_verify = skip_tls_verify\n",
    "        self._dex_username = dex_username\n",
    "        self._dex_password = dex_password\n",
    "        self._dex_auth_type = dex_auth_type\n",
    "        self._client = None\n",
    "\n",
    "        # disable SSL verification, if requested\n",
    "        if self._skip_tls_verify:\n",
    "            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "        # ensure `dex_default_auth_type` is valid\n",
    "        if self._dex_auth_type not in [\"ldap\", \"local\"]:\n",
    "            raise ValueError(\n",
    "                f\"Invalid `dex_auth_type` '{self._dex_auth_type}', must be one of: ['ldap', 'local']\"\n",
    "            )\n",
    "\n",
    "    def _get_session_cookies(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the session cookies by authenticating against Dex\n",
    "        :return: a string of session cookies in the form \"key1=value1; key2=value2\"\n",
    "        \"\"\"\n",
    "\n",
    "        # use a persistent session (for cookies)\n",
    "        s = requests.Session()\n",
    "\n",
    "        # GET the api_url, which should redirect to Dex\n",
    "        resp = s.get(\n",
    "            self._api_url, allow_redirects=True, verify=not self._skip_tls_verify\n",
    "        )\n",
    "        if resp.status_code == 200:\n",
    "            pass\n",
    "        elif resp.status_code == 403:\n",
    "            # if we get 403, we might be at the oauth2-proxy sign-in page\n",
    "            # the default path to start the sign-in flow is `/oauth2/start?rd=<url>`\n",
    "            url_obj = urlsplit(resp.url)\n",
    "            url_obj = url_obj._replace(\n",
    "                path=\"/oauth2/start\", query=urlencode({\"rd\": url_obj.path})\n",
    "            )\n",
    "            resp = s.get(\n",
    "                url_obj.geturl(), allow_redirects=True, verify=not self._skip_tls_verify\n",
    "            )\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f\"HTTP status code '{resp.status_code}' for GET against: {self._api_url}\"\n",
    "            )\n",
    "\n",
    "        # if we were NOT redirected, then the endpoint is unsecured\n",
    "        if len(resp.history) == 0:\n",
    "            # no cookies are needed\n",
    "            return \"\"\n",
    "\n",
    "        # if we are at `../auth` path, we need to select an auth type\n",
    "        url_obj = urlsplit(resp.url)\n",
    "        if re.search(r\"/auth$\", url_obj.path):\n",
    "            url_obj = url_obj._replace(\n",
    "                path=re.sub(r\"/auth$\", f\"/auth/{self._dex_auth_type}\", url_obj.path)\n",
    "            )\n",
    "\n",
    "        # if we are at `../auth/xxxx/login` path, then we are at the login page\n",
    "        if re.search(r\"/auth/.*/login$\", url_obj.path):\n",
    "            dex_login_url = url_obj.geturl()\n",
    "        else:\n",
    "            # otherwise, we need to follow a redirect to the login page\n",
    "            resp = s.get(\n",
    "                url_obj.geturl(), allow_redirects=True, verify=not self._skip_tls_verify\n",
    "            )\n",
    "            if resp.status_code != 200:\n",
    "                raise RuntimeError(\n",
    "                    f\"HTTP status code '{resp.status_code}' for GET against: {url_obj.geturl()}\"\n",
    "                )\n",
    "            dex_login_url = resp.url\n",
    "\n",
    "        # attempt Dex login\n",
    "        resp = s.post(\n",
    "            dex_login_url,\n",
    "            data={\"login\": self._dex_username, \"password\": self._dex_password},\n",
    "            allow_redirects=True,\n",
    "            verify=not self._skip_tls_verify,\n",
    "        )\n",
    "        if resp.status_code != 200:\n",
    "            raise RuntimeError(\n",
    "                f\"HTTP status code '{resp.status_code}' for POST against: {dex_login_url}\"\n",
    "            )\n",
    "\n",
    "        # if we were NOT redirected, then the login credentials were probably invalid\n",
    "        if len(resp.history) == 0:\n",
    "            raise RuntimeError(\n",
    "                f\"Login credentials are probably invalid - \"\n",
    "                f\"No redirect after POST to: {dex_login_url}\"\n",
    "            )\n",
    "\n",
    "        # if we are at `../approval` path, we need to approve the login\n",
    "        url_obj = urlsplit(resp.url)\n",
    "        if re.search(r\"/approval$\", url_obj.path):\n",
    "            dex_approval_url = url_obj.geturl()\n",
    "\n",
    "            # approve the login\n",
    "            resp = s.post(\n",
    "                dex_approval_url,\n",
    "                data={\"approval\": \"approve\"},\n",
    "                allow_redirects=True,\n",
    "                verify=not self._skip_tls_verify,\n",
    "            )\n",
    "            if resp.status_code != 200:\n",
    "                raise RuntimeError(\n",
    "                    f\"HTTP status code '{resp.status_code}' for POST against: {url_obj.geturl()}\"\n",
    "                )\n",
    "\n",
    "        return \"; \".join([f\"{c.name}={c.value}\" for c in s.cookies])\n",
    "\n",
    "    def _create_kfp_client(self) -> kfp.Client:\n",
    "        try:\n",
    "            session_cookies = self._get_session_cookies()\n",
    "        except Exception as ex:\n",
    "            raise RuntimeError(f\"Failed to get Dex session cookies\") from ex\n",
    "\n",
    "        # monkey patch the kfp.Client to support disabling SSL verification\n",
    "        # kfp only added support in v2: https://github.com/kubeflow/pipelines/pull/7174\n",
    "        original_load_config = kfp.Client._load_config\n",
    "\n",
    "        def patched_load_config(client_self, *args, **kwargs):\n",
    "            config = original_load_config(client_self, *args, **kwargs)\n",
    "            config.verify_ssl = not self._skip_tls_verify\n",
    "            return config\n",
    "\n",
    "        patched_kfp_client = kfp.Client\n",
    "        patched_kfp_client._load_config = patched_load_config\n",
    "\n",
    "        return patched_kfp_client(\n",
    "            host=self._api_url,\n",
    "            cookies=session_cookies,\n",
    "        )\n",
    "\n",
    "    def create_kfp_client(self) -> kfp.Client:\n",
    "        \"\"\"Get a newly authenticated Kubeflow Pipelines client.\"\"\"\n",
    "        return self._create_kfp_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5ce6a5af-ae17-48fd-80e8-ee942ed1efef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/kfp/client/client.py:159: FutureWarning: This client only works with Kubeflow Pipeline v2.0.0-beta.2 and later versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tutorial] Data passing in python components\n",
      "[Tutorial] DSL - Control structures\n",
      "Citizen_pipeline16\n",
      "Customer_Segmentation_Pipeline\n",
      "recommendation_pipeline\n",
      "pipeline matched\n",
      "8f2c9e3c-3e51-4099-abb5-071bdcc59e1d\n",
      "Version ID: 946a8cd5-e7ca-4e7b-a9b7-bbb1f71a1982, Name: recommendation_pipeline\n",
      "Deleted version: 946a8cd5-e7ca-4e7b-a9b7-bbb1f71a1982\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://192.168.80.155:31904/pipeline/#/pipelines/details/894d4646-06eb-417d-b950-e1f587a88566\" target=\"_blank\" >Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://192.168.80.155:31904/pipeline/#/experiments/details/af85287c-a6f4-45e8-b8a5-3fc053065211\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://192.168.80.155:31904/pipeline/#/runs/details/8921b833-6012-4e80-ad83-9876134e5152\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kfp_client_manager = KFPClientManager(\n",
    "    api_url=\"http://192.168.80.155:31904/pipeline\",\n",
    "    skip_tls_verify=True,\n",
    "\n",
    "    dex_username=\"user@dlytica.com\",\n",
    "    dex_password=\"dlytica@D123#\",\n",
    "\n",
    "    # can be 'ldap' or 'local' depending on your Dex configuration\n",
    "    dex_auth_type=\"local\",\n",
    ")\n",
    "\n",
    "# get a newly authenticated KFP client\n",
    "# TIP: long-lived sessions might need to get a new client when their session expires\n",
    "kfp_client = kfp_client_manager.create_kfp_client()\n",
    "\n",
    "# # test the client by listing experiments\n",
    "# experiments = kfp_client.list_experiments(namespace=\"kubeflow-user-example-com\")\n",
    "# print(experiments)\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "pipeline_name = 'recommendation_pipeline'\n",
    "pipelines = kfp_client.list_pipelines(page_size=100)\n",
    "# experiments = kfp_client.list_experiments()\n",
    "# print(experiments)\n",
    "# print(pipelines)\n",
    "for pipeline in pipelines.pipelines:\n",
    "    print(pipeline.display_name)\n",
    "    if pipeline.display_name == pipeline_name:\n",
    "        print(\"pipeline matched\")\n",
    "        pipeline_id = pipeline.pipeline_id\n",
    "        print(pipeline_id)\n",
    "        # pipeline_id = '4758a6db-8057-463f-a185-a68f3e45d920'\n",
    "        pipeline_versions = kfp_client.list_pipeline_versions(pipeline_id=pipeline_id, page_size=100)\n",
    "        for version in pipeline_versions.pipeline_versions:\n",
    "            print(f\"Version ID: {version.pipeline_version_id}, Name: {version.display_name}\")\n",
    "        for version in pipeline_versions.pipeline_versions:\n",
    "            version_id = version.pipeline_version_id\n",
    "            try:\n",
    "                kfp_client.delete_pipeline_version(pipeline_id = pipeline_id, pipeline_version_id=version_id)  \n",
    "                print(f\"Deleted version: {version_id}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete version {version_id}: {e}\")\n",
    "        kfp_client.delete_pipeline(pipeline.pipeline_id)\n",
    "description = 'this is automl pipeline'\n",
    "kfp_client.upload_pipeline('customer_segmentation_pipeline.yaml',pipeline_name = pipeline_name, description = description)\n",
    "\n",
    "run = kfp_client.create_run_from_pipeline_func(\n",
    "    customer_segmentation_pipeline,\n",
    "    arguments={\"minio_bucket\": \"ai360ctzn-customer-segmentation\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9586bf9-3017-4a2a-9a3f-18c3de2d21f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
